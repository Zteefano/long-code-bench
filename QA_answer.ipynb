{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jsonl file\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "data = load_jsonl('output/Llama_QA.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "A)\n",
      "A)\n",
      "A)\n",
      "A)\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "for point in data:\n",
    "    if point['generation'] != 'A':\n",
    "        print(point['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are going to be provided the content of a\n",
      "repository and a question about it. Provide the answer to the question by\n",
      "stating ONLY the letter associated to the question.\n",
      "\n",
      "Repository:\n",
      "[start of .gitignore]\n",
      "*.egg-info\n",
      "*.dist-info\n",
      "*.pyc\n",
      "*.so\n",
      "*.dll\n",
      "build\n",
      "dist\n",
      "venv*/\n",
      "docs/_build\n",
      "src/wheel/_version.py\n",
      "__pycache__\n",
      ".coverage\n",
      ".pytest_cache\n",
      ".ruff_cache\n",
      ".mypy_cache\n",
      ".tox\n",
      ".idea\n",
      ".cache\n",
      ".eggs\n",
      "\n",
      "[end of .gitignore]\n",
      "[start of pyproject.toml]\n",
      "[build-system]\n",
      "requires = [\"flit_core >=3.8,<4\"]\n",
      "build-backend = \"flit_core.buildapi\"\n",
      "\n",
      "[project]\n",
      "name = \"wheel\"\n",
      "description = \"A built-package format for Python\"\n",
      "readme = \"README.rst\"\n",
      "license = {file = \"LICENSE.txt\"}\n",
      "classifiers = [\n",
      "    \"Development Status :: 5 - Production/Stable\",\n",
      "    \"Intended Audience :: Developers\",\n",
      "    \"Topic :: System :: Archiving :: Packaging\",\n",
      "    \"License :: OSI Approved :: MIT License\",\n",
      "    \"Programming Language :: Python\",\n",
      "    \"Programming Language :: Python :: 3 :: Only\",\n",
      "    \"Programming Language :: Python :: 3.8\",\n",
      "    \"Programming Language :: Python :: 3.9\",\n",
      "    \"Programming Language :: Python :: 3.10\",\n",
      "    \"Programming Language :: Python :: 3.11\",\n",
      "    \"Programming Language :: Python :: 3.12\",\n",
      "    \"Programming Language :: Python :: 3.13\",\n",
      "]\n",
      "authors = [{name = \"Daniel Holth\", email = \"dholth@fastmail.fm\"}]\n",
      "maintainers = [{name = \"Alex Grönholm\", email = \"alex.gronholm@nextday.fi\"}]\n",
      "keywords = [\"wheel\", \"packaging\"]\n",
      "requires-python = \">=3.8\"\n",
      "dynamic = [\"version\"]\n",
      "\n",
      "[project.urls]\n",
      "Documentation = \"https://wheel.readthedocs.io/\"\n",
      "Changelog = \"https://wheel.readthedocs.io/en/stable/news.html\"\n",
      "\"Issue Tracker\" = \"https://github.com/pypa/wheel/issues\"\n",
      "Source = \"https://github.com/pypa/wheel\"\n",
      "\n",
      "[project.scripts]\n",
      "wheel = \"wheel.cli:main\"\n",
      "\n",
      "[project.entry-points.\"distutils.commands\"]\n",
      "bdist_wheel = \"wheel.bdist_wheel:bdist_wheel\"\n",
      "\n",
      "[project.optional-dependencies]\n",
      "test = [\n",
      "    \"pytest >= 6.0.0\",\n",
      "    \"setuptools >= 65\",\n",
      "]\n",
      "\n",
      "[tool.flit.sdist]\n",
      "include = [\n",
      "    \"LICENSE*\",\n",
      "    \"docs/**/*.py\",\n",
      "    \"docs/**/*.rst\",\n",
      "    \"docs/Makefile\",\n",
      "    \"docs/make.bat\",\n",
      "    \"manpages/*.rst\",\n",
      "    \"tests/**/*.py\",\n",
      "    \"tests/**/*.txt\",\n",
      "    \"tests/**/*.c\",\n",
      "    \"tests/**/*.h\",\n",
      "    \"tests/**/*.cfg\",\n",
      "    \"tests/testdata/macosx_minimal_system_version/*.dylib\",\n",
      "    \"tests/testdata/test-1.0-py2.py3-none-any.whl\",\n",
      "]\n",
      "exclude = [\n",
      "    \".cirrus.yml\",\n",
      "    \".github/**\",\n",
      "    \".gitignore\",\n",
      "    \".pre-commit-config.yaml\",\n",
      "    \".readthedocs.yml\",\n",
      "    \"**/__pycache__\",\n",
      "]\n",
      "\n",
      "[tool.pytest.ini_options]\n",
      "minversion = \"6.0\"\n",
      "addopts = [\"-rsfE\", \"--tb=short\", \"--strict-markers\", \"--strict-config\"]\n",
      "xfail_strict = true\n",
      "filterwarnings = [\n",
      "    \"error\",\n",
      "    \"ignore::Warning:_pytest.*\",\n",
      "]\n",
      "log_cli_level = \"info\"\n",
      "testpaths = [\"test\"]\n",
      "\n",
      "[tool.coverage.run]\n",
      "source = [\"wheel\"]\n",
      "omit = [\"*/vendored/*\"]\n",
      "exclude_also = [\n",
      "    \"@overload\",\n",
      "    \"if TYPE_CHECKING:\"\n",
      "]\n",
      "\n",
      "[tool.coverage.report]\n",
      "show_missing = true\n",
      "exclude_also = [\n",
      "    \"@abstractmethod\",\n",
      "]\n",
      "\n",
      "[tool.ruff]\n",
      "extend-exclude = [\"src/wheel/vendored\"]\n",
      "\n",
      "[tool.ruff.lint]\n",
      "extend-select = [\n",
      "    \"B\",            # flake8-bugbear\n",
      "    \"G\",            # flake8-logging-format\n",
      "    \"I\",            # isort\n",
      "    \"ISC\",          # flake8-implicit-str-concat\n",
      "    \"PGH\",          # pygrep-hooks\n",
      "    \"RUF100\",       # unused noqa (yesqa)\n",
      "    \"UP\",           # pyupgrade\n",
      "    \"W\",            # pycodestyle warnings\n",
      "]\n",
      "\n",
      "# Tox (https://tox.wiki/) is a tool for running tests in multiple virtualenvs.\n",
      "# This configuration file will run the test suite on all supported python\n",
      "# versions. To use it, \"pipx install tox\" and then run \"tox\" from this\n",
      "# directory.\n",
      "\n",
      "[tool.tox]\n",
      "legacy_tox_ini = '''\n",
      "[tox]\n",
      "envlist = py38, py39, py310, py311, py312, py313, pypy3, lint, pkg\n",
      "minversion = 4.0.0\n",
      "skip_missing_interpreters = true\n",
      "\n",
      "[testenv]\n",
      "package = wheel\n",
      "wheel_build_env = .pkg\n",
      "depends = lint\n",
      "commands = {env_python} -b -m pytest {posargs}\n",
      "extras = test\n",
      "set_env =\n",
      "  PYTHONWARNDEFAULTENCODING = 1\n",
      "\n",
      "[testenv:lint]\n",
      "depends =\n",
      "basepython = python3\n",
      "deps = pre-commit\n",
      "commands = pre-commit run --all-files --show-diff-on-failure\n",
      "skip_install = true\n",
      "\n",
      "[testenv:pkg]\n",
      "basepython = python3\n",
      "deps =\n",
      "    build\n",
      "    flit>=3.8\n",
      "commands = {envpython} -b -m pytest tests/test_sdist.py {posargs}\n",
      "'''\n",
      "\n",
      "[end of pyproject.toml]\n",
      "[start of .readthedocs.yml]\n",
      "version: 2\n",
      "\n",
      "formats:\n",
      "  - htmlzip\n",
      "  - pdf\n",
      "\n",
      "sphinx:\n",
      "  configuration: docs/conf.py\n",
      "\n",
      "build:\n",
      "  os: ubuntu-22.04\n",
      "  tools:\n",
      "    python: \"3.8\"\n",
      "\n",
      "python:\n",
      "   install:\n",
      "      - method: pip\n",
      "        path: .\n",
      "\n",
      "[end of .readthedocs.yml]\n",
      "[start of .pre-commit-config.yaml]\n",
      "exclude: ^src/wheel/vendored\n",
      "\n",
      "repos:\n",
      "- repo: https://github.com/pre-commit/pre-commit-hooks\n",
      "  rev: v5.0.0\n",
      "  hooks:\n",
      "  - id: check-added-large-files\n",
      "  - id: check-case-conflict\n",
      "  - id: check-merge-conflict\n",
      "  - id: check-symlinks\n",
      "  - id: check-toml\n",
      "  - id: check-yaml\n",
      "  - id: debug-statements\n",
      "  - id: end-of-file-fixer\n",
      "  - id: mixed-line-ending\n",
      "    args: [\"--fix=lf\"]\n",
      "  - id: trailing-whitespace\n",
      "    exclude: \"tests/cli/test_convert.py\"\n",
      "\n",
      "- repo: https://github.com/astral-sh/ruff-pre-commit\n",
      "  rev: v0.6.9\n",
      "  hooks:\n",
      "    - id: ruff\n",
      "      args: [--fix, --show-fixes]\n",
      "    - id: ruff-format\n",
      "\n",
      "- repo: https://github.com/codespell-project/codespell\n",
      "  rev: v2.3.0\n",
      "  hooks:\n",
      "  - id: codespell\n",
      "\n",
      "- repo: https://github.com/pre-commit/pygrep-hooks\n",
      "  rev: v1.10.0\n",
      "  hooks:\n",
      "  - id: rst-backticks\n",
      "  - id: rst-directive-colons\n",
      "  - id: rst-inline-touching-normal\n",
      "\n",
      "ci:\n",
      "  autoupdate_schedule: quarterly\n",
      "\n",
      "[end of .pre-commit-config.yaml]\n",
      "[start of README.rst]\n",
      "wheel\n",
      "=====\n",
      "\n",
      "This is a command line tool for manipulating Python wheel files, as defined in\n",
      "`PEP 427`_. It contains the following functionality:\n",
      "\n",
      "* Convert ``.egg`` archives into ``.whl``\n",
      "* Unpack wheel archives\n",
      "* Repack wheel archives\n",
      "* Add or remove tags in existing wheel archives\n",
      "\n",
      ".. _PEP 427: https://www.python.org/dev/peps/pep-0427/\n",
      "\n",
      "Historical note\n",
      "---------------\n",
      "\n",
      "This project used to contain the implementation of the setuptools_ ``bdist_wheel``\n",
      "command, but as of setuptools v70.1, it no longer needs ``wheel`` installed for that to\n",
      "work. Thus, you should install this **only** if you intend to use the ``wheel`` command\n",
      "line tool!\n",
      "\n",
      ".. _setuptools: https://pypi.org/project/setuptools/\n",
      "\n",
      "Documentation\n",
      "-------------\n",
      "\n",
      "The documentation_ can be found on Read The Docs.\n",
      "\n",
      ".. _documentation: https://wheel.readthedocs.io/\n",
      "\n",
      "Code of Conduct\n",
      "---------------\n",
      "\n",
      "Everyone interacting in the wheel project's codebases, issue trackers, chat\n",
      "rooms, and mailing lists is expected to follow the `PSF Code of Conduct`_.\n",
      "\n",
      ".. _PSF Code of Conduct: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md\n",
      "\n",
      "[end of README.rst]\n",
      "[start of LICENSE.txt]\n",
      "MIT License\n",
      "\n",
      "Copyright (c) 2012 Daniel Holth <dholth@fastmail.fm> and contributors\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a\n",
      "copy of this software and associated documentation files (the \"Software\"),\n",
      "to deal in the Software without restriction, including without limitation\n",
      "the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
      "and/or sell copies of the Software, and to permit persons to whom the\n",
      "Software is furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included\n",
      "in all copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
      "THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n",
      "OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n",
      "ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n",
      "OTHER DEALINGS IN THE SOFTWARE.\n",
      "\n",
      "[end of LICENSE.txt]\n",
      "[start of docs/user_guide.rst]\n",
      "User Guide\n",
      "==========\n",
      "\n",
      "Building Wheels\n",
      "---------------\n",
      "\n",
      "To build a wheel for your project::\n",
      "\n",
      "    python -m pip install build\n",
      "    python -m build --wheel\n",
      "\n",
      "This will build any C extensions in the project and then package those and the\n",
      "pure Python code into a ``.whl`` file in the ``dist`` directory.\n",
      "\n",
      "If your project contains no C extensions and is expected to work on both\n",
      "Python 2 and 3, you will want to tell wheel to produce universal wheels by\n",
      "adding this to your ``setup.cfg`` file:\n",
      "\n",
      ".. code-block:: ini\n",
      "\n",
      "    [bdist_wheel]\n",
      "    universal = 1\n",
      "\n",
      "\n",
      "Including license files in the generated wheel file\n",
      "---------------------------------------------------\n",
      "\n",
      "Several open source licenses require the license text to be included in every\n",
      "distributable artifact of the project. By default, ``wheel`` conveniently\n",
      "includes files matching the following glob_ patterns in the ``.dist-info``\n",
      "directory:\n",
      "\n",
      "* ``AUTHORS*``\n",
      "* ``COPYING*``\n",
      "* ``LICEN[CS]E*``\n",
      "* ``NOTICE*``\n",
      "\n",
      "This can be overridden by setting the ``license_files`` option in the\n",
      "``[metadata]`` section of the project's ``setup.cfg``. For example:\n",
      "\n",
      ".. code-block:: cfg\n",
      "\n",
      "   [metadata]\n",
      "   license_files =\n",
      "      license.txt\n",
      "      3rdparty/*.txt\n",
      "\n",
      "No matter the path, all the matching license files are written in the wheel in\n",
      "the ``.dist-info`` directory based on their file name only.\n",
      "\n",
      "By specifying an empty ``license_files`` option, you can disable this\n",
      "functionality entirely.\n",
      "\n",
      ".. note:: There used to be an option called ``license_file`` (singular).\n",
      "    As of wheel v0.32, this option has been deprecated in favor of the more\n",
      "    versatile ``license_files`` option.\n",
      "\n",
      ".. _glob: https://docs.python.org/library/glob.html\n",
      "\n",
      "Converting Eggs to Wheels\n",
      "-------------------------\n",
      "\n",
      "The wheel tool is capable of converting eggs to the wheel format.\n",
      "It works on both ``.egg`` files and ``.egg`` directories, and you can convert\n",
      "multiple eggs with a single command::\n",
      "\n",
      "    wheel convert blah-1.2.3-py2.7.egg foo-2.0b1-py3.5.egg\n",
      "\n",
      "The command supports wildcard expansion as well (via :func:`~glob.iglob`) to\n",
      "accommodate shells that do not do such expansion natively::\n",
      "\n",
      "    wheel convert *.egg\n",
      "\n",
      "By default, the resulting wheels are written to the current working directory.\n",
      "This can be changed with the ``--dest-dir`` option::\n",
      "\n",
      "    wheel convert --dest-dir /tmp blah-1.2.3-py2.7.egg\n",
      "\n",
      "Installing Wheels\n",
      "-----------------\n",
      "\n",
      "To install a wheel file, use pip_::\n",
      "\n",
      "    $ pip install someproject-1.5.0-py2-py3-none.whl\n",
      "\n",
      ".. _pip: https://pypi.org/project/pip/\n",
      "\n",
      "[end of docs/user_guide.rst]\n",
      "[start of docs/news.rst]\n",
      "Release Notes\n",
      "=============\n",
      "\n",
      "**UNRELEASED**\n",
      "\n",
      "- Fixed an exception when calling the ``convert`` command with an empty description\n",
      "  field\n",
      "\n",
      "**0.45.1 (2024-11-23)**\n",
      "\n",
      "- Fixed pure Python wheels converted from eggs and wininst files having the ABI tag in\n",
      "  the file name\n",
      "\n",
      "**0.45.0 (2024-11-08)**\n",
      "\n",
      "- Refactored the ``convert`` command to not need setuptools to be installed\n",
      "- Don't configure setuptools logging unless running ``bdist_wheel``\n",
      "- Added a redirection from ``wheel.bdist_wheel.bdist_wheel`` to\n",
      "  ``setuptools.command.bdist_wheel.bdist_wheel`` to improve compatibility with\n",
      "  ``setuptools``' latest fixes.\n",
      "\n",
      "  Projects are still advised to migrate away from the deprecated  module and import\n",
      "  the ``setuptools``' implementation explicitly. (PR by @abravalheri)\n",
      "\n",
      "**0.44.0 (2024-08-04)**\n",
      "\n",
      "- Canonicalized requirements in METADATA file (PR by Wim Jeantine-Glenn)\n",
      "- Deprecated the ``bdist_wheel`` module, as the code was migrated to ``setuptools``\n",
      "  itself\n",
      "\n",
      "**0.43.0 (2024-03-11)**\n",
      "\n",
      "- Dropped support for Python 3.7\n",
      "- Updated vendored ``packaging`` to 24.0\n",
      "\n",
      "**0.42.0 (2023-11-26)**\n",
      "\n",
      "- Allowed removing build tag with ``wheel tags --build \"\"``\n",
      "- Fixed ``wheel pack`` and ``wheel tags`` writing updated ``WHEEL`` fields after a\n",
      "  blank line, causing other tools to ignore them\n",
      "- Fixed ``wheel pack`` and ``wheel tags`` writing ``WHEEL`` with CRLF line endings or\n",
      "  a mix of CRLF and LF\n",
      "- Fixed ``wheel pack --build-number \"\"`` not removing build tag from ``WHEEL``\n",
      "  (above changes by Benjamin Gilbert)\n",
      "\n",
      "**0.41.3 (2023-10-30)**\n",
      "\n",
      "- Updated vendored ``packaging`` to 23.2\n",
      "- Fixed ABI tag generation for CPython 3.13a1 on Windows (PR by Sam Gross)\n",
      "\n",
      "**0.41.2 (2023-08-22)**\n",
      "\n",
      "- Fixed platform tag detection for GraalPy and 32-bit python running on an aarch64\n",
      "  kernel (PR by Matthieu Darbois)\n",
      "- Fixed ``wheel tags`` to not list directories in ``RECORD`` files\n",
      "  (PR by Mike Taves)\n",
      "- Fixed ABI tag generation for GraalPy (PR by Michael Simacek)\n",
      "\n",
      "**0.41.1 (2023-08-05)**\n",
      "\n",
      "- Fixed naming of the ``data_dir`` directory in the presence of local version segment\n",
      "  given via ``egg_info.tag_build`` (PR by Anderson Bravalheri)\n",
      "- Fixed version specifiers in ``Requires-Dist`` being wrapped in parentheses\n",
      "\n",
      "**0.41.0 (2023-07-22)**\n",
      "\n",
      "- Added full support of the build tag syntax to ``wheel tags`` (you can now set a build\n",
      "  tag like ``123mytag``)\n",
      "- Fixed warning on Python 3.12 about ``onerror`` deprecation. (PR by Henry Schreiner)\n",
      "- Support testing on Python 3.12 betas (PR by Ewout ter Hoeven)\n",
      "\n",
      "**0.40.0 (2023-03-14)**\n",
      "\n",
      "- Added a ``wheel tags`` command to modify tags on an existing wheel\n",
      "  (PR by Henry Schreiner)\n",
      "- Updated vendored ``packaging`` to 23.0\n",
      "- ``wheel unpack`` now preserves the executable attribute of extracted files\n",
      "- Fixed spaces in platform names not being converted to underscores (PR by David Tucker)\n",
      "- Fixed ``RECORD`` files in generated wheels missing the regular file attribute\n",
      "- Fixed ``DeprecationWarning`` about the use of the deprecated ``pkg_resources`` API\n",
      "  (PR by Thomas Grainger)\n",
      "- Wheel now uses flit-core as a build backend (PR by Henry Schreiner)\n",
      "\n",
      "**0.38.4 (2022-11-09)**\n",
      "\n",
      "- Fixed ``PKG-INFO`` conversion in ``bdist_wheel`` mangling UTF-8 header values in\n",
      "  ``METADATA`` (PR by Anderson Bravalheri)\n",
      "\n",
      "**0.38.3 (2022-11-08)**\n",
      "\n",
      "- Fixed install failure when used with ``--no-binary``, reported on Ubuntu 20.04, by\n",
      "  removing ``setup_requires`` from ``setup.cfg``\n",
      "\n",
      "**0.38.2 (2022-11-05)**\n",
      "\n",
      "- Fixed regression introduced in v0.38.1 which broke parsing of wheel file names with\n",
      "  multiple platform tags\n",
      "\n",
      "**0.38.1 (2022-11-04)**\n",
      "\n",
      "- Removed install dependency on setuptools\n",
      "- The future-proof fix in 0.36.0 for converting PyPy's SOABI into a abi tag was\n",
      "  faulty. Fixed so that future changes in the SOABI will not change the tag.\n",
      "\n",
      "**0.38.0 (2022-10-21)**\n",
      "\n",
      "- Dropped support for Python < 3.7\n",
      "- Updated vendored ``packaging`` to 21.3\n",
      "- Replaced all uses of ``distutils`` with ``setuptools``\n",
      "- The handling of ``license_files`` (including glob patterns and default\n",
      "  values) is now delegated to ``setuptools>=57.0.0`` (#466).\n",
      "  The package dependencies were updated to reflect this change.\n",
      "- Fixed potential DoS attack via the ``WHEEL_INFO_RE`` regular expression\n",
      "- Fixed ``ValueError: ZIP does not support timestamps before 1980`` when using\n",
      "  ``SOURCE_DATE_EPOCH=0`` or when on-disk timestamps are earlier than 1980-01-01. Such\n",
      "  timestamps are now changed to the minimum value before packaging.\n",
      "\n",
      "**0.37.1 (2021-12-22)**\n",
      "\n",
      "- Fixed ``wheel pack`` duplicating the ``WHEEL`` contents when the build number has\n",
      "  changed (#415)\n",
      "- Fixed parsing of file names containing commas in ``RECORD`` (PR by Hood Chatham)\n",
      "\n",
      "**0.37.0 (2021-08-09)**\n",
      "\n",
      "- Added official Python 3.10 support\n",
      "- Updated vendored ``packaging`` library to v20.9\n",
      "\n",
      "**0.36.2 (2020-12-13)**\n",
      "\n",
      "- Updated vendored ``packaging`` library to v20.8\n",
      "- Fixed wheel sdist missing ``LICENSE.txt``\n",
      "- Don't use default ``macos/arm64`` deployment target in calculating the\n",
      "  platform tag for fat binaries (PR by Ronald Oussoren)\n",
      "\n",
      "**0.36.1 (2020-12-04)**\n",
      "\n",
      "- Fixed ``AssertionError`` when ``MACOSX_DEPLOYMENT_TARGET`` was set to ``11``\n",
      "  (PR by Grzegorz Bokota and François-Xavier Coudert)\n",
      "- Fixed regression introduced in 0.36.0 on Python 2.7 when a custom generator\n",
      "  name was passed as unicode (Scikit-build)\n",
      "  (``TypeError: 'unicode' does not have the buffer interface``)\n",
      "\n",
      "**0.36.0 (2020-12-01)**\n",
      "\n",
      "- Added official Python 3.9 support\n",
      "- Updated vendored ``packaging`` library to v20.7\n",
      "- Switched to always using LF as line separator when generating ``WHEEL`` files\n",
      "  (on Windows, CRLF was being used instead)\n",
      "- The ABI tag is taken from  the sysconfig SOABI value. On PyPy the SOABI value\n",
      "  is ``pypy37-pp73`` which is not compliant with PEP 3149, as it should have\n",
      "  both the API tag and the platform tag. This change future-proofs any change\n",
      "  in PyPy's SOABI tag to make sure only the ABI tag is used by wheel.\n",
      "- Fixed regression and test for ``bdist_wheel --plat-name``. It was ignored for\n",
      "  C extensions in v0.35, but the regression was not detected by tests.\n",
      "\n",
      "**0.35.1 (2020-08-14)**\n",
      "\n",
      "- Replaced install dependency on ``packaging`` with a vendored copy of its\n",
      "  ``tags`` module\n",
      "- Fixed ``bdist_wheel`` not working on FreeBSD due to mismatching platform tag\n",
      "  name (it was not being converted to lowercase)\n",
      "\n",
      "**0.35.0 (2020-08-13)**\n",
      "\n",
      "- Switched to the packaging_ library for computing wheel tags\n",
      "- Fixed a resource leak in ``WheelFile.open()`` (PR by Jon Dufresne)\n",
      "\n",
      ".. _packaging: https://pypi.org/project/packaging/\n",
      "\n",
      "**0.34.2 (2020-01-30)**\n",
      "\n",
      "- Fixed installation of ``wheel`` from sdist on environments without Unicode\n",
      "  file name support\n",
      "\n",
      "**0.34.1 (2020-01-27)**\n",
      "\n",
      "- Fixed installation of ``wheel`` from sdist which was broken due to a chicken\n",
      "  and egg problem with PEP 517 and setuptools_scm\n",
      "\n",
      "**0.34.0 (2020-01-27)**\n",
      "\n",
      "- Dropped Python 3.4 support\n",
      "- Added automatic platform tag detection for macOS binary wheels\n",
      "  (PR by Grzegorz Bokota)\n",
      "- Added the ``--compression=`` option to the ``bdist_wheel`` command\n",
      "- Fixed PyPy tag generation to work with the updated semantics (#328)\n",
      "- Updated project packaging and testing configuration for :pep:`517`\n",
      "- Moved the contents of setup.py to setup.cfg\n",
      "- Fixed duplicate RECORD file when using ``wheel pack`` on Windows\n",
      "- Fixed bdist_wheel failing at cleanup on Windows with a read-only source tree\n",
      "- Fixed ``wheel pack`` not respecting the existing build tag in ``WHEEL``\n",
      "- Switched the project to use the \"src\" layout\n",
      "- Switched to setuptools_scm_ for versioning\n",
      "\n",
      " .. _setuptools_scm: https://github.com/pypa/setuptools_scm/\n",
      "\n",
      "**0.33.6 (2019-08-18)**\n",
      "\n",
      "- Fixed regression from 0.33.5 that broke building binary wheels against the\n",
      "  limited ABI\n",
      "- Fixed egg2wheel compatibility with the future release of Python 3.10\n",
      "  (PR by Anthony Sottile)\n",
      "\n",
      "**0.33.5 (2019-08-17)**\n",
      "\n",
      "- Don't add the ``m`` ABI flag to wheel names on Python 3.8 (PR by rdb)\n",
      "- Updated ``MANIFEST.in`` to include many previously omitted files in the sdist\n",
      "\n",
      "**0.33.4 (2019-05-12)**\n",
      "\n",
      "- Reverted PR #289 (adding directory entries to the wheel file) due to\n",
      "  incompatibility with ``distlib.wheel``\n",
      "\n",
      "**0.33.3 (2019-05-10)** (redacted release)\n",
      "\n",
      "- Fixed wheel build failures on some systems due to all attributes being\n",
      "  preserved (PR by Matt Wozniski)\n",
      "\n",
      "**0.33.2 (2019-05-08)** (redacted release)\n",
      "\n",
      "- Fixed empty directories missing from the wheel (PR by Jason R. Coombs)\n",
      "\n",
      "**0.33.1 (2019-02-19)**\n",
      "\n",
      "- Fixed the ``--build-number`` option for ``wheel pack`` not being applied\n",
      "\n",
      "**0.33.0 (2019-02-11)**\n",
      "\n",
      "- Added the ``--build-number`` option to the ``wheel pack`` command\n",
      "- Fixed bad shebangs sneaking into wheels\n",
      "- Fixed documentation issue with ``wheel pack`` erroneously being called\n",
      "  ``wheel repack``\n",
      "- Fixed filenames with \"bad\" characters (like commas) not being quoted in\n",
      "  ``RECORD`` (PR by Paul Moore)\n",
      "- Sort requirements extras to ensure deterministic builds\n",
      "  (PR by PoncinMatthieu)\n",
      "- Forced ``inplace = False`` when building a C extension for the wheel\n",
      "\n",
      "**0.32.3 (2018-11-18)**\n",
      "\n",
      "- Fixed compatibility with Python 2.7.0 – 2.7.3\n",
      "- Fixed handling of direct URL requirements with markers (PR by Benoit Pierre)\n",
      "\n",
      "**0.32.2 (2018-10-20)**\n",
      "\n",
      "- Fixed build number appearing in the ``.dist-info`` directory name\n",
      "- Made wheel file name parsing more permissive\n",
      "- Fixed wrong Python tag in wheels converted from eggs\n",
      "  (PR by John T. Wodder II)\n",
      "\n",
      "**0.32.1 (2018-10-03)**\n",
      "\n",
      "- Fixed ``AttributeError: 'Requirement' object has no attribute 'url'`` on\n",
      "  setuptools/pkg_resources versions older than 18.8 (PR by Benoit Pierre)\n",
      "- Fixed ``AttributeError: 'module' object has no attribute\n",
      "  'algorithms_available'`` on Python < 2.7.9 (PR by Benoit Pierre)\n",
      "- Fixed permissions on the generated ``.dist-info/RECORD`` file\n",
      "\n",
      "**0.32.0 (2018-09-29)**\n",
      "\n",
      "- Removed wheel signing and verifying features\n",
      "- Removed the \"wheel install\" and \"wheel installscripts\" commands\n",
      "- Added the ``wheel pack`` command\n",
      "- Allowed multiple license files to be specified using the ``license_files``\n",
      "  option\n",
      "- Deprecated the ``license_file`` option\n",
      "- Eliminated duplicate lines from generated requirements in\n",
      "  ``.dist-info/METADATA`` (thanks to Wim Glenn for the contribution)\n",
      "- Fixed handling of direct URL specifiers in requirements\n",
      "  (PR by Benoit Pierre)\n",
      "- Fixed canonicalization of extras (PR by Benoit Pierre)\n",
      "- Warn when the deprecated ``[wheel]`` section is used in ``setup.cfg``\n",
      "  (PR by Jon Dufresne)\n",
      "\n",
      "**0.31.1 (2018-05-13)**\n",
      "\n",
      "- Fixed arch as ``None`` when converting eggs to wheels\n",
      "\n",
      "**0.31.0 (2018-04-01)**\n",
      "\n",
      "- Fixed displaying of errors on Python 3\n",
      "- Fixed single digit versions in wheel files not being properly recognized\n",
      "- Fixed wrong character encodings being used (instead of UTF-8) to read and\n",
      "  write ``RECORD`` (this sometimes crashed bdist_wheel too)\n",
      "- Enabled Zip64 support in wheels by default\n",
      "- Metadata-Version is now 2.1\n",
      "- Dropped DESCRIPTION.rst and metadata.json from the list of generated files\n",
      "- Dropped support for the non-standard, undocumented ``provides-extra`` and\n",
      "  ``requires-dist`` keywords in setup.cfg metadata\n",
      "- Deprecated all wheel signing and signature verification commands\n",
      "- Removed the (already defunct) ``tool`` extras from setup.py\n",
      "\n",
      "**0.30.0 (2017-09-10)**\n",
      "\n",
      "- Added py-limited-api {cp32|cp33|cp34|...} flag to produce cpNN.abi3.{arch}\n",
      "  tags on CPython 3.\n",
      "- Documented the ``license_file`` metadata key\n",
      "- Improved Python, abi tagging for ``wheel convert``. Thanks Ales Erjavec.\n",
      "- Fixed ``>`` being prepended to lines starting with \"From\" in the long\n",
      "  description\n",
      "- Added support for specifying a build number (as per PEP 427).\n",
      "  Thanks Ian Cordasco.\n",
      "- Made the order of files in generated ZIP files deterministic.\n",
      "  Thanks Matthias Bach.\n",
      "- Made the order of requirements in metadata deterministic. Thanks Chris Lamb.\n",
      "- Fixed ``wheel install`` clobbering existing files\n",
      "- Improved the error message when trying to verify an unsigned wheel file\n",
      "- Removed support for Python 2.6, 3.2 and 3.3.\n",
      "\n",
      "**0.29.0 (2016-02-06)**\n",
      "\n",
      "- Fix compression type of files in archive (Issue #155, Pull Request #62,\n",
      "  thanks Xavier Fernandez)\n",
      "\n",
      "**0.28.0 (2016-02-05)**\n",
      "\n",
      "- Fix file modes in archive (Issue #154)\n",
      "\n",
      "**0.27.0 (2016-02-05)**\n",
      "\n",
      "- Support forcing a platform tag using ``--plat-name`` on pure-Python wheels,\n",
      "  as well as nonstandard platform tags on non-pure wheels (Pull Request #60,\n",
      "  Issue #144, thanks Andrés Díaz)\n",
      "- Add SOABI tags to platform-specific wheels built for Python 2.X (Pull Request\n",
      "  #55, Issue #63, Issue #101)\n",
      "- Support reproducible wheel files, wheels that can be rebuilt and will hash to\n",
      "  the same values as previous builds (Pull Request #52, Issue #143, thanks\n",
      "  Barry Warsaw)\n",
      "- Support for changes in keyring >= 8.0 (Pull Request #61, thanks Jason R.\n",
      "  Coombs)\n",
      "- Use the file context manager when checking if dependency_links.txt is empty,\n",
      "  fixes problems building wheels under PyPy on Windows  (Issue #150, thanks\n",
      "  Cosimo Lupo)\n",
      "- Don't attempt to (recursively) create a build directory ending with ``..``\n",
      "  (invalid on all platforms, but code was only executed on Windows) (Issue #91)\n",
      "- Added the PyPA Code of Conduct (Pull Request #56)\n",
      "\n",
      "**0.26.0 (2015-09-18)**\n",
      "\n",
      "- Fix multiple entrypoint comparison failure on Python 3 (Issue #148)\n",
      "\n",
      "**0.25.0 (2015-09-16)**\n",
      "\n",
      "- Add Python 3.5 to tox configuration\n",
      "- Deterministic (sorted) metadata\n",
      "- Fix tagging for Python 3.5 compatibility\n",
      "- Support py2-none-'arch' and py3-none-'arch' tags\n",
      "- Treat data-only wheels as pure\n",
      "- Write to temporary file and rename when using wheel install --force\n",
      "\n",
      "**0.24.0 (2014-07-06)**\n",
      "\n",
      "- The python tag used for pure-python packages is now .pyN (major version\n",
      "  only). This change actually occurred in 0.23.0 when the --python-tag\n",
      "  option was added, but was not explicitly mentioned in the changelog then.\n",
      "- wininst2wheel and egg2wheel removed. Use \"wheel convert [archive]\"\n",
      "  instead.\n",
      "- Wheel now supports setuptools style conditional requirements via the\n",
      "  extras_require={} syntax. Separate 'extra' names from conditions using\n",
      "  the : character. Wheel's own setup.py does this. (The empty-string\n",
      "  extra is the same as install_requires.) These conditional requirements\n",
      "  should work the same whether the package is installed by wheel or\n",
      "  by setup.py.\n",
      "\n",
      "**0.23.0 (2014-03-31)**\n",
      "\n",
      "- Compatibility tag flags added to the bdist_wheel command\n",
      "- sdist should include files necessary for tests\n",
      "- 'wheel convert' can now also convert unpacked eggs to wheel\n",
      "- Rename pydist.json to metadata.json to avoid stepping on the PEP\n",
      "- The --skip-scripts option has been removed, and not generating scripts is now\n",
      "  the default. The option was a temporary approach until installers could\n",
      "  generate scripts themselves. That is now the case with pip 1.5 and later.\n",
      "  Note that using pip 1.4 to install a wheel without scripts will leave the\n",
      "  installation without entry-point wrappers. The \"wheel install-scripts\"\n",
      "  command can be used to generate the scripts in such cases.\n",
      "- Thank you contributors\n",
      "\n",
      "**0.22.0 (2013-09-15)**\n",
      "\n",
      "- Include entry_points.txt, scripts a.k.a. commands, in experimental\n",
      "  pydist.json\n",
      "- Improved test_requires parsing\n",
      "- Python 2.6 fixes, \"wheel version\" command courtesy pombredanne\n",
      "\n",
      "**0.21.0 (2013-07-20)**\n",
      "\n",
      "- Pregenerated scripts are the default again.\n",
      "- \"setup.py bdist_wheel --skip-scripts\" turns them off.\n",
      "- setuptools is no longer a listed requirement for the 'wheel'\n",
      "  package. It is of course still required in order for bdist_wheel\n",
      "  to work.\n",
      "- \"python -m wheel\" avoids importing pkg_resources until it's necessary.\n",
      "\n",
      "**0.20.0**\n",
      "\n",
      "- No longer include console_scripts in wheels. Ordinary scripts (shell files,\n",
      "  standalone Python files) are included as usual.\n",
      "- Include new command \"python -m wheel install-scripts [distribution\n",
      "  [distribution ...]]\" to install the console_scripts (setuptools-style\n",
      "  scripts using pkg_resources) for a distribution.\n",
      "\n",
      "**0.19.0 (2013-07-19)**\n",
      "\n",
      "- pymeta.json becomes pydist.json\n",
      "\n",
      "**0.18.0 (2013-07-04)**\n",
      "\n",
      "- Python 3 Unicode improvements\n",
      "\n",
      "**0.17.0 (2013-06-23)**\n",
      "\n",
      "- Support latest PEP-426 \"pymeta.json\" (json-format metadata)\n",
      "\n",
      "**0.16.0 (2013-04-29)**\n",
      "\n",
      "- Python 2.6 compatibility bugfix (thanks John McFarlane)\n",
      "- Bugfix for C-extension tags for CPython 3.3 (using SOABI)\n",
      "- Bugfix for bdist_wininst converter \"wheel convert\"\n",
      "- Bugfix for dists where \"is pure\" is None instead of True or False\n",
      "- Python 3 fix for moving Unicode Description to metadata body\n",
      "- Include rudimentary API documentation in Sphinx (thanks Kevin Horn)\n",
      "\n",
      "**0.15.0 (2013-01-14)**\n",
      "\n",
      "- Various improvements\n",
      "\n",
      "**0.14.0 (2012-10-27)**\n",
      "\n",
      "- Changed the signature format to better comply with the current JWS spec.\n",
      "  Breaks all existing signatures.\n",
      "- Include ``wheel unsign`` command to remove RECORD.jws from an archive.\n",
      "- Put the description in the newly allowed payload section of PKG-INFO\n",
      "  (METADATA) files.\n",
      "\n",
      "**0.13.0 (2012-10-17)**\n",
      "\n",
      "- Use distutils instead of sysconfig to get installation paths; can install\n",
      "  headers.\n",
      "- Improve WheelFile() sort.\n",
      "- Allow bootstrap installs without any pkg_resources.\n",
      "\n",
      "**0.12.0 (2012-10-06)**\n",
      "\n",
      "- Unit test for wheel.tool.install\n",
      "\n",
      "**0.11.0 (2012-10-17)**\n",
      "\n",
      "- API cleanup\n",
      "\n",
      "**0.10.3 (2012-10-03)**\n",
      "\n",
      "- Scripts fixer fix\n",
      "\n",
      "**0.10.2 (2012-10-02)**\n",
      "\n",
      "- Fix keygen\n",
      "\n",
      "**0.10.1 (2012-09-30)**\n",
      "\n",
      "- Preserve attributes on install.\n",
      "\n",
      "**0.10.0 (2012-09-30)**\n",
      "\n",
      "- Include a copy of pkg_resources. Wheel can now install into a virtualenv\n",
      "  that does not have distribute (though most packages still require\n",
      "  pkg_resources to actually work; wheel install distribute)\n",
      "- Define a new setup.cfg section [wheel]. universal=1 will\n",
      "  apply the py2.py3-none-any tag for pure python wheels.\n",
      "\n",
      "**0.9.7 (2012-09-20)**\n",
      "\n",
      "- Only import dirspec when needed. dirspec is only needed to find the\n",
      "  configuration for keygen/signing operations.\n",
      "\n",
      "**0.9.6 (2012-09-19)**\n",
      "\n",
      "- requires-dist from setup.cfg overwrites any requirements from setup.py\n",
      "  Care must be taken that the requirements are the same in both cases,\n",
      "  or just always install from wheel.\n",
      "- drop dirspec requirement on win32\n",
      "- improved command line utility, adds 'wheel convert [egg or wininst]' to\n",
      "  convert legacy binary formats to wheel\n",
      "\n",
      "**0.9.5 (2012-09-15)**\n",
      "\n",
      "- Wheel's own wheel file can be executed by Python, and can install itself:\n",
      "  ``python wheel-0.9.5-py27-none-any/wheel install ...``\n",
      "- Use argparse; basic ``wheel install`` command should run with only stdlib\n",
      "  dependencies.\n",
      "- Allow requires_dist in setup.cfg's [metadata] section. In addition to\n",
      "  dependencies in setup.py, but will only be interpreted when installing\n",
      "  from wheel, not from sdist. Can be qualified with environment markers.\n",
      "\n",
      "**0.9.4 (2012-09-11)**\n",
      "\n",
      "- Fix wheel.signatures in sdist\n",
      "\n",
      "**0.9.3 (2012-09-10)**\n",
      "\n",
      "- Integrated digital signatures support without C extensions.\n",
      "- Integrated \"wheel install\" command (single package, no dependency\n",
      "  resolution) including compatibility check.\n",
      "- Support Python 3.3\n",
      "- Use Metadata 1.3 (PEP 426)\n",
      "\n",
      "**0.9.2 (2012-08-29)**\n",
      "\n",
      "- Automatic signing if WHEEL_TOOL points to the wheel binary\n",
      "- Even more Python 3 fixes\n",
      "\n",
      "**0.9.1 (2012-08-28)**\n",
      "\n",
      "- 'wheel sign' uses the keys generated by 'wheel keygen' (instead of generating\n",
      "  a new key at random each time)\n",
      "- Python 2/3 encoding/decoding fixes\n",
      "- Run tests on Python 2.6 (without signature verification)\n",
      "\n",
      "**0.9 (2012-08-22)**\n",
      "\n",
      "- Updated digital signatures scheme\n",
      "- Python 3 support for digital signatures\n",
      "- Always verify RECORD hashes on extract\n",
      "- \"wheel\" command line tool to sign, verify, unpack wheel files\n",
      "\n",
      "**0.8 (2012-08-17)**\n",
      "\n",
      "- none/any draft pep tags update\n",
      "- improved wininst2wheel script\n",
      "- doc changes and other improvements\n",
      "\n",
      "**0.7 (2012-07-28)**\n",
      "\n",
      "- sort .dist-info at end of wheel archive\n",
      "- Windows & Python 3 fixes from Paul Moore\n",
      "- pep8\n",
      "- scripts to convert wininst & egg to wheel\n",
      "\n",
      "**0.6 (2012-07-23)**\n",
      "\n",
      "- require distribute >= 0.6.28\n",
      "- stop using verlib\n",
      "\n",
      "**0.5 (2012-07-17)**\n",
      "\n",
      "- working pretty well\n",
      "\n",
      "**0.4.2 (2012-07-12)**\n",
      "\n",
      "- hyphenated name fix\n",
      "\n",
      "**0.4 (2012-07-11)**\n",
      "\n",
      "- improve test coverage\n",
      "- improve Windows compatibility\n",
      "- include tox.ini courtesy of Marc Abramowitz\n",
      "- draft hmac sha-256 signing function\n",
      "\n",
      "**0.3 (2012-07-04)**\n",
      "\n",
      "- prototype egg2wheel conversion script\n",
      "\n",
      "**0.2 (2012-07-03)**\n",
      "\n",
      "- Python 3 compatibility\n",
      "\n",
      "**0.1 (2012-06-30)**\n",
      "\n",
      "- Initial version\n",
      "\n",
      "[end of docs/news.rst]\n",
      "[start of docs/make.bat]\n",
      "@ECHO OFF\n",
      "\n",
      "REM Command file for Sphinx documentation\n",
      "\n",
      "if \"%SPHINXBUILD%\" == \"\" (\n",
      "\tset SPHINXBUILD=sphinx-build\n",
      ")\n",
      "set BUILDDIR=_build\n",
      "set ALLSPHINXOPTS=-d %BUILDDIR%/doctrees %SPHINXOPTS% .\n",
      "set I18NSPHINXOPTS=%SPHINXOPTS% .\n",
      "if NOT \"%PAPER%\" == \"\" (\n",
      "\tset ALLSPHINXOPTS=-D latex_paper_size=%PAPER% %ALLSPHINXOPTS%\n",
      "\tset I18NSPHINXOPTS=-D latex_paper_size=%PAPER% %I18NSPHINXOPTS%\n",
      ")\n",
      "\n",
      "if \"%1\" == \"\" goto help\n",
      "\n",
      "if \"%1\" == \"help\" (\n",
      "\t:help\n",
      "\techo.Please use `make ^<target^>` where ^<target^> is one of\n",
      "\techo.  html       to make standalone HTML files\n",
      "\techo.  dirhtml    to make HTML files named index.html in directories\n",
      "\techo.  singlehtml to make a single large HTML file\n",
      "\techo.  pickle     to make pickle files\n",
      "\techo.  json       to make JSON files\n",
      "\techo.  htmlhelp   to make HTML files and a HTML help project\n",
      "\techo.  qthelp     to make HTML files and a qthelp project\n",
      "\techo.  devhelp    to make HTML files and a Devhelp project\n",
      "\techo.  epub       to make an epub\n",
      "\techo.  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter\n",
      "\techo.  text       to make text files\n",
      "\techo.  man        to make manual pages\n",
      "\techo.  texinfo    to make Texinfo files\n",
      "\techo.  gettext    to make PO message catalogs\n",
      "\techo.  changes    to make an overview over all changed/added/deprecated items\n",
      "\techo.  linkcheck  to check all external links for integrity\n",
      "\techo.  doctest    to run all doctests embedded in the documentation if enabled\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"clean\" (\n",
      "\tfor /d %%i in (%BUILDDIR%\\*) do rmdir /q /s %%i\n",
      "\tdel /q /s %BUILDDIR%\\*\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"html\" (\n",
      "\t%SPHINXBUILD% -b html %ALLSPHINXOPTS% %BUILDDIR%/html\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished. The HTML pages are in %BUILDDIR%/html.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"dirhtml\" (\n",
      "\t%SPHINXBUILD% -b dirhtml %ALLSPHINXOPTS% %BUILDDIR%/dirhtml\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished. The HTML pages are in %BUILDDIR%/dirhtml.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"singlehtml\" (\n",
      "\t%SPHINXBUILD% -b singlehtml %ALLSPHINXOPTS% %BUILDDIR%/singlehtml\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished. The HTML pages are in %BUILDDIR%/singlehtml.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"pickle\" (\n",
      "\t%SPHINXBUILD% -b pickle %ALLSPHINXOPTS% %BUILDDIR%/pickle\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished; now you can process the pickle files.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"json\" (\n",
      "\t%SPHINXBUILD% -b json %ALLSPHINXOPTS% %BUILDDIR%/json\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished; now you can process the JSON files.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"htmlhelp\" (\n",
      "\t%SPHINXBUILD% -b htmlhelp %ALLSPHINXOPTS% %BUILDDIR%/htmlhelp\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished; now you can run HTML Help Workshop with the ^\n",
      ".hhp project file in %BUILDDIR%/htmlhelp.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"qthelp\" (\n",
      "\t%SPHINXBUILD% -b qthelp %ALLSPHINXOPTS% %BUILDDIR%/qthelp\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished; now you can run \"qcollectiongenerator\" with the ^\n",
      ".qhcp project file in %BUILDDIR%/qthelp, like this:\n",
      "\techo.^> qcollectiongenerator %BUILDDIR%\\qthelp\\wheel.qhcp\n",
      "\techo.To view the help file:\n",
      "\techo.^> assistant -collectionFile %BUILDDIR%\\qthelp\\wheel.ghc\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"devhelp\" (\n",
      "\t%SPHINXBUILD% -b devhelp %ALLSPHINXOPTS% %BUILDDIR%/devhelp\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"epub\" (\n",
      "\t%SPHINXBUILD% -b epub %ALLSPHINXOPTS% %BUILDDIR%/epub\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished. The epub file is in %BUILDDIR%/epub.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"latex\" (\n",
      "\t%SPHINXBUILD% -b latex %ALLSPHINXOPTS% %BUILDDIR%/latex\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished; the LaTeX files are in %BUILDDIR%/latex.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"text\" (\n",
      "\t%SPHINXBUILD% -b text %ALLSPHINXOPTS% %BUILDDIR%/text\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished. The text files are in %BUILDDIR%/text.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"man\" (\n",
      "\t%SPHINXBUILD% -b man %ALLSPHINXOPTS% %BUILDDIR%/man\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished. The manual pages are in %BUILDDIR%/man.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"texinfo\" (\n",
      "\t%SPHINXBUILD% -b texinfo %ALLSPHINXOPTS% %BUILDDIR%/texinfo\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished. The Texinfo files are in %BUILDDIR%/texinfo.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"gettext\" (\n",
      "\t%SPHINXBUILD% -b gettext %I18NSPHINXOPTS% %BUILDDIR%/locale\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Build finished. The message catalogs are in %BUILDDIR%/locale.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"changes\" (\n",
      "\t%SPHINXBUILD% -b changes %ALLSPHINXOPTS% %BUILDDIR%/changes\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.The overview file is in %BUILDDIR%/changes.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"linkcheck\" (\n",
      "\t%SPHINXBUILD% -b linkcheck %ALLSPHINXOPTS% %BUILDDIR%/linkcheck\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Link check complete; look for any errors in the above output ^\n",
      "or in %BUILDDIR%/linkcheck/output.txt.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      "if \"%1\" == \"doctest\" (\n",
      "\t%SPHINXBUILD% -b doctest %ALLSPHINXOPTS% %BUILDDIR%/doctest\n",
      "\tif errorlevel 1 exit /b 1\n",
      "\techo.\n",
      "\techo.Testing of doctests in the sources finished, look at the ^\n",
      "results in %BUILDDIR%/doctest/output.txt.\n",
      "\tgoto end\n",
      ")\n",
      "\n",
      ":end\n",
      "\n",
      "[end of docs/make.bat]\n",
      "[start of docs/index.rst]\n",
      "wheel\n",
      "=====\n",
      "\n",
      "`GitHub <https://github.com/pypa/wheel>`_ |\n",
      "`PyPI <https://pypi.org/pypi/wheel/>`_ |\n",
      "User IRC: #pypa |\n",
      "Dev IRC: #pypa-dev\n",
      "\n",
      "This library is the reference implementation of the Python wheel packaging\n",
      "standard, as defined in `PEP 427`_.\n",
      "\n",
      "It has two different roles:\n",
      "\n",
      "#. A setuptools_ extension for building wheels that provides the\n",
      "   ``bdist_wheel`` setuptools command\n",
      "#. A command line tool for working with wheel files\n",
      "\n",
      ".. _PEP 427: https://www.python.org/dev/peps/pep-0427/\n",
      ".. _setuptools: https://pypi.org/project/setuptools/\n",
      "\n",
      ".. toctree::\n",
      "   :maxdepth: 2\n",
      "\n",
      "   quickstart\n",
      "   installing\n",
      "   user_guide\n",
      "   reference/index\n",
      "   development\n",
      "   news\n",
      "\n",
      "[end of docs/index.rst]\n",
      "[start of docs/Makefile]\n",
      "# Makefile for Sphinx documentation\n",
      "#\n",
      "\n",
      "# You can set these variables from the command line.\n",
      "SPHINXOPTS    =\n",
      "SPHINXBUILD   = sphinx-build\n",
      "PAPER         =\n",
      "BUILDDIR      = _build\n",
      "\n",
      "# Internal variables.\n",
      "PAPEROPT_a4     = -D latex_paper_size=a4\n",
      "PAPEROPT_letter = -D latex_paper_size=letter\n",
      "ALLSPHINXOPTS   = -d $(BUILDDIR)/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) .\n",
      "# the i18n builder cannot share the environment and doctrees with the others\n",
      "I18NSPHINXOPTS  = $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) .\n",
      "\n",
      ".PHONY: help clean html dirhtml singlehtml pickle json htmlhelp qthelp devhelp epub latex latexpdf text man changes linkcheck doctest gettext\n",
      "\n",
      "help:\n",
      "\t@echo \"Please use \\`make <target>' where <target> is one of\"\n",
      "\t@echo \"  html       to make standalone HTML files\"\n",
      "\t@echo \"  dirhtml    to make HTML files named index.html in directories\"\n",
      "\t@echo \"  singlehtml to make a single large HTML file\"\n",
      "\t@echo \"  pickle     to make pickle files\"\n",
      "\t@echo \"  json       to make JSON files\"\n",
      "\t@echo \"  htmlhelp   to make HTML files and a HTML help project\"\n",
      "\t@echo \"  qthelp     to make HTML files and a qthelp project\"\n",
      "\t@echo \"  devhelp    to make HTML files and a Devhelp project\"\n",
      "\t@echo \"  epub       to make an epub\"\n",
      "\t@echo \"  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter\"\n",
      "\t@echo \"  latexpdf   to make LaTeX files and run them through pdflatex\"\n",
      "\t@echo \"  text       to make text files\"\n",
      "\t@echo \"  man        to make manual pages\"\n",
      "\t@echo \"  texinfo    to make Texinfo files\"\n",
      "\t@echo \"  info       to make Texinfo files and run them through makeinfo\"\n",
      "\t@echo \"  gettext    to make PO message catalogs\"\n",
      "\t@echo \"  changes    to make an overview of all changed/added/deprecated items\"\n",
      "\t@echo \"  linkcheck  to check all external links for integrity\"\n",
      "\t@echo \"  doctest    to run all doctests embedded in the documentation (if enabled)\"\n",
      "\n",
      "clean:\n",
      "\t-rm -rf $(BUILDDIR)/*\n",
      "\n",
      "html:\n",
      "\t$(SPHINXBUILD) -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html\n",
      "\t@echo\n",
      "\t@echo \"Build finished. The HTML pages are in $(BUILDDIR)/html.\"\n",
      "\n",
      "dirhtml:\n",
      "\t$(SPHINXBUILD) -b dirhtml $(ALLSPHINXOPTS) $(BUILDDIR)/dirhtml\n",
      "\t@echo\n",
      "\t@echo \"Build finished. The HTML pages are in $(BUILDDIR)/dirhtml.\"\n",
      "\n",
      "singlehtml:\n",
      "\t$(SPHINXBUILD) -b singlehtml $(ALLSPHINXOPTS) $(BUILDDIR)/singlehtml\n",
      "\t@echo\n",
      "\t@echo \"Build finished. The HTML page is in $(BUILDDIR)/singlehtml.\"\n",
      "\n",
      "pickle:\n",
      "\t$(SPHINXBUILD) -b pickle $(ALLSPHINXOPTS) $(BUILDDIR)/pickle\n",
      "\t@echo\n",
      "\t@echo \"Build finished; now you can process the pickle files.\"\n",
      "\n",
      "json:\n",
      "\t$(SPHINXBUILD) -b json $(ALLSPHINXOPTS) $(BUILDDIR)/json\n",
      "\t@echo\n",
      "\t@echo \"Build finished; now you can process the JSON files.\"\n",
      "\n",
      "htmlhelp:\n",
      "\t$(SPHINXBUILD) -b htmlhelp $(ALLSPHINXOPTS) $(BUILDDIR)/htmlhelp\n",
      "\t@echo\n",
      "\t@echo \"Build finished; now you can run HTML Help Workshop with the\" \\\n",
      "\t      \".hhp project file in $(BUILDDIR)/htmlhelp.\"\n",
      "\n",
      "qthelp:\n",
      "\t$(SPHINXBUILD) -b qthelp $(ALLSPHINXOPTS) $(BUILDDIR)/qthelp\n",
      "\t@echo\n",
      "\t@echo \"Build finished; now you can run \"qcollectiongenerator\" with the\" \\\n",
      "\t      \".qhcp project file in $(BUILDDIR)/qthelp, like this:\"\n",
      "\t@echo \"# qcollectiongenerator $(BUILDDIR)/qthelp/wheel.qhcp\"\n",
      "\t@echo \"To view the help file:\"\n",
      "\t@echo \"# assistant -collectionFile $(BUILDDIR)/qthelp/wheel.qhc\"\n",
      "\n",
      "devhelp:\n",
      "\t$(SPHINXBUILD) -b devhelp $(ALLSPHINXOPTS) $(BUILDDIR)/devhelp\n",
      "\t@echo\n",
      "\t@echo \"Build finished.\"\n",
      "\t@echo \"To view the help file:\"\n",
      "\t@echo \"# mkdir -p $$HOME/.local/share/devhelp/wheel\"\n",
      "\t@echo \"# ln -s $(BUILDDIR)/devhelp $$HOME/.local/share/devhelp/wheel\"\n",
      "\t@echo \"# devhelp\"\n",
      "\n",
      "epub:\n",
      "\t$(SPHINXBUILD) -b epub $(ALLSPHINXOPTS) $(BUILDDIR)/epub\n",
      "\t@echo\n",
      "\t@echo \"Build finished. The epub file is in $(BUILDDIR)/epub.\"\n",
      "\n",
      "latex:\n",
      "\t$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex\n",
      "\t@echo\n",
      "\t@echo \"Build finished; the LaTeX files are in $(BUILDDIR)/latex.\"\n",
      "\t@echo \"Run \\`make' in that directory to run these through (pdf)latex\" \\\n",
      "\t      \"(use \\`make latexpdf' here to do that automatically).\"\n",
      "\n",
      "latexpdf:\n",
      "\t$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex\n",
      "\t@echo \"Running LaTeX files through pdflatex...\"\n",
      "\t$(MAKE) -C $(BUILDDIR)/latex all-pdf\n",
      "\t@echo \"pdflatex finished; the PDF files are in $(BUILDDIR)/latex.\"\n",
      "\n",
      "text:\n",
      "\t$(SPHINXBUILD) -b text $(ALLSPHINXOPTS) $(BUILDDIR)/text\n",
      "\t@echo\n",
      "\t@echo \"Build finished. The text files are in $(BUILDDIR)/text.\"\n",
      "\n",
      "man:\n",
      "\t$(SPHINXBUILD) -b man $(ALLSPHINXOPTS) $(BUILDDIR)/man\n",
      "\t@echo\n",
      "\t@echo \"Build finished. The manual pages are in $(BUILDDIR)/man.\"\n",
      "\n",
      "texinfo:\n",
      "\t$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo\n",
      "\t@echo\n",
      "\t@echo \"Build finished. The Texinfo files are in $(BUILDDIR)/texinfo.\"\n",
      "\t@echo \"Run \\`make' in that directory to run these through makeinfo\" \\\n",
      "\t      \"(use \\`make info' here to do that automatically).\"\n",
      "\n",
      "info:\n",
      "\t$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo\n",
      "\t@echo \"Running Texinfo files through makeinfo...\"\n",
      "\tmake -C $(BUILDDIR)/texinfo info\n",
      "\t@echo \"makeinfo finished; the Info files are in $(BUILDDIR)/texinfo.\"\n",
      "\n",
      "gettext:\n",
      "\t$(SPHINXBUILD) -b gettext $(I18NSPHINXOPTS) $(BUILDDIR)/locale\n",
      "\t@echo\n",
      "\t@echo \"Build finished. The message catalogs are in $(BUILDDIR)/locale.\"\n",
      "\n",
      "changes:\n",
      "\t$(SPHINXBUILD) -b changes $(ALLSPHINXOPTS) $(BUILDDIR)/changes\n",
      "\t@echo\n",
      "\t@echo \"The overview file is in $(BUILDDIR)/changes.\"\n",
      "\n",
      "linkcheck:\n",
      "\t$(SPHINXBUILD) -b linkcheck $(ALLSPHINXOPTS) $(BUILDDIR)/linkcheck\n",
      "\t@echo\n",
      "\t@echo \"Link check complete; look for any errors in the above output \" \\\n",
      "\t      \"or in $(BUILDDIR)/linkcheck/output.txt.\"\n",
      "\n",
      "doctest:\n",
      "\t$(SPHINXBUILD) -b doctest $(ALLSPHINXOPTS) $(BUILDDIR)/doctest\n",
      "\t@echo \"Testing of doctests in the sources finished, look at the \" \\\n",
      "\t      \"results in $(BUILDDIR)/doctest/output.txt.\"\n",
      "\n",
      "[end of docs/Makefile]\n",
      "[start of docs/conf.py]\n",
      "#\n",
      "# wheel documentation build configuration file, created by\n",
      "# sphinx-quickstart on Thu Jul 12 00:14:09 2012.\n",
      "#\n",
      "# This file is execfile()d with the current directory set to its containing dir.\n",
      "#\n",
      "# Note that not all possible configuration values are present in this\n",
      "# autogenerated file.\n",
      "#\n",
      "# All configuration values have a default; values that are commented out\n",
      "# serve to show the default.\n",
      "from __future__ import annotations\n",
      "\n",
      "import os\n",
      "import re\n",
      "\n",
      "# If extensions (or modules to document with autodoc) are in another directory,\n",
      "# add these directories to sys.path here. If the directory is relative to the\n",
      "# documentation root, use os.path.abspath to make it absolute, like shown here.\n",
      "# sys.path.insert(0, os.path.abspath('.'))\n",
      "\n",
      "# -- General configuration -----------------------------------------------------\n",
      "\n",
      "# If your documentation needs a minimal Sphinx version, state it here.\n",
      "# needs_sphinx = '1.0'\n",
      "\n",
      "# Add any Sphinx extension module names here, as strings. They can be extensions\n",
      "# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n",
      "extensions = [\"sphinx.ext.intersphinx\"]\n",
      "\n",
      "# Add any paths that contain templates here, relative to this directory.\n",
      "templates_path = [\"_templates\"]\n",
      "\n",
      "# The suffix of source filenames.\n",
      "source_suffix = \".rst\"\n",
      "\n",
      "# The encoding of source files.\n",
      "# source_encoding = 'utf-8-sig'\n",
      "\n",
      "# The master toctree document.\n",
      "master_doc = \"index\"\n",
      "\n",
      "# General information about the project.\n",
      "project = \"wheel\"\n",
      "copyright = \"2012, Daniel Holth\"\n",
      "\n",
      "# The version info for the project you're documenting, acts as replacement for\n",
      "# |version| and |release|, also used in various other places throughout the\n",
      "# built documents.\n",
      "#\n",
      "here = os.path.abspath(os.path.dirname(__file__))\n",
      "with open(\n",
      "    os.path.join(here, \"..\", \"src\", \"wheel\", \"__init__.py\"), encoding=\"utf8\"\n",
      ") as version_file:\n",
      "    match = re.search(r'__version__ = \"((\\d+\\.\\d+\\.\\d+).*)\"', version_file.read())\n",
      "    # The short X.Y version.\n",
      "    version = match.group(2)\n",
      "\n",
      "    # The full version, including alpha/beta/rc tags.\n",
      "    release = match.group(1)\n",
      "\n",
      "# The language for content autogenerated by Sphinx. Refer to documentation\n",
      "# for a list of supported languages.\n",
      "# language = None\n",
      "\n",
      "# There are two options for replacing |today|: either, you set today to some\n",
      "# non-false value, then it is used:\n",
      "# today = ''\n",
      "# Else, today_fmt is used as the format for a strftime call.\n",
      "# today_fmt = '%B %d, %Y'\n",
      "\n",
      "# List of patterns, relative to source directory, that match files and\n",
      "# directories to ignore when looking for source files.\n",
      "exclude_patterns = [\"_build\"]\n",
      "\n",
      "# The reST default role (used for this markup: `text`) to use for all documents.\n",
      "# default_role = None\n",
      "\n",
      "# If true, '()' will be appended to :func: etc. cross-reference text.\n",
      "# add_function_parentheses = True\n",
      "\n",
      "# If true, the current module name will be prepended to all description\n",
      "# unit titles (such as .. function::).\n",
      "# add_module_names = True\n",
      "\n",
      "# If true, sectionauthor and moduleauthor directives will be shown in the\n",
      "# output. They are ignored by default.\n",
      "# show_authors = False\n",
      "\n",
      "# The name of the Pygments (syntax highlighting) style to use.\n",
      "pygments_style = \"sphinx\"\n",
      "highlight_language = \"bash\"\n",
      "\n",
      "# A list of ignored prefixes for module index sorting.\n",
      "# modindex_common_prefix = []\n",
      "\n",
      "intersphinx_mapping = {\"python\": (\"https://docs.python.org/\", None)}\n",
      "\n",
      "\n",
      "# -- Options for HTML output ---------------------------------------------------\n",
      "\n",
      "# The theme to use for HTML and HTML Help pages.  See the documentation for\n",
      "# a list of builtin themes.\n",
      "html_theme = \"default\"\n",
      "\n",
      "# Theme options are theme-specific and customize the look and feel of a theme\n",
      "# further.  For a list of options available for each theme, see the\n",
      "# documentation.\n",
      "# html_theme_options = {}\n",
      "\n",
      "# Add any paths that contain custom themes here, relative to this directory.\n",
      "# html_theme_path = []\n",
      "\n",
      "# The name for this set of Sphinx documents.  If None, it defaults to\n",
      "# \"<project> v<release> documentation\".\n",
      "# html_title = None\n",
      "\n",
      "# A shorter title for the navigation bar.  Default is the same as html_title.\n",
      "# html_short_title = None\n",
      "\n",
      "# The name of an image file (relative to this directory) to place at the top\n",
      "# of the sidebar.\n",
      "# html_logo = None\n",
      "\n",
      "# The name of an image file (within the static path) to use as favicon of the\n",
      "# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n",
      "# pixels large.\n",
      "# html_favicon = None\n",
      "\n",
      "# Add any paths that contain custom static files (such as style sheets) here,\n",
      "# relative to this directory. They are copied after the builtin static files,\n",
      "# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n",
      "# html_static_path = ['_static']\n",
      "\n",
      "# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n",
      "# using the given strftime format.\n",
      "# html_last_updated_fmt = '%b %d, %Y'\n",
      "\n",
      "# If true, SmartyPants will be used to convert quotes and dashes to\n",
      "# typographically correct entities.\n",
      "# html_use_smartypants = True\n",
      "\n",
      "# Custom sidebar templates, maps document names to template names.\n",
      "# html_sidebars = {}\n",
      "\n",
      "# Additional templates that should be rendered to pages, maps page names to\n",
      "# template names.\n",
      "# html_additional_pages = {}\n",
      "\n",
      "# If false, no module index is generated.\n",
      "# html_domain_indices = True\n",
      "\n",
      "# If false, no index is generated.\n",
      "# html_use_index = True\n",
      "\n",
      "# If true, the index is split into individual pages for each letter.\n",
      "# html_split_index = False\n",
      "\n",
      "# If true, links to the reST sources are added to the pages.\n",
      "# html_show_sourcelink = True\n",
      "\n",
      "# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n",
      "# html_show_sphinx = True\n",
      "\n",
      "# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n",
      "# html_show_copyright = True\n",
      "\n",
      "# If true, an OpenSearch description file will be output, and all pages will\n",
      "# contain a <link> tag referring to it.  The value of this option must be the\n",
      "# base URL from which the finished HTML is served.\n",
      "# html_use_opensearch = ''\n",
      "\n",
      "# This is the file name suffix for HTML files (e.g. \".xhtml\").\n",
      "# html_file_suffix = None\n",
      "\n",
      "# Output file base name for HTML help builder.\n",
      "htmlhelp_basename = \"wheeldoc\"\n",
      "\n",
      "\n",
      "# -- Options for LaTeX output --------------------------------------------------\n",
      "\n",
      "latex_elements = {\n",
      "    # The paper size ('letterpaper' or 'a4paper').\n",
      "    # 'papersize': 'letterpaper',\n",
      "    # The font size ('10pt', '11pt' or '12pt').\n",
      "    # 'pointsize': '10pt',\n",
      "    # Additional stuff for the LaTeX preamble.\n",
      "    # 'preamble': '',\n",
      "}\n",
      "\n",
      "# Grouping the document tree into LaTeX files. List of tuples\n",
      "# (source start file, target name, title, author, documentclass [howto/manual]).\n",
      "latex_documents = [\n",
      "    (\"index\", \"wheel.tex\", \"wheel Documentation\", \"Daniel Holth\", \"manual\"),\n",
      "]\n",
      "\n",
      "# The name of an image file (relative to this directory) to place at the top of\n",
      "# the title page.\n",
      "# latex_logo = None\n",
      "\n",
      "# For \"manual\" documents, if this is true, then toplevel headings are parts,\n",
      "# not chapters.\n",
      "# latex_use_parts = False\n",
      "\n",
      "# If true, show page references after internal links.\n",
      "# latex_show_pagerefs = False\n",
      "\n",
      "# If true, show URL addresses after external links.\n",
      "# latex_show_urls = False\n",
      "\n",
      "# Documents to append as an appendix to all manuals.\n",
      "# latex_appendices = []\n",
      "\n",
      "# If false, no module index is generated.\n",
      "# latex_domain_indices = True\n",
      "\n",
      "\n",
      "# -- Options for manual page output --------------------------------------------\n",
      "\n",
      "# One entry per manual page. List of tuples\n",
      "# (source start file, name, description, authors, manual section).\n",
      "man_pages = [\n",
      "    (\"manpages/wheel\", \"wheel\", \"wheel Documentation\", [\"Daniel Holth\"], 1),\n",
      "]\n",
      "\n",
      "# If true, show URL addresses after external links.\n",
      "# man_show_urls = False\n",
      "\n",
      "\n",
      "# -- Options for Texinfo output ------------------------------------------------\n",
      "\n",
      "# Grouping the document tree into Texinfo files. List of tuples\n",
      "# (source start file, target name, title, author,\n",
      "#  dir menu entry, description, category)\n",
      "texinfo_documents = [\n",
      "    (\n",
      "        \"index\",\n",
      "        \"wheel\",\n",
      "        \"wheel Documentation\",\n",
      "        \"Daniel Holth\",\n",
      "        \"wheel\",\n",
      "        \"One line description of project.\",\n",
      "        \"Miscellaneous\",\n",
      "    ),\n",
      "]\n",
      "\n",
      "# Documents to append as an appendix to all manuals.\n",
      "# texinfo_appendices = []\n",
      "\n",
      "# If false, no module index is generated.\n",
      "# texinfo_domain_indices = True\n",
      "\n",
      "# How to display URL addresses: 'footnote', 'no', or 'inline'.\n",
      "# texinfo_show_urls = 'footnote'\n",
      "\n",
      "[end of docs/conf.py]\n",
      "[start of docs/story.rst]\n",
      "The Story of Wheel\n",
      "==================\n",
      "\n",
      "I was impressed with Tarek’s packaging talk at PyCon 2010, and I\n",
      "admire PEP 345 (Metadata for Python Software Packages 1.2) and PEP 376\n",
      "(Database of Installed Python Distributions) which standardize a richer\n",
      "metadata format and show how distributions should be installed on disk. So\n",
      "naturally with all the hubbub about ``packaging`` in Python 3.3, I decided\n",
      "to try it to reap the benefits of a more standardized and predictable\n",
      "Python packaging experience.\n",
      "\n",
      "I began by converting ``cryptacular``, a password hashing package which\n",
      "has a simple C extension, to use ``setup.cfg``. I downloaded the Python 3.3\n",
      "source, struggled with the difference between ``setup.py`` and ``setup.cfg``\n",
      "syntax, fixed the ``define_macros`` feature, stopped using the missing\n",
      "``extras`` functionality, and several hours later I was able to generate my\n",
      "``METADATA`` from ``setup.cfg``. I rejoiced at my newfound freedom from the\n",
      "tyranny of arbitrary code execution during the build and install process.\n",
      "\n",
      "It was a lot of work. The package is worse off than before, and it can’t\n",
      "be built or installed without patching the Python source code itself.\n",
      "\n",
      "It was about that time that distutils-sig had a discussion about the\n",
      "need to include a generated ``setup.cfg`` from ``setup.cfg`` because\n",
      "``setup.cfg`` wasn’t static enough. Wait, what?\n",
      "\n",
      "Of course there is a different way to massively simplify the install\n",
      "process. It’s called built or binary packages. You never have to run\n",
      "``setup.py`` because there is no ``setup.py``. There is only METADATA aka\n",
      "PKG-INFO. Installation has two steps: ‘build package’; ‘install\n",
      "package’, and you can skip the first step, have someone else do it\n",
      "for you, do it on another machine, or install the build system from a\n",
      "binary package and let the build system handle the building. The build\n",
      "is still complicated, but installation is simple.\n",
      "\n",
      "With the binary package strategy people who want to install use a simple,\n",
      "compatible installer, and people who want to package use whatever is\n",
      "convenient for them for as long as it meets their needs. No one has\n",
      "to rewrite ``setup.py`` for their own or the 20k+ other packages on PyPI\n",
      "unless a different build system does a better job.\n",
      "\n",
      "Wheel is my attempt to benefit from the excellent distutils-sig work\n",
      "without having to fix the intractable ``distutils`` software itself. Like\n",
      "``METADATA`` and ``.dist-info`` directories but unlike Extension(), it’s\n",
      "simple enough that there really could be alternate implementations; the\n",
      "simplest (but less than ideal) installer is nothing more than “unzip\n",
      "archive.whl” somewhere on sys.path.\n",
      "\n",
      "If you’ve made it this far you probably wonder whether I’ve heard\n",
      "of eggs. Some comparisons:\n",
      "\n",
      "* Wheel is an installation format; egg is importable. Wheel archives do not need to include .pyc and are less tied to a specific Python version or implementation. Wheel can install (pure Python) packages built with previous versions of Python so you don’t always have to wait for the packager to catch up.\n",
      "\n",
      "* Wheel uses .dist-info directories; egg uses .egg-info. Wheel is compatible with the new world of Python ``packaging`` and the new concepts it brings.\n",
      "\n",
      "* Wheel has a richer file naming convention for today’s multi-implementation world. A single wheel archive can indicate its compatibility with a number of Python language versions and implementations, ABIs, and system architectures. Historically the ABI has been specific to a CPython release, but when we get a longer-term ABI, wheel will be ready.\n",
      "\n",
      "* Wheel is lossless. The first wheel implementation ``bdist_wheel`` always generates ``egg-info``, and then converts it to a ``.whl``. Later tools will allow for the conversion of existing eggs and bdist_wininst distributions.\n",
      "\n",
      "* Wheel is versioned. Every wheel file contains the version of the wheel specification and the implementation that packaged it. Hopefully the next migration can simply be to Wheel 2.0.\n",
      "\n",
      "I hope you will benefit from wheel.\n",
      "\n",
      "[end of docs/story.rst]\n",
      "[start of docs/development.rst]\n",
      "Development\n",
      "===========\n",
      "\n",
      "Pull Requests\n",
      "-------------\n",
      "\n",
      "- Submit Pull Requests against the ``main`` branch.\n",
      "- Provide a good description of what you're doing and why.\n",
      "- Provide tests that cover your changes and try to run the tests locally first.\n",
      "\n",
      "**Example**. Assuming you set up GitHub account, forked wheel repository from\n",
      "https://github.com/pypa/wheel to your own page via web interface, and your\n",
      "fork is located at https://github.com/yourname/wheel\n",
      "\n",
      "::\n",
      "\n",
      "  $ git clone git@github.com:pypa/wheel.git\n",
      "  $ cd wheel\n",
      "  # ...\n",
      "  $ git diff\n",
      "  $ git add <modified> ...\n",
      "  $ git status\n",
      "  $ git commit\n",
      "\n",
      "You may reference relevant issues in commit messages (like #1259) to\n",
      "make GitHub link issues and commits together, and with phrase like\n",
      "\"fixes #1259\" you can even close relevant issues automatically. Now\n",
      "push the changes to your fork::\n",
      "\n",
      "  $ git push git@github.com:yourname/wheel.git\n",
      "\n",
      "Open Pull Requests page at https://github.com/yourname/wheel/pulls and\n",
      "click \"New pull request\". That's it.\n",
      "\n",
      "Automated Testing\n",
      "-----------------\n",
      "\n",
      "All pull requests and merges to ``main`` branch are tested in `GitHub Actions`_\n",
      "based on the workflows in the ``.github`` directory.\n",
      "\n",
      "The only way to trigger the test suite to run again for a pull request is to\n",
      "submit another change to the pull branch.\n",
      "\n",
      ".. _GitHub Actions: https://github.com/actions\n",
      "\n",
      "Running Tests Locally\n",
      "---------------------\n",
      "\n",
      "Python requirements: tox_ or pytest_\n",
      "\n",
      "To run the tests via tox against all matching interpreters::\n",
      "\n",
      "  $ tox\n",
      "\n",
      "To run the tests via tox against a specific environment::\n",
      "\n",
      "  $ tox -e py35\n",
      "\n",
      "Alternatively, you can run the tests via pytest using your default interpreter::\n",
      "\n",
      "  $ pip install -e .[test]  # Installs the test dependencies\n",
      "  $ pytest                  # Runs the tests with the current interpreter\n",
      "\n",
      "The above pip install command will replace the current interpreter's installed\n",
      "wheel package with the development package being tested. If you use this\n",
      "workflow, it is recommended to run it under a virtualenv_.\n",
      "\n",
      ".. _tox: https://pypi.org/project/tox/\n",
      ".. _pytest: https://pypi.org/project/pytest/\n",
      ".. _virtualenv: https://pypi.org/project/virtualenv/\n",
      "\n",
      "Getting Involved\n",
      "----------------\n",
      "\n",
      "The wheel project welcomes help in the following ways:\n",
      "\n",
      "- Making Pull Requests for code, tests, or docs.\n",
      "- Commenting on open issues and pull requests.\n",
      "- Helping to answer questions on the `mailing list`_.\n",
      "\n",
      ".. _`mailing list`: https://mail.python.org/mailman/listinfo/distutils-sig\n",
      "\n",
      "Release Process\n",
      "---------------\n",
      "\n",
      "To make a new release:\n",
      "\n",
      "#. Edit ``docs/news.rst`` and replace ``**UNRELEASED**`` with a release version\n",
      "   and date, like ``**X.Y.Z (20XX-YY-ZZ)**``.\n",
      "#. Replace the ``__version__`` attribute in ``src/wheel/__init__.py`` with the\n",
      "   same version number as above (without the date of course).\n",
      "#. Create a new git tag matching the version exactly\n",
      "#. Push the new tag to GitHub\n",
      "\n",
      "Pushing a new tag to GitHub will trigger the publish workflow which package the\n",
      "project and publish the resulting artifacts to PyPI.\n",
      "\n",
      "[end of docs/development.rst]\n",
      "[start of docs/quickstart.rst]\n",
      "Quickstart\n",
      "==========\n",
      "\n",
      "To build a wheel for your project::\n",
      "\n",
      "    python -m pip install build\n",
      "    python -m build --wheel\n",
      "\n",
      "The wheel will go to ``dist/yourproject-<tags>.whl``.\n",
      "\n",
      "If you want to make universal (Python 2/3 compatible, pure Python) wheels, add the following\n",
      "section to your ``setup.cfg``::\n",
      "\n",
      "    [bdist_wheel]\n",
      "    universal = 1\n",
      "\n",
      "To convert an ``.egg`` or file to a wheel::\n",
      "\n",
      "    wheel convert youreggfile.egg\n",
      "\n",
      "Similarly, to convert a Windows installer (made using ``python setup.py bdist_wininst``) to a\n",
      "wheel::\n",
      "\n",
      "    wheel convert yourinstaller.exe\n",
      "\n",
      "[end of docs/quickstart.rst]\n",
      "[start of docs/installing.rst]\n",
      "Installation\n",
      "============\n",
      "\n",
      "You can use pip_ to install wheel::\n",
      "\n",
      "    pip install wheel\n",
      "\n",
      "If you do not have pip_ installed, see its documentation for\n",
      "`installation instructions`_.\n",
      "\n",
      "If you prefer using your system package manager to install Python packages, you\n",
      "can typically find the wheel package under one of the following package names:\n",
      "\n",
      "* python-wheel\n",
      "* python3-wheel\n",
      "\n",
      ".. _pip: https://pip.pypa.io/en/stable/\n",
      ".. _installation instructions: https://pip.pypa.io/en/stable/installing/\n",
      "\n",
      "Python and OS Compatibility\n",
      "---------------------------\n",
      "\n",
      "wheel should work on any Python implementation and operating system and is\n",
      "compatible with Python version 3.7 and upwards.\n",
      "\n",
      "[end of docs/installing.rst]\n",
      "[start of .git/packed-refs]\n",
      "# pack-refs with: peeled fully-peeled sorted \n",
      "c341c7fdeba963917110d6bdafcb92728f8edfc7 refs/remotes/origin/deprecate-apis\n",
      "ac5c0d9e78002f4aae9fd44e88dab3f5c2aebb2c refs/remotes/origin/fix-367\n",
      "ef6b4027e74a8271d89ee44b38e43e273882ef77 refs/remotes/origin/fix-411\n",
      "edb5f7a4572d0ac9193fb1fcc64ee28c74019946 refs/remotes/origin/main\n",
      "8d90b8a71e0e02e3457596e3f7991b6dfbf8118d refs/remotes/origin/pre-commit-ci-update-config\n",
      "59fd006757488aa38a655bb9c8a2904575a2d416 refs/remotes/origin/publicapi\n",
      "9ee6da6d6cab3fa171f2704df5f934474acc4aae refs/tags/0.10.0\n",
      "92e52e6745ec6b1178c2f42585c2f37ab04902a8 refs/tags/0.10.1\n",
      "126aaacc9f24fa69b6b74d4e742f645f6bb65cf3 refs/tags/0.10.2\n",
      "7021b0ee2a51e5d9447d06038db58e9e597625f7 refs/tags/0.10.3\n",
      "8dd29d841e15596bc9b5d1c7f7576b2a66113f92 refs/tags/0.11.0\n",
      "2861596953a6ddfb2c4f052a46a2e2b4c5619245 refs/tags/0.12.0\n",
      "60fa189589155e43f33587f67eb44bb4fd7d5b58 refs/tags/0.13.0\n",
      "dffddb1d5af9e736fdd6192ca9256dc767bfab51 refs/tags/0.15.0\n",
      "f65ab72e2c6b04d10a4153807f473c9e9af421d5 refs/tags/0.16.0\n",
      "474826f4d64064e852b541f099fa69dcf5dd20fa refs/tags/0.21.0\n",
      "f1cc0b8a8601040b864e1e784e160744c7e3d965 refs/tags/0.22.0\n",
      "75c59f175c8cb2bfcb6660927232b40df1e149c4 refs/tags/0.23.0\n",
      "cd0935c4fe92a15ea83057a9357347ae8d6a138e refs/tags/0.24.0\n",
      "4b9fd6bf095c1b4427ed65ac19e4d5666a719f33 refs/tags/0.25.0\n",
      "acb3cf8177034c014cfe90a691a850b5322951b5 refs/tags/0.26.0\n",
      "379fe05f71fc6144d5f58e582b6580339479e238 refs/tags/0.27.0\n",
      "36997ae4545f3c876771a53f11ae4aa9127cabfe refs/tags/0.28.0\n",
      "bc574d6ed73dabee02e4e00764172d6284a37cb4 refs/tags/0.29.0\n",
      "3c9cc212a76b334307985cebfa7ebf269a419223 refs/tags/0.30.0\n",
      "13d6ff3e8ef509b469d6bb6dd9cd8870ac3f2a16 refs/tags/0.31.0\n",
      "3dc261abc98a5e43bc7fcf5783d080aaf8f9f0cf refs/tags/0.31.1\n",
      "46e55b9bc7ac63d4c3a4dee706515968d58848ac refs/tags/0.32.0\n",
      "e284ddf676adffa22f6d6a7b02be7328acfe23c4 refs/tags/0.32.1\n",
      "3d3708c659f7b9988f958539e7748a1a01ee6e77 refs/tags/0.32.2\n",
      "11584b78a56b4eedba4eedc55c9dac8aa2dd7070 refs/tags/0.32.3\n",
      "bb54233a2dc855951fa40a48d5730c67f9d570c8 refs/tags/0.33.0\n",
      "c15924d80ed2235f277569d99bb9d80f59c9b19d refs/tags/0.33.1\n",
      "9534258081b4abf3af817dab976bc95ecc4bfa30 refs/tags/0.33.2\n",
      "7b782219d5322ee66cef293c5c5b49fbd4133a0d refs/tags/0.33.3\n",
      "882cd05d9bf4c4fc72568d806e227b5cf14df1d7 refs/tags/0.33.4\n",
      "9ee9726620f80232ca49991dbeef83963d94482a refs/tags/0.33.5\n",
      "b227ddd5beaba49294017d061d501f6d433393b0 refs/tags/0.33.6\n",
      "8b2312799f46359ac78f3b011dc758c45518e9e1 refs/tags/0.34.0\n",
      "d6267870e5bf15651ead6c91e61ad47bb363cbb9 refs/tags/0.34.1\n",
      "^43078c409ebe9b7d5d70c2476fcb82055e8f0b97\n",
      "0036f80513457a95a309c81d8a63f113b52dbafa refs/tags/0.34.2\n",
      "4253bbb865823101d1a1757d12239fc74f49c4e7 refs/tags/0.35.0\n",
      "bbc60829bea2eb25ccd236b9c6c16c9ee4f0d2ef refs/tags/0.35.1\n",
      "39b6397ba944d82f1fe37d043382f97b78a919d6 refs/tags/0.36.0\n",
      "64550e15fdbc96d5690dc872257edc859c218068 refs/tags/0.36.1\n",
      "4fb47f98550f3f43fc0b8c73f518592124ae21bd refs/tags/0.36.2\n",
      "b8c4aa055cea0132776e7e53edce3538710d5b68 refs/tags/0.37.0\n",
      "c716c87ffa528ac9a31de3ed3217058848dc2cd2 refs/tags/0.37.1\n",
      "fcb94cd51f7dfd40d88dff37415750d9e3638d3b refs/tags/0.38.0\n",
      "6f1608dc1a88f00d2390b9044bd655753f39234c refs/tags/0.38.1\n",
      "44193907eb308930de05deed863fb4d157c5c866 refs/tags/0.38.2\n",
      "daeb157c8207b1459fc7baa5f234c165a7a5faf4 refs/tags/0.38.3\n",
      "814c2efe8e40051039c5a6de6945e04ecdd162ee refs/tags/0.38.4\n",
      "461f7a3a2d4824009ca4855b3300d61e6d405dc4 refs/tags/0.40.0\n",
      "95c2d83cde174ed3c34d5a5b53dc3b3a1ace91b6 refs/tags/0.41.0\n",
      "b626a4a2ee6a887c7ad4b984fcb4d465dbfa337d refs/tags/0.41.1\n",
      "e76040b0d970a0fe6211c31888fc929078ba02d9 refs/tags/0.41.2\n",
      "a899f1c69ffddf564b904a5de974457c213dc36a refs/tags/0.41.3\n",
      "63a09bb3d92630c1d35f4fcc6f79250256aeeb5c refs/tags/0.42.0\n",
      "fa33dfd01fd665c1fd90097563b34bce4b5527ef refs/tags/0.43.0\n",
      "7bb46d7727e6e89fe56b3c78297b3af2672bbbe2 refs/tags/0.44.0\n",
      "d78f0e372199f8294556345d867af4d3cf118418 refs/tags/0.45.0\n",
      "7855525de4093257e7bfb434877265e227356566 refs/tags/0.45.1\n",
      "d02f91dceda7c4a16cc26ea80f0eecfff638e4b2 refs/tags/0.5\n",
      "bdf9d5747a724664bd3588ab471907a900b45c9a refs/tags/0.6\n",
      "b2930f5f5d3c898389bdbdd7c854a0b9d320cf14 refs/tags/0.7\n",
      "49b1d5dac1cdeaf98998b1e5bdb0e2cf134edff3 refs/tags/0.9\n",
      "6952bc635deb0afdf36c5f325aa166636f1c2a52 refs/tags/0.9.1\n",
      "ca65be72feaf5ebe92ef4c9e91bd722360c1ad2c refs/tags/0.9.3\n",
      "8db2d0d4760a0886998e3fca4206f5db3230d223 refs/tags/0.9.4\n",
      "0c4752551e4fa1d4e9786fc874ae97edf52c0369 refs/tags/0.9.6\n",
      "a118f5845570ebaf6566b8075dc1dcaf117d7d43 refs/tags/0.9.7\n",
      "\n",
      "[end of .git/packed-refs]\n",
      "[start of .git/config]\n",
      "[core]\n",
      "\trepositoryformatversion = 0\n",
      "\tfilemode = true\n",
      "\tbare = false\n",
      "\tlogallrefupdates = true\n",
      "[remote \"origin\"]\n",
      "\turl = https://github.com/pypa/wheel.git\n",
      "\tfetch = +refs/heads/*:refs/remotes/origin/*\n",
      "[branch \"main\"]\n",
      "\tremote = origin\n",
      "\tmerge = refs/heads/main\n",
      "\n",
      "[end of .git/config]\n",
      "[start of .git/description]\n",
      "Unnamed repository; edit this file 'description' to name the repository.\n",
      "\n",
      "[end of .git/description]\n",
      "[start of .git/HEAD]\n",
      "ref: refs/heads/main\n",
      "\n",
      "[end of .git/HEAD]\n",
      "[start of .git/index]\n",
      "None\n",
      "[end of .git/index]\n",
      "[start of .github/codecov.yml]\n",
      "coverage:\n",
      "  status:\n",
      "    patch:\n",
      "      default:\n",
      "        informational: true\n",
      "\n",
      "[end of .github/codecov.yml]\n",
      "[start of docs/reference/wheel_convert.rst]\n",
      "wheel convert\n",
      "=============\n",
      "\n",
      "Usage\n",
      "-----\n",
      "\n",
      "::\n",
      "\n",
      "    wheel convert [options] <egg_file_or_directory> [egg_file_or_directory...]\n",
      "\n",
      "\n",
      "Description\n",
      "-----------\n",
      "\n",
      "Convert one or more eggs (``.egg``; made with ``bdist_egg``) or Windows\n",
      "installers (``.exe``; made with ``bdist_wininst``) into wheels.\n",
      "\n",
      "Egg names must match the standard format:\n",
      "\n",
      "* ``<project>-<version>-pyX.Y`` for pure Python wheels\n",
      "* ``<project>-<version>-pyX.Y-<arch>`` for binary wheels\n",
      "\n",
      "\n",
      "Options\n",
      "-------\n",
      "\n",
      ".. option:: -d, --dest-dir <dir>\n",
      "\n",
      "    Directory to store the generated wheels in (defaults to current directory).\n",
      "\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "* Convert a single egg file::\n",
      "\n",
      "    $ wheel convert foobar-1.2.3-py2.7.egg\n",
      "    $ ls *.whl\n",
      "    foobar-1.2.3-py27-none.whl\n",
      "\n",
      "* If the egg file name is invalid::\n",
      "\n",
      "    $ wheel convert pycharm-debug.egg\n",
      "    \"pycharm-debug.egg\" is not a valid egg name (must match at least name-version-pyX.Y.egg)\n",
      "    $ echo $?\n",
      "    1\n",
      "\n",
      "[end of docs/reference/wheel_convert.rst]\n",
      "[start of docs/reference/wheel_unpack.rst]\n",
      "wheel unpack\n",
      "============\n",
      "\n",
      "Usage\n",
      "-----\n",
      "\n",
      "::\n",
      "\n",
      "    wheel unpack <wheel_file>\n",
      "\n",
      "\n",
      "Description\n",
      "-----------\n",
      "\n",
      "Unpack the given wheel file.\n",
      "\n",
      "This is the equivalent of ``unzip <wheel_file>``, except that it also checks\n",
      "that the hashes and file sizes match with those in ``RECORD`` and exits with an\n",
      "error if it encounters a mismatch.\n",
      "\n",
      "\n",
      "Options\n",
      "-------\n",
      "\n",
      ".. option:: -d, --dest <dir>\n",
      "\n",
      "    Directory to unpack the wheel into.\n",
      "\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "* Unpack a wheel::\n",
      "\n",
      "    $ wheel unpack someproject-1.5.0-py2-py3-none.whl\n",
      "    Unpacking to: ./someproject-1.5.0\n",
      "\n",
      "* If a file's hash does not match::\n",
      "\n",
      "    $ wheel unpack someproject-1.5.0-py2-py3-none.whl\n",
      "    Unpacking to: ./someproject-1.5.0\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    wheel.install.BadWheelFile: Bad hash for file 'mypackage/module.py'\n",
      "    $ echo $?\n",
      "    1\n",
      "\n",
      "[end of docs/reference/wheel_unpack.rst]\n",
      "[start of docs/reference/wheel_tags.rst]\n",
      "wheel tags\n",
      "==========\n",
      "\n",
      "Usage\n",
      "-----\n",
      "\n",
      "::\n",
      "\n",
      "    wheel tags [-h] [--remove] [--python-tag TAG] [--abi-tag TAG] [--platform-tag TAG] [--build NUMBER] WHEEL [...]\n",
      "\n",
      "Description\n",
      "-----------\n",
      "\n",
      "Make a new wheel with given tags from an existing wheel. Any tags left\n",
      "unspecified will remain the same. Multiple tags are separated by a \".\" Starting\n",
      "with a \"+\" will append to the existing tags.  Starting with a \"-\" will remove a\n",
      "tag. Be sure to use the equals syntax on the shell so that it does not get\n",
      "parsed as an extra option, such as ``--python-tag=-py2``. The original file\n",
      "will remain unless ``--remove`` is given. The output filename(s) will be\n",
      "displayed on stdout for further processing.\n",
      "\n",
      "\n",
      "Options\n",
      "-------\n",
      "\n",
      ".. option:: --remove\n",
      "\n",
      "    Remove the original wheel, keeping only the retagged wheel.\n",
      "\n",
      ".. option:: --python-tag=TAG\n",
      "\n",
      "    Override the python tag (prepend with \"+\" to append, \"-\" to remove).\n",
      "    Multiple tags can be separated with a dot.\n",
      "\n",
      ".. option:: --abi-tag=TAG\n",
      "\n",
      "    Override the abi tag (prepend with \"+\" to append, \"-\" to remove).\n",
      "    Multiple tags can be separated with a dot.\n",
      "\n",
      ".. option:: --platform-tag=TAG\n",
      "\n",
      "    Override the platform tag (prepend with \"+\" to append, \"-\" to remove).\n",
      "    Multiple tags can be separated with a dot.\n",
      "\n",
      ".. option:: --build=NUMBER\n",
      "\n",
      "    Specify a build number.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "* Replace a wheel's Python specific tags with generic tags (if no Python extensions are present, for example)::\n",
      "\n",
      "    $ wheel tags --python-tag=py2.py3 --abi-tag=none cmake-3.20.2-cp39-cp39-win_amd64.whl\n",
      "    cmake-3.20.2-py2.py3-none-win_amd64.whl\n",
      "\n",
      "* Add compatibility tags for macOS universal wheels and older pips::\n",
      "\n",
      "    $ wheel tags \\\n",
      "        --platform-tag=+macosx_10_9_x86_64.macosx_11_0_arm64 \\\n",
      "        ninja-1.11.1-py2.py3-none-macosx_10_9_universal2.whl\n",
      "    ninja-1.11.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.whl\n",
      "\n",
      "[end of docs/reference/wheel_tags.rst]\n",
      "[start of docs/reference/index.rst]\n",
      "Reference Guide\n",
      "===============\n",
      "\n",
      ".. toctree::\n",
      "   :maxdepth: 2\n",
      "\n",
      "   wheel_convert\n",
      "   wheel_unpack\n",
      "   wheel_pack\n",
      "   wheel_tags\n",
      "\n",
      "[end of docs/reference/index.rst]\n",
      "[start of docs/reference/wheel_pack.rst]\n",
      "wheel pack\n",
      "==========\n",
      "\n",
      "Usage\n",
      "-----\n",
      "\n",
      "::\n",
      "\n",
      "    wheel pack <wheel_directory>\n",
      "\n",
      "\n",
      "Description\n",
      "-----------\n",
      "\n",
      "Repack a previously unpacked wheel file.\n",
      "\n",
      "This command can be used to repack a wheel file after its contents have been modified.\n",
      "This is the equivalent of ``zip -r <wheel_file> <wheel_directory>`` except that it regenerates the\n",
      "``RECORD`` file which contains hashes of all included files.\n",
      "\n",
      "\n",
      "Options\n",
      "-------\n",
      "\n",
      ".. option:: -d, --dest-dir <dir>\n",
      "\n",
      "    Directory to put the new wheel file into.\n",
      "\n",
      ".. option:: --build-number <tag>\n",
      "\n",
      "    Override the build tag in the new wheel file name\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "* Unpack a wheel, add a dummy module and then repack it (with a new build number)::\n",
      "\n",
      "    $ wheel unpack someproject-1.5.0-py2-py3-none.whl\n",
      "    Unpacking to: ./someproject-1.5.0\n",
      "    $ touch someproject-1.5.0/somepackage/module.py\n",
      "    $ wheel pack --build-number 2 someproject-1.5.0\n",
      "    Repacking wheel as ./someproject-1.5.0-2-py2-py3-none.whl...OK\n",
      "\n",
      "[end of docs/reference/wheel_pack.rst]\n",
      "[start of docs/manpages/wheel.rst]\n",
      ":orphan:\n",
      "\n",
      "wheel manual page\n",
      "=================\n",
      "\n",
      "Synopsis\n",
      "--------\n",
      "\n",
      "**wheel** [*command*] [*options*]\n",
      "\n",
      "\n",
      "Description\n",
      "-----------\n",
      "\n",
      ":program:`wheel` installs and operates on `PEP 427`_ format binary wheels.\n",
      "\n",
      "\n",
      "Commands\n",
      "--------\n",
      "  ``unpack``\n",
      "    Unpack wheel\n",
      "\n",
      "  ``pack``\n",
      "    Repack a previously unpacked wheel\n",
      "\n",
      "  ``convert``\n",
      "    Convert egg or wininst to wheel\n",
      "\n",
      "  ``tags``\n",
      "    Change the tags on a wheel file\n",
      "\n",
      "  ``version``\n",
      "    Print version and exit\n",
      "\n",
      "  ``help``\n",
      "    Show this help\n",
      "\n",
      "Try ``wheel <command> --help`` for more information.\n",
      "\n",
      "\n",
      "Options\n",
      "-------\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "\n",
      ".. _`PEP 427`: https://www.python.org/dev/peps/pep-0427/\n",
      "\n",
      "[end of docs/manpages/wheel.rst]\n",
      "[start of .git/logs/HEAD]\n",
      "0000000000000000000000000000000000000000 edb5f7a4572d0ac9193fb1fcc64ee28c74019946 Stefano Rando <stefano.rando.c@protonmail.com> 1739925436 -0800\tclone: from https://github.com/pypa/wheel.git\n",
      "\n",
      "[end of .git/logs/HEAD]\n",
      "[start of .git/info/exclude]\n",
      "# git ls-files --others --exclude-from=.git/info/exclude\n",
      "# Lines that start with '#' are comments.\n",
      "# For a project mostly in C, the following would be a good set of\n",
      "# exclude patterns (uncomment them if you want to use them):\n",
      "# *.[oa]\n",
      "# *~\n",
      "\n",
      "[end of .git/info/exclude]\n",
      "[start of .git/hooks/pre-commit.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to verify what is about to be committed.\n",
      "# Called by \"git commit\" with no arguments.  The hook should\n",
      "# exit with non-zero status after issuing an appropriate message if\n",
      "# it wants to stop the commit.\n",
      "#\n",
      "# To enable this hook, rename this file to \"pre-commit\".\n",
      "\n",
      "if git rev-parse --verify HEAD >/dev/null 2>&1\n",
      "then\n",
      "\tagainst=HEAD\n",
      "else\n",
      "\t# Initial commit: diff against an empty tree object\n",
      "\tagainst=$(git hash-object -t tree /dev/null)\n",
      "fi\n",
      "\n",
      "# If you want to allow non-ASCII filenames set this variable to true.\n",
      "allownonascii=$(git config --bool hooks.allownonascii)\n",
      "\n",
      "# Redirect output to stderr.\n",
      "exec 1>&2\n",
      "\n",
      "# Cross platform projects tend to avoid non-ASCII filenames; prevent\n",
      "# them from being added to the repository. We exploit the fact that the\n",
      "# printable range starts at the space character and ends with tilde.\n",
      "if [ \"$allownonascii\" != \"true\" ] &&\n",
      "\t# Note that the use of brackets around a tr range is ok here, (it's\n",
      "\t# even required, for portability to Solaris 10's /usr/bin/tr), since\n",
      "\t# the square bracket bytes happen to fall in the designated range.\n",
      "\ttest $(git diff --cached --name-only --diff-filter=A -z $against |\n",
      "\t  LC_ALL=C tr -d '[ -~]\\0' | wc -c) != 0\n",
      "then\n",
      "\tcat <<\\EOF\n",
      "Error: Attempt to add a non-ASCII file name.\n",
      "\n",
      "This can cause problems if you want to work with people on other platforms.\n",
      "\n",
      "To be portable it is advisable to rename the file.\n",
      "\n",
      "If you know what you are doing you can disable this check using:\n",
      "\n",
      "  git config hooks.allownonascii true\n",
      "EOF\n",
      "\texit 1\n",
      "fi\n",
      "\n",
      "# If there are whitespace errors, print the offending file names and fail.\n",
      "exec git diff-index --check --cached $against --\n",
      "\n",
      "[end of .git/hooks/pre-commit.sample]\n",
      "[start of .git/hooks/post-update.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to prepare a packed repository for use over\n",
      "# dumb transports.\n",
      "#\n",
      "# To enable this hook, rename this file to \"post-update\".\n",
      "\n",
      "exec git update-server-info\n",
      "\n",
      "[end of .git/hooks/post-update.sample]\n",
      "[start of .git/hooks/applypatch-msg.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to check the commit log message taken by\n",
      "# applypatch from an e-mail message.\n",
      "#\n",
      "# The hook should exit with non-zero status after issuing an\n",
      "# appropriate message if it wants to stop the commit.  The hook is\n",
      "# allowed to edit the commit message file.\n",
      "#\n",
      "# To enable this hook, rename this file to \"applypatch-msg\".\n",
      "\n",
      ". git-sh-setup\n",
      "commitmsg=\"$(git rev-parse --git-path hooks/commit-msg)\"\n",
      "test -x \"$commitmsg\" && exec \"$commitmsg\" ${1+\"$@\"}\n",
      ":\n",
      "\n",
      "[end of .git/hooks/applypatch-msg.sample]\n",
      "[start of .git/hooks/pre-push.sample]\n",
      "#!/bin/sh\n",
      "\n",
      "# An example hook script to verify what is about to be pushed.  Called by \"git\n",
      "# push\" after it has checked the remote status, but before anything has been\n",
      "# pushed.  If this script exits with a non-zero status nothing will be pushed.\n",
      "#\n",
      "# This hook is called with the following parameters:\n",
      "#\n",
      "# $1 -- Name of the remote to which the push is being done\n",
      "# $2 -- URL to which the push is being done\n",
      "#\n",
      "# If pushing without using a named remote those arguments will be equal.\n",
      "#\n",
      "# Information about the commits which are being pushed is supplied as lines to\n",
      "# the standard input in the form:\n",
      "#\n",
      "#   <local ref> <local sha1> <remote ref> <remote sha1>\n",
      "#\n",
      "# This sample shows how to prevent push of commits where the log message starts\n",
      "# with \"WIP\" (work in progress).\n",
      "\n",
      "remote=\"$1\"\n",
      "url=\"$2\"\n",
      "\n",
      "z40=0000000000000000000000000000000000000000\n",
      "\n",
      "while read local_ref local_sha remote_ref remote_sha\n",
      "do\n",
      "\tif [ \"$local_sha\" = $z40 ]\n",
      "\tthen\n",
      "\t\t# Handle delete\n",
      "\t\t:\n",
      "\telse\n",
      "\t\tif [ \"$remote_sha\" = $z40 ]\n",
      "\t\tthen\n",
      "\t\t\t# New branch, examine all commits\n",
      "\t\t\trange=\"$local_sha\"\n",
      "\t\telse\n",
      "\t\t\t# Update to existing branch, examine new commits\n",
      "\t\t\trange=\"$remote_sha..$local_sha\"\n",
      "\t\tfi\n",
      "\n",
      "\t\t# Check for WIP commit\n",
      "\t\tcommit=`git rev-list -n 1 --grep '^WIP' \"$range\"`\n",
      "\t\tif [ -n \"$commit\" ]\n",
      "\t\tthen\n",
      "\t\t\techo >&2 \"Found WIP commit in $local_ref, not pushing\"\n",
      "\t\t\texit 1\n",
      "\t\tfi\n",
      "\tfi\n",
      "done\n",
      "\n",
      "exit 0\n",
      "\n",
      "[end of .git/hooks/pre-push.sample]\n",
      "[start of .git/hooks/pre-receive.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to make use of push options.\n",
      "# The example simply echoes all push options that start with 'echoback='\n",
      "# and rejects all pushes when the \"reject\" push option is used.\n",
      "#\n",
      "# To enable this hook, rename this file to \"pre-receive\".\n",
      "\n",
      "if test -n \"$GIT_PUSH_OPTION_COUNT\"\n",
      "then\n",
      "\ti=0\n",
      "\twhile test \"$i\" -lt \"$GIT_PUSH_OPTION_COUNT\"\n",
      "\tdo\n",
      "\t\teval \"value=\\$GIT_PUSH_OPTION_$i\"\n",
      "\t\tcase \"$value\" in\n",
      "\t\techoback=*)\n",
      "\t\t\techo \"echo from the pre-receive-hook: ${value#*=}\" >&2\n",
      "\t\t\t;;\n",
      "\t\treject)\n",
      "\t\t\texit 1\n",
      "\t\tesac\n",
      "\t\ti=$((i + 1))\n",
      "\tdone\n",
      "fi\n",
      "\n",
      "[end of .git/hooks/pre-receive.sample]\n",
      "[start of .git/hooks/pre-rebase.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# Copyright (c) 2006, 2008 Junio C Hamano\n",
      "#\n",
      "# The \"pre-rebase\" hook is run just before \"git rebase\" starts doing\n",
      "# its job, and can prevent the command from running by exiting with\n",
      "# non-zero status.\n",
      "#\n",
      "# The hook is called with the following parameters:\n",
      "#\n",
      "# $1 -- the upstream the series was forked from.\n",
      "# $2 -- the branch being rebased (or empty when rebasing the current branch).\n",
      "#\n",
      "# This sample shows how to prevent topic branches that are already\n",
      "# merged to 'next' branch from getting rebased, because allowing it\n",
      "# would result in rebasing already published history.\n",
      "\n",
      "publish=next\n",
      "basebranch=\"$1\"\n",
      "if test \"$#\" = 2\n",
      "then\n",
      "\ttopic=\"refs/heads/$2\"\n",
      "else\n",
      "\ttopic=`git symbolic-ref HEAD` ||\n",
      "\texit 0 ;# we do not interrupt rebasing detached HEAD\n",
      "fi\n",
      "\n",
      "case \"$topic\" in\n",
      "refs/heads/??/*)\n",
      "\t;;\n",
      "*)\n",
      "\texit 0 ;# we do not interrupt others.\n",
      "\t;;\n",
      "esac\n",
      "\n",
      "# Now we are dealing with a topic branch being rebased\n",
      "# on top of master.  Is it OK to rebase it?\n",
      "\n",
      "# Does the topic really exist?\n",
      "git show-ref -q \"$topic\" || {\n",
      "\techo >&2 \"No such branch $topic\"\n",
      "\texit 1\n",
      "}\n",
      "\n",
      "# Is topic fully merged to master?\n",
      "not_in_master=`git rev-list --pretty=oneline ^master \"$topic\"`\n",
      "if test -z \"$not_in_master\"\n",
      "then\n",
      "\techo >&2 \"$topic is fully merged to master; better remove it.\"\n",
      "\texit 1 ;# we could allow it, but there is no point.\n",
      "fi\n",
      "\n",
      "# Is topic ever merged to next?  If so you should not be rebasing it.\n",
      "only_next_1=`git rev-list ^master \"^$topic\" ${publish} | sort`\n",
      "only_next_2=`git rev-list ^master           ${publish} | sort`\n",
      "if test \"$only_next_1\" = \"$only_next_2\"\n",
      "then\n",
      "\tnot_in_topic=`git rev-list \"^$topic\" master`\n",
      "\tif test -z \"$not_in_topic\"\n",
      "\tthen\n",
      "\t\techo >&2 \"$topic is already up to date with master\"\n",
      "\t\texit 1 ;# we could allow it, but there is no point.\n",
      "\telse\n",
      "\t\texit 0\n",
      "\tfi\n",
      "else\n",
      "\tnot_in_next=`git rev-list --pretty=oneline ^${publish} \"$topic\"`\n",
      "\t/usr/bin/perl -e '\n",
      "\t\tmy $topic = $ARGV[0];\n",
      "\t\tmy $msg = \"* $topic has commits already merged to public branch:\\n\";\n",
      "\t\tmy (%not_in_next) = map {\n",
      "\t\t\t/^([0-9a-f]+) /;\n",
      "\t\t\t($1 => 1);\n",
      "\t\t} split(/\\n/, $ARGV[1]);\n",
      "\t\tfor my $elem (map {\n",
      "\t\t\t\t/^([0-9a-f]+) (.*)$/;\n",
      "\t\t\t\t[$1 => $2];\n",
      "\t\t\t} split(/\\n/, $ARGV[2])) {\n",
      "\t\t\tif (!exists $not_in_next{$elem->[0]}) {\n",
      "\t\t\t\tif ($msg) {\n",
      "\t\t\t\t\tprint STDERR $msg;\n",
      "\t\t\t\t\tundef $msg;\n",
      "\t\t\t\t}\n",
      "\t\t\t\tprint STDERR \" $elem->[1]\\n\";\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\t' \"$topic\" \"$not_in_next\" \"$not_in_master\"\n",
      "\texit 1\n",
      "fi\n",
      "\n",
      "<<\\DOC_END\n",
      "\n",
      "This sample hook safeguards topic branches that have been\n",
      "published from being rewound.\n",
      "\n",
      "The workflow assumed here is:\n",
      "\n",
      " * Once a topic branch forks from \"master\", \"master\" is never\n",
      "   merged into it again (either directly or indirectly).\n",
      "\n",
      " * Once a topic branch is fully cooked and merged into \"master\",\n",
      "   it is deleted.  If you need to build on top of it to correct\n",
      "   earlier mistakes, a new topic branch is created by forking at\n",
      "   the tip of the \"master\".  This is not strictly necessary, but\n",
      "   it makes it easier to keep your history simple.\n",
      "\n",
      " * Whenever you need to test or publish your changes to topic\n",
      "   branches, merge them into \"next\" branch.\n",
      "\n",
      "The script, being an example, hardcodes the publish branch name\n",
      "to be \"next\", but it is trivial to make it configurable via\n",
      "$GIT_DIR/config mechanism.\n",
      "\n",
      "With this workflow, you would want to know:\n",
      "\n",
      "(1) ... if a topic branch has ever been merged to \"next\".  Young\n",
      "    topic branches can have stupid mistakes you would rather\n",
      "    clean up before publishing, and things that have not been\n",
      "    merged into other branches can be easily rebased without\n",
      "    affecting other people.  But once it is published, you would\n",
      "    not want to rewind it.\n",
      "\n",
      "(2) ... if a topic branch has been fully merged to \"master\".\n",
      "    Then you can delete it.  More importantly, you should not\n",
      "    build on top of it -- other people may already want to\n",
      "    change things related to the topic as patches against your\n",
      "    \"master\", so if you need further changes, it is better to\n",
      "    fork the topic (perhaps with the same name) afresh from the\n",
      "    tip of \"master\".\n",
      "\n",
      "Let's look at this example:\n",
      "\n",
      "\t\t   o---o---o---o---o---o---o---o---o---o \"next\"\n",
      "\t\t  /       /           /           /\n",
      "\t\t /   a---a---b A     /           /\n",
      "\t\t/   /               /           /\n",
      "\t       /   /   c---c---c---c B         /\n",
      "\t      /   /   /             \\         /\n",
      "\t     /   /   /   b---b C     \\       /\n",
      "\t    /   /   /   /             \\     /\n",
      "    ---o---o---o---o---o---o---o---o---o---o---o \"master\"\n",
      "\n",
      "\n",
      "A, B and C are topic branches.\n",
      "\n",
      " * A has one fix since it was merged up to \"next\".\n",
      "\n",
      " * B has finished.  It has been fully merged up to \"master\" and \"next\",\n",
      "   and is ready to be deleted.\n",
      "\n",
      " * C has not merged to \"next\" at all.\n",
      "\n",
      "We would want to allow C to be rebased, refuse A, and encourage\n",
      "B to be deleted.\n",
      "\n",
      "To compute (1):\n",
      "\n",
      "\tgit rev-list ^master ^topic next\n",
      "\tgit rev-list ^master        next\n",
      "\n",
      "\tif these match, topic has not merged in next at all.\n",
      "\n",
      "To compute (2):\n",
      "\n",
      "\tgit rev-list master..topic\n",
      "\n",
      "\tif this is empty, it is fully merged to \"master\".\n",
      "\n",
      "DOC_END\n",
      "\n",
      "[end of .git/hooks/pre-rebase.sample]\n",
      "[start of .git/hooks/update.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to block unannotated tags from entering.\n",
      "# Called by \"git receive-pack\" with arguments: refname sha1-old sha1-new\n",
      "#\n",
      "# To enable this hook, rename this file to \"update\".\n",
      "#\n",
      "# Config\n",
      "# ------\n",
      "# hooks.allowunannotated\n",
      "#   This boolean sets whether unannotated tags will be allowed into the\n",
      "#   repository.  By default they won't be.\n",
      "# hooks.allowdeletetag\n",
      "#   This boolean sets whether deleting tags will be allowed in the\n",
      "#   repository.  By default they won't be.\n",
      "# hooks.allowmodifytag\n",
      "#   This boolean sets whether a tag may be modified after creation. By default\n",
      "#   it won't be.\n",
      "# hooks.allowdeletebranch\n",
      "#   This boolean sets whether deleting branches will be allowed in the\n",
      "#   repository.  By default they won't be.\n",
      "# hooks.denycreatebranch\n",
      "#   This boolean sets whether remotely creating branches will be denied\n",
      "#   in the repository.  By default this is allowed.\n",
      "#\n",
      "\n",
      "# --- Command line\n",
      "refname=\"$1\"\n",
      "oldrev=\"$2\"\n",
      "newrev=\"$3\"\n",
      "\n",
      "# --- Safety check\n",
      "if [ -z \"$GIT_DIR\" ]; then\n",
      "\techo \"Don't run this script from the command line.\" >&2\n",
      "\techo \" (if you want, you could supply GIT_DIR then run\" >&2\n",
      "\techo \"  $0 <ref> <oldrev> <newrev>)\" >&2\n",
      "\texit 1\n",
      "fi\n",
      "\n",
      "if [ -z \"$refname\" -o -z \"$oldrev\" -o -z \"$newrev\" ]; then\n",
      "\techo \"usage: $0 <ref> <oldrev> <newrev>\" >&2\n",
      "\texit 1\n",
      "fi\n",
      "\n",
      "# --- Config\n",
      "allowunannotated=$(git config --bool hooks.allowunannotated)\n",
      "allowdeletebranch=$(git config --bool hooks.allowdeletebranch)\n",
      "denycreatebranch=$(git config --bool hooks.denycreatebranch)\n",
      "allowdeletetag=$(git config --bool hooks.allowdeletetag)\n",
      "allowmodifytag=$(git config --bool hooks.allowmodifytag)\n",
      "\n",
      "# check for no description\n",
      "projectdesc=$(sed -e '1q' \"$GIT_DIR/description\")\n",
      "case \"$projectdesc\" in\n",
      "\"Unnamed repository\"* | \"\")\n",
      "\techo \"*** Project description file hasn't been set\" >&2\n",
      "\texit 1\n",
      "\t;;\n",
      "esac\n",
      "\n",
      "# --- Check types\n",
      "# if $newrev is 0000...0000, it's a commit to delete a ref.\n",
      "zero=\"0000000000000000000000000000000000000000\"\n",
      "if [ \"$newrev\" = \"$zero\" ]; then\n",
      "\tnewrev_type=delete\n",
      "else\n",
      "\tnewrev_type=$(git cat-file -t $newrev)\n",
      "fi\n",
      "\n",
      "case \"$refname\",\"$newrev_type\" in\n",
      "\trefs/tags/*,commit)\n",
      "\t\t# un-annotated tag\n",
      "\t\tshort_refname=${refname##refs/tags/}\n",
      "\t\tif [ \"$allowunannotated\" != \"true\" ]; then\n",
      "\t\t\techo \"*** The un-annotated tag, $short_refname, is not allowed in this repository\" >&2\n",
      "\t\t\techo \"*** Use 'git tag [ -a | -s ]' for tags you want to propagate.\" >&2\n",
      "\t\t\texit 1\n",
      "\t\tfi\n",
      "\t\t;;\n",
      "\trefs/tags/*,delete)\n",
      "\t\t# delete tag\n",
      "\t\tif [ \"$allowdeletetag\" != \"true\" ]; then\n",
      "\t\t\techo \"*** Deleting a tag is not allowed in this repository\" >&2\n",
      "\t\t\texit 1\n",
      "\t\tfi\n",
      "\t\t;;\n",
      "\trefs/tags/*,tag)\n",
      "\t\t# annotated tag\n",
      "\t\tif [ \"$allowmodifytag\" != \"true\" ] && git rev-parse $refname > /dev/null 2>&1\n",
      "\t\tthen\n",
      "\t\t\techo \"*** Tag '$refname' already exists.\" >&2\n",
      "\t\t\techo \"*** Modifying a tag is not allowed in this repository.\" >&2\n",
      "\t\t\texit 1\n",
      "\t\tfi\n",
      "\t\t;;\n",
      "\trefs/heads/*,commit)\n",
      "\t\t# branch\n",
      "\t\tif [ \"$oldrev\" = \"$zero\" -a \"$denycreatebranch\" = \"true\" ]; then\n",
      "\t\t\techo \"*** Creating a branch is not allowed in this repository\" >&2\n",
      "\t\t\texit 1\n",
      "\t\tfi\n",
      "\t\t;;\n",
      "\trefs/heads/*,delete)\n",
      "\t\t# delete branch\n",
      "\t\tif [ \"$allowdeletebranch\" != \"true\" ]; then\n",
      "\t\t\techo \"*** Deleting a branch is not allowed in this repository\" >&2\n",
      "\t\t\texit 1\n",
      "\t\tfi\n",
      "\t\t;;\n",
      "\trefs/remotes/*,commit)\n",
      "\t\t# tracking branch\n",
      "\t\t;;\n",
      "\trefs/remotes/*,delete)\n",
      "\t\t# delete tracking branch\n",
      "\t\tif [ \"$allowdeletebranch\" != \"true\" ]; then\n",
      "\t\t\techo \"*** Deleting a tracking branch is not allowed in this repository\" >&2\n",
      "\t\t\texit 1\n",
      "\t\tfi\n",
      "\t\t;;\n",
      "\t*)\n",
      "\t\t# Anything else (is there anything else?)\n",
      "\t\techo \"*** Update hook: unknown type of update to ref $refname of type $newrev_type\" >&2\n",
      "\t\texit 1\n",
      "\t\t;;\n",
      "esac\n",
      "\n",
      "# --- Finished\n",
      "exit 0\n",
      "\n",
      "[end of .git/hooks/update.sample]\n",
      "[start of .git/hooks/pre-merge-commit.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to verify what is about to be committed.\n",
      "# Called by \"git merge\" with no arguments.  The hook should\n",
      "# exit with non-zero status after issuing an appropriate message to\n",
      "# stderr if it wants to stop the merge commit.\n",
      "#\n",
      "# To enable this hook, rename this file to \"pre-merge-commit\".\n",
      "\n",
      ". git-sh-setup\n",
      "test -x \"$GIT_DIR/hooks/pre-commit\" &&\n",
      "        exec \"$GIT_DIR/hooks/pre-commit\"\n",
      ":\n",
      "\n",
      "[end of .git/hooks/pre-merge-commit.sample]\n",
      "[start of .git/hooks/fsmonitor-watchman.sample]\n",
      "#!/usr/bin/perl\n",
      "\n",
      "use strict;\n",
      "use warnings;\n",
      "use IPC::Open2;\n",
      "\n",
      "# An example hook script to integrate Watchman\n",
      "# (https://facebook.github.io/watchman/) with git to speed up detecting\n",
      "# new and modified files.\n",
      "#\n",
      "# The hook is passed a version (currently 1) and a time in nanoseconds\n",
      "# formatted as a string and outputs to stdout all files that have been\n",
      "# modified since the given time. Paths must be relative to the root of\n",
      "# the working tree and separated by a single NUL.\n",
      "#\n",
      "# To enable this hook, rename this file to \"query-watchman\" and set\n",
      "# 'git config core.fsmonitor .git/hooks/query-watchman'\n",
      "#\n",
      "my ($version, $time) = @ARGV;\n",
      "\n",
      "# Check the hook interface version\n",
      "\n",
      "if ($version == 1) {\n",
      "\t# convert nanoseconds to seconds\n",
      "\t# subtract one second to make sure watchman will return all changes\n",
      "\t$time = int ($time / 1000000000) - 1;\n",
      "} else {\n",
      "\tdie \"Unsupported query-fsmonitor hook version '$version'.\\n\" .\n",
      "\t    \"Falling back to scanning...\\n\";\n",
      "}\n",
      "\n",
      "my $git_work_tree;\n",
      "if ($^O =~ 'msys' || $^O =~ 'cygwin') {\n",
      "\t$git_work_tree = Win32::GetCwd();\n",
      "\t$git_work_tree =~ tr/\\\\/\\//;\n",
      "} else {\n",
      "\trequire Cwd;\n",
      "\t$git_work_tree = Cwd::cwd();\n",
      "}\n",
      "\n",
      "my $retry = 1;\n",
      "\n",
      "launch_watchman();\n",
      "\n",
      "sub launch_watchman {\n",
      "\n",
      "\tmy $pid = open2(\\*CHLD_OUT, \\*CHLD_IN, 'watchman -j --no-pretty')\n",
      "\t    or die \"open2() failed: $!\\n\" .\n",
      "\t    \"Falling back to scanning...\\n\";\n",
      "\n",
      "\t# In the query expression below we're asking for names of files that\n",
      "\t# changed since $time but were not transient (ie created after\n",
      "\t# $time but no longer exist).\n",
      "\t#\n",
      "\t# To accomplish this, we're using the \"since\" generator to use the\n",
      "\t# recency index to select candidate nodes and \"fields\" to limit the\n",
      "\t# output to file names only.\n",
      "\n",
      "\tmy $query = <<\"\tEND\";\n",
      "\t\t[\"query\", \"$git_work_tree\", {\n",
      "\t\t\t\"since\": $time,\n",
      "\t\t\t\"fields\": [\"name\"]\n",
      "\t\t}]\n",
      "\tEND\n",
      "\n",
      "\tprint CHLD_IN $query;\n",
      "\tclose CHLD_IN;\n",
      "\tmy $response = do {local $/; <CHLD_OUT>};\n",
      "\n",
      "\tdie \"Watchman: command returned no output.\\n\" .\n",
      "\t    \"Falling back to scanning...\\n\" if $response eq \"\";\n",
      "\tdie \"Watchman: command returned invalid output: $response\\n\" .\n",
      "\t    \"Falling back to scanning...\\n\" unless $response =~ /^\\{/;\n",
      "\n",
      "\tmy $json_pkg;\n",
      "\teval {\n",
      "\t\trequire JSON::XS;\n",
      "\t\t$json_pkg = \"JSON::XS\";\n",
      "\t\t1;\n",
      "\t} or do {\n",
      "\t\trequire JSON::PP;\n",
      "\t\t$json_pkg = \"JSON::PP\";\n",
      "\t};\n",
      "\n",
      "\tmy $o = $json_pkg->new->utf8->decode($response);\n",
      "\n",
      "\tif ($retry > 0 and $o->{error} and $o->{error} =~ m/unable to resolve root .* directory (.*) is not watched/) {\n",
      "\t\tprint STDERR \"Adding '$git_work_tree' to watchman's watch list.\\n\";\n",
      "\t\t$retry--;\n",
      "\t\tqx/watchman watch \"$git_work_tree\"/;\n",
      "\t\tdie \"Failed to make watchman watch '$git_work_tree'.\\n\" .\n",
      "\t\t    \"Falling back to scanning...\\n\" if $? != 0;\n",
      "\n",
      "\t\t# Watchman will always return all files on the first query so\n",
      "\t\t# return the fast \"everything is dirty\" flag to git and do the\n",
      "\t\t# Watchman query just to get it over with now so we won't pay\n",
      "\t\t# the cost in git to look up each individual file.\n",
      "\t\tprint \"/\\0\";\n",
      "\t\teval { launch_watchman() };\n",
      "\t\texit 0;\n",
      "\t}\n",
      "\n",
      "\tdie \"Watchman: $o->{error}.\\n\" .\n",
      "\t    \"Falling back to scanning...\\n\" if $o->{error};\n",
      "\n",
      "\tbinmode STDOUT, \":utf8\";\n",
      "\tlocal $, = \"\\0\";\n",
      "\tprint @{$o->{files}};\n",
      "}\n",
      "\n",
      "[end of .git/hooks/fsmonitor-watchman.sample]\n",
      "[start of .git/hooks/pre-applypatch.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to verify what is about to be committed\n",
      "# by applypatch from an e-mail message.\n",
      "#\n",
      "# The hook should exit with non-zero status after issuing an\n",
      "# appropriate message if it wants to stop the commit.\n",
      "#\n",
      "# To enable this hook, rename this file to \"pre-applypatch\".\n",
      "\n",
      ". git-sh-setup\n",
      "precommit=\"$(git rev-parse --git-path hooks/pre-commit)\"\n",
      "test -x \"$precommit\" && exec \"$precommit\" ${1+\"$@\"}\n",
      ":\n",
      "\n",
      "[end of .git/hooks/pre-applypatch.sample]\n",
      "[start of .git/hooks/commit-msg.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to check the commit log message.\n",
      "# Called by \"git commit\" with one argument, the name of the file\n",
      "# that has the commit message.  The hook should exit with non-zero\n",
      "# status after issuing an appropriate message if it wants to stop the\n",
      "# commit.  The hook is allowed to edit the commit message file.\n",
      "#\n",
      "# To enable this hook, rename this file to \"commit-msg\".\n",
      "\n",
      "# Uncomment the below to add a Signed-off-by line to the message.\n",
      "# Doing this in a hook is a bad idea in general, but the prepare-commit-msg\n",
      "# hook is more suited to it.\n",
      "#\n",
      "# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\\(.*>\\).*$/Signed-off-by: \\1/p')\n",
      "# grep -qs \"^$SOB\" \"$1\" || echo \"$SOB\" >> \"$1\"\n",
      "\n",
      "# This example catches duplicate Signed-off-by lines.\n",
      "\n",
      "test \"\" = \"$(grep '^Signed-off-by: ' \"$1\" |\n",
      "\t sort | uniq -c | sed -e '/^[ \t]*1[ \t]/d')\" || {\n",
      "\techo >&2 Duplicate Signed-off-by lines.\n",
      "\texit 1\n",
      "}\n",
      "\n",
      "[end of .git/hooks/commit-msg.sample]\n",
      "[start of .git/hooks/prepare-commit-msg.sample]\n",
      "#!/bin/sh\n",
      "#\n",
      "# An example hook script to prepare the commit log message.\n",
      "# Called by \"git commit\" with the name of the file that has the\n",
      "# commit message, followed by the description of the commit\n",
      "# message's source.  The hook's purpose is to edit the commit\n",
      "# message file.  If the hook fails with a non-zero status,\n",
      "# the commit is aborted.\n",
      "#\n",
      "# To enable this hook, rename this file to \"prepare-commit-msg\".\n",
      "\n",
      "# This hook includes three examples. The first one removes the\n",
      "# \"# Please enter the commit message...\" help message.\n",
      "#\n",
      "# The second includes the output of \"git diff --name-status -r\"\n",
      "# into the message, just before the \"git status\" output.  It is\n",
      "# commented because it doesn't cope with --amend or with squashed\n",
      "# commits.\n",
      "#\n",
      "# The third example adds a Signed-off-by line to the message, that can\n",
      "# still be edited.  This is rarely a good idea.\n",
      "\n",
      "COMMIT_MSG_FILE=$1\n",
      "COMMIT_SOURCE=$2\n",
      "SHA1=$3\n",
      "\n",
      "/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' \"$COMMIT_MSG_FILE\"\n",
      "\n",
      "# case \"$COMMIT_SOURCE,$SHA1\" in\n",
      "#  ,|template,)\n",
      "#    /usr/bin/perl -i.bak -pe '\n",
      "#       print \"\\n\" . `git diff --cached --name-status -r`\n",
      "# \t if /^#/ && $first++ == 0' \"$COMMIT_MSG_FILE\" ;;\n",
      "#  *) ;;\n",
      "# esac\n",
      "\n",
      "# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\\(.*>\\).*$/Signed-off-by: \\1/p')\n",
      "# git interpret-trailers --in-place --trailer \"$SOB\" \"$COMMIT_MSG_FILE\"\n",
      "# if test -z \"$COMMIT_SOURCE\"\n",
      "# then\n",
      "#   /usr/bin/perl -i.bak -pe 'print \"\\n\" if !$first_line++' \"$COMMIT_MSG_FILE\"\n",
      "# fi\n",
      "\n",
      "[end of .git/hooks/prepare-commit-msg.sample]\n",
      "[start of .git/logs/refs/heads/main]\n",
      "0000000000000000000000000000000000000000 edb5f7a4572d0ac9193fb1fcc64ee28c74019946 Stefano Rando <stefano.rando.c@protonmail.com> 1739925436 -0800\tclone: from https://github.com/pypa/wheel.git\n",
      "\n",
      "[end of .git/logs/refs/heads/main]\n",
      "[start of .git/logs/refs/remotes/origin/HEAD]\n",
      "0000000000000000000000000000000000000000 edb5f7a4572d0ac9193fb1fcc64ee28c74019946 Stefano Rando <stefano.rando.c@protonmail.com> 1739925436 -0800\tclone: from https://github.com/pypa/wheel.git\n",
      "\n",
      "[end of .git/logs/refs/remotes/origin/HEAD]\n",
      "[start of .git/objects/pack/pack-aa906fb73da19762881b71d7dfe917e0a19474a8.pack]\n",
      "None\n",
      "[end of .git/objects/pack/pack-aa906fb73da19762881b71d7dfe917e0a19474a8.pack]\n",
      "[start of .git/objects/pack/pack-aa906fb73da19762881b71d7dfe917e0a19474a8.idx]\n",
      "None\n",
      "[end of .git/objects/pack/pack-aa906fb73da19762881b71d7dfe917e0a19474a8.idx]\n",
      "[start of .git/refs/heads/main]\n",
      "edb5f7a4572d0ac9193fb1fcc64ee28c74019946\n",
      "\n",
      "[end of .git/refs/heads/main]\n",
      "[start of .git/refs/remotes/origin/HEAD]\n",
      "ref: refs/remotes/origin/main\n",
      "\n",
      "[end of .git/refs/remotes/origin/HEAD]\n",
      "[start of .github/workflows/publish.yml]\n",
      "name: Publish packages to PyPI\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    tags:\n",
      "      - \"[0-9]+.[0-9]+.[0-9]+\"\n",
      "      - \"[0-9]+.[0-9]+.[0-9]+.post[0-9]+\"\n",
      "      - \"[0-9]+.[0-9]+.[0-9]+[a-b][0-9]+\"\n",
      "      - \"[0-9]+.[0-9]+.[0-9]+rc[0-9]+\"\n",
      "\n",
      "jobs:\n",
      "  build:\n",
      "    runs-on: ubuntu-latest\n",
      "    environment: release\n",
      "    steps:\n",
      "    - uses: actions/checkout@v4\n",
      "    - name: Set up Python\n",
      "      uses: actions/setup-python@v5\n",
      "      with:\n",
      "        python-version: 3.x\n",
      "    - name: Install dependencies\n",
      "      run: pip install flit\n",
      "    - name: Create packages\n",
      "      run: flit build --setup-py\n",
      "    - name: Store package artifacts\n",
      "      uses: actions/upload-artifact@v4\n",
      "      with:\n",
      "        name: dist\n",
      "        path: dist\n",
      "\n",
      "  publish:\n",
      "    needs: build\n",
      "    runs-on: ubuntu-latest\n",
      "    environment: release\n",
      "    permissions:\n",
      "      id-token: write\n",
      "    steps:\n",
      "    - name: Retrieve package artifacts\n",
      "      uses: actions/download-artifact@v4\n",
      "    - name: Upload packages\n",
      "      uses: pypa/gh-action-pypi-publish@release/v1\n",
      "\n",
      "  release:\n",
      "    name: Create a GitHub release\n",
      "    needs: build\n",
      "    runs-on: ubuntu-latest\n",
      "    permissions:\n",
      "      contents: write\n",
      "    steps:\n",
      "    - uses: actions/checkout@v4\n",
      "    - id: changelog\n",
      "      uses: agronholm/release-notes@v1\n",
      "      with:\n",
      "        path: docs/news.rst\n",
      "        version_pattern: \"^\\\\*\\\\*([0-9a-z.]+) \"\n",
      "    - uses: ncipollo/release-action@v1\n",
      "      with:\n",
      "        body: ${{ steps.changelog.outputs.changelog }}\n",
      "\n",
      "[end of .github/workflows/publish.yml]\n",
      "[start of .github/workflows/test.yml]\n",
      "name: Run the test suite\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches: [main]\n",
      "  pull_request:\n",
      "  workflow_dispatch:\n",
      "  schedule:\n",
      "    - cron: '0 6 * * 1'\n",
      "\n",
      "jobs:\n",
      "  test:\n",
      "    strategy:\n",
      "      fail-fast: false\n",
      "      matrix:\n",
      "        os: [ubuntu-latest]\n",
      "        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\", \"pypy-3.10\"]\n",
      "        include:\n",
      "        - os: macos-latest\n",
      "          python-version: \"3.8\"\n",
      "        - os: windows-latest\n",
      "          python-version: \"3.8\"\n",
      "        - os: macos-latest\n",
      "          python-version: \"3.13\"\n",
      "        - os: windows-latest\n",
      "          python-version: \"3.13\"\n",
      "        - os: macos-latest\n",
      "          python-version: \"pypy-3.10\"\n",
      "    runs-on: ${{ matrix.os }}\n",
      "    steps:\n",
      "    - uses: actions/checkout@v4\n",
      "    - name: Set up Python ${{ matrix.python-version }}\n",
      "      uses: actions/setup-python@v5\n",
      "      with:\n",
      "        python-version: ${{ matrix.python-version }}\n",
      "        allow-prereleases: true\n",
      "        cache: pip\n",
      "        cache-dependency-path: pyproject.toml\n",
      "    - name: Install the project\n",
      "      run: pip install --no-binary=wheel .\n",
      "    - name: Install test dependencies\n",
      "      run: pip install .[test] coverage[toml]\n",
      "    - name: Include SDist check dependencies\n",
      "      if: matrix.python-version == '3.12'\n",
      "      run: pip install build flit\n",
      "    - name: Test with pytest\n",
      "      run: coverage run -m pytest -v\n",
      "      env:\n",
      "        PYTHONWARNDEFAULTENCODING: 1\n",
      "    - name: Generate coverage report\n",
      "      run: coverage xml\n",
      "    - name: Send coverage data to Codecov\n",
      "      uses: codecov/codecov-action@v4\n",
      "      with:\n",
      "        file: coverage.xml\n",
      "      env:\n",
      "        CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\n",
      "\n",
      "[end of .github/workflows/test.yml]\n",
      "[start of src/wheel/metadata.py]\n",
      "\"\"\"\n",
      "Tools for converting old- to new-style metadata.\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import functools\n",
      "import itertools\n",
      "import os.path\n",
      "import re\n",
      "import textwrap\n",
      "from email.message import Message\n",
      "from email.parser import Parser\n",
      "from typing import Generator, Iterable, Iterator, Literal\n",
      "\n",
      "from .vendored.packaging.requirements import Requirement\n",
      "\n",
      "\n",
      "def _nonblank(str: str) -> bool | Literal[\"\"]:\n",
      "    return str and not str.startswith(\"#\")\n",
      "\n",
      "\n",
      "@functools.singledispatch\n",
      "def yield_lines(iterable: Iterable[str]) -> Iterator[str]:\n",
      "    r\"\"\"\n",
      "    Yield valid lines of a string or iterable.\n",
      "    >>> list(yield_lines(''))\n",
      "    []\n",
      "    >>> list(yield_lines(['foo', 'bar']))\n",
      "    ['foo', 'bar']\n",
      "    >>> list(yield_lines('foo\\nbar'))\n",
      "    ['foo', 'bar']\n",
      "    >>> list(yield_lines('\\nfoo\\n#bar\\nbaz #comment'))\n",
      "    ['foo', 'baz #comment']\n",
      "    >>> list(yield_lines(['foo\\nbar', 'baz', 'bing\\n\\n\\n']))\n",
      "    ['foo', 'bar', 'baz', 'bing']\n",
      "    \"\"\"\n",
      "    return itertools.chain.from_iterable(map(yield_lines, iterable))\n",
      "\n",
      "\n",
      "@yield_lines.register(str)\n",
      "def _(text: str) -> Iterator[str]:\n",
      "    return filter(_nonblank, map(str.strip, text.splitlines()))\n",
      "\n",
      "\n",
      "def split_sections(\n",
      "    s: str | Iterator[str],\n",
      ") -> Generator[tuple[str | None, list[str]], None, None]:\n",
      "    \"\"\"Split a string or iterable thereof into (section, content) pairs\n",
      "    Each ``section`` is a stripped version of the section header (\"[section]\")\n",
      "    and each ``content`` is a list of stripped lines excluding blank lines and\n",
      "    comment-only lines.  If there are any such lines before the first section\n",
      "    header, they're returned in a first ``section`` of ``None``.\n",
      "    \"\"\"\n",
      "    section = None\n",
      "    content: list[str] = []\n",
      "    for line in yield_lines(s):\n",
      "        if line.startswith(\"[\"):\n",
      "            if line.endswith(\"]\"):\n",
      "                if section or content:\n",
      "                    yield section, content\n",
      "                section = line[1:-1].strip()\n",
      "                content = []\n",
      "            else:\n",
      "                raise ValueError(\"Invalid section heading\", line)\n",
      "        else:\n",
      "            content.append(line)\n",
      "\n",
      "    # wrap up last segment\n",
      "    yield section, content\n",
      "\n",
      "\n",
      "def safe_extra(extra: str) -> str:\n",
      "    \"\"\"Convert an arbitrary string to a standard 'extra' name\n",
      "    Any runs of non-alphanumeric characters are replaced with a single '_',\n",
      "    and the result is always lowercased.\n",
      "    \"\"\"\n",
      "    return re.sub(\"[^A-Za-z0-9.-]+\", \"_\", extra).lower()\n",
      "\n",
      "\n",
      "def safe_name(name: str) -> str:\n",
      "    \"\"\"Convert an arbitrary string to a standard distribution name\n",
      "    Any runs of non-alphanumeric/. characters are replaced with a single '-'.\n",
      "    \"\"\"\n",
      "    return re.sub(\"[^A-Za-z0-9.]+\", \"-\", name)\n",
      "\n",
      "\n",
      "def requires_to_requires_dist(requirement: Requirement) -> str:\n",
      "    \"\"\"Return the version specifier for a requirement in PEP 345/566 fashion.\"\"\"\n",
      "    if requirement.url:\n",
      "        return \" @ \" + requirement.url\n",
      "\n",
      "    requires_dist: list[str] = []\n",
      "    for spec in requirement.specifier:\n",
      "        requires_dist.append(spec.operator + spec.version)\n",
      "\n",
      "    if requires_dist:\n",
      "        return \" \" + \",\".join(sorted(requires_dist))\n",
      "    else:\n",
      "        return \"\"\n",
      "\n",
      "\n",
      "def convert_requirements(requirements: list[str]) -> Iterator[str]:\n",
      "    \"\"\"Yield Requires-Dist: strings for parsed requirements strings.\"\"\"\n",
      "    for req in requirements:\n",
      "        parsed_requirement = Requirement(req)\n",
      "        spec = requires_to_requires_dist(parsed_requirement)\n",
      "        extras = \",\".join(sorted(safe_extra(e) for e in parsed_requirement.extras))\n",
      "        if extras:\n",
      "            extras = f\"[{extras}]\"\n",
      "\n",
      "        yield safe_name(parsed_requirement.name) + extras + spec\n",
      "\n",
      "\n",
      "def generate_requirements(\n",
      "    extras_require: dict[str | None, list[str]],\n",
      ") -> Iterator[tuple[str, str]]:\n",
      "    \"\"\"\n",
      "    Convert requirements from a setup()-style dictionary to\n",
      "    ('Requires-Dist', 'requirement') and ('Provides-Extra', 'extra') tuples.\n",
      "\n",
      "    extras_require is a dictionary of {extra: [requirements]} as passed to setup(),\n",
      "    using the empty extra {'': [requirements]} to hold install_requires.\n",
      "    \"\"\"\n",
      "    for extra, depends in extras_require.items():\n",
      "        condition = \"\"\n",
      "        extra = extra or \"\"\n",
      "        if \":\" in extra:  # setuptools extra:condition syntax\n",
      "            extra, condition = extra.split(\":\", 1)\n",
      "\n",
      "        extra = safe_extra(extra)\n",
      "        if extra:\n",
      "            yield \"Provides-Extra\", extra\n",
      "            if condition:\n",
      "                condition = \"(\" + condition + \") and \"\n",
      "            condition += f\"extra == '{extra}'\"\n",
      "\n",
      "        if condition:\n",
      "            condition = \" ; \" + condition\n",
      "\n",
      "        for new_req in convert_requirements(depends):\n",
      "            canonical_req = str(Requirement(new_req + condition))\n",
      "            yield \"Requires-Dist\", canonical_req\n",
      "\n",
      "\n",
      "def pkginfo_to_metadata(egg_info_path: str, pkginfo_path: str) -> Message:\n",
      "    \"\"\"\n",
      "    Convert .egg-info directory with PKG-INFO to the Metadata 2.1 format\n",
      "    \"\"\"\n",
      "    with open(pkginfo_path, encoding=\"utf-8\") as headers:\n",
      "        pkg_info = Parser().parse(headers)\n",
      "\n",
      "    pkg_info.replace_header(\"Metadata-Version\", \"2.1\")\n",
      "    # Those will be regenerated from `requires.txt`.\n",
      "    del pkg_info[\"Provides-Extra\"]\n",
      "    del pkg_info[\"Requires-Dist\"]\n",
      "    requires_path = os.path.join(egg_info_path, \"requires.txt\")\n",
      "    if os.path.exists(requires_path):\n",
      "        with open(requires_path, encoding=\"utf-8\") as requires_file:\n",
      "            requires = requires_file.read()\n",
      "\n",
      "        parsed_requirements = sorted(split_sections(requires), key=lambda x: x[0] or \"\")\n",
      "        for extra, reqs in parsed_requirements:\n",
      "            for key, value in generate_requirements({extra: reqs}):\n",
      "                if (key, value) not in pkg_info.items():\n",
      "                    pkg_info[key] = value\n",
      "\n",
      "    description = pkg_info[\"Description\"]\n",
      "    if description:\n",
      "        description_lines = pkg_info[\"Description\"].splitlines()\n",
      "        dedented_description = \"\\n\".join(\n",
      "            # if the first line of long_description is blank,\n",
      "            # the first line here will be indented.\n",
      "            (\n",
      "                description_lines[0].lstrip(),\n",
      "                textwrap.dedent(\"\\n\".join(description_lines[1:])),\n",
      "                \"\\n\",\n",
      "            )\n",
      "        )\n",
      "        pkg_info.set_payload(dedented_description)\n",
      "        del pkg_info[\"Description\"]\n",
      "\n",
      "    return pkg_info\n",
      "\n",
      "[end of src/wheel/metadata.py]\n",
      "[start of src/wheel/bdist_wheel.py]\n",
      "from typing import TYPE_CHECKING\n",
      "from warnings import warn\n",
      "\n",
      "warn(\n",
      "    \"The 'wheel' package is no longer the canonical location of the 'bdist_wheel' \"\n",
      "    \"command, and will be removed in a future release. Please update to setuptools \"\n",
      "    \"v70.1 or later which contains an integrated version of this command.\",\n",
      "    DeprecationWarning,\n",
      "    stacklevel=1,\n",
      ")\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from ._bdist_wheel import bdist_wheel as bdist_wheel\n",
      "else:\n",
      "    try:\n",
      "        # Better integration/compatibility with setuptools:\n",
      "        # in the case new fixes or PEPs are implemented in setuptools\n",
      "        # there is no need to backport them to the deprecated code base.\n",
      "        # This is useful in the case of old packages in the ecosystem\n",
      "        # that are still used but have low maintenance.\n",
      "        from setuptools.command.bdist_wheel import bdist_wheel\n",
      "    except ImportError:\n",
      "        # Only used in the case of old setuptools versions.\n",
      "        # If the user wants to get the latest fixes/PEPs,\n",
      "        # they are encouraged to address the deprecation warning.\n",
      "        from ._bdist_wheel import bdist_wheel as bdist_wheel\n",
      "\n",
      "[end of src/wheel/bdist_wheel.py]\n",
      "[start of src/wheel/wheelfile.py]\n",
      "from __future__ import annotations\n",
      "\n",
      "import csv\n",
      "import hashlib\n",
      "import os.path\n",
      "import re\n",
      "import stat\n",
      "import time\n",
      "from io import StringIO, TextIOWrapper\n",
      "from typing import IO, TYPE_CHECKING, Literal\n",
      "from zipfile import ZIP_DEFLATED, ZipFile, ZipInfo\n",
      "\n",
      "from wheel.cli import WheelError\n",
      "from wheel.util import log, urlsafe_b64decode, urlsafe_b64encode\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from typing import Protocol, Sized, Union\n",
      "\n",
      "    from typing_extensions import Buffer\n",
      "\n",
      "    StrPath = Union[str, os.PathLike[str]]\n",
      "\n",
      "    class SizedBuffer(Sized, Buffer, Protocol): ...\n",
      "\n",
      "\n",
      "# Non-greedy matching of an optional build number may be too clever (more\n",
      "# invalid wheel filenames will match). Separate regex for .dist-info?\n",
      "WHEEL_INFO_RE = re.compile(\n",
      "    r\"\"\"^(?P<namever>(?P<name>[^\\s-]+?)-(?P<ver>[^\\s-]+?))(-(?P<build>\\d[^\\s-]*))?\n",
      "     -(?P<pyver>[^\\s-]+?)-(?P<abi>[^\\s-]+?)-(?P<plat>\\S+)\\.whl$\"\"\",\n",
      "    re.VERBOSE,\n",
      ")\n",
      "MINIMUM_TIMESTAMP = 315532800  # 1980-01-01 00:00:00 UTC\n",
      "\n",
      "\n",
      "def get_zipinfo_datetime(timestamp: float | None = None):\n",
      "    # Some applications need reproducible .whl files, but they can't do this without\n",
      "    # forcing the timestamp of the individual ZipInfo objects. See issue #143.\n",
      "    timestamp = int(os.environ.get(\"SOURCE_DATE_EPOCH\", timestamp or time.time()))\n",
      "    timestamp = max(timestamp, MINIMUM_TIMESTAMP)\n",
      "    return time.gmtime(timestamp)[0:6]\n",
      "\n",
      "\n",
      "class WheelFile(ZipFile):\n",
      "    \"\"\"A ZipFile derivative class that also reads SHA-256 hashes from\n",
      "    .dist-info/RECORD and checks any read files against those.\n",
      "    \"\"\"\n",
      "\n",
      "    _default_algorithm = hashlib.sha256\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        file: StrPath,\n",
      "        mode: Literal[\"r\", \"w\", \"x\", \"a\"] = \"r\",\n",
      "        compression: int = ZIP_DEFLATED,\n",
      "    ):\n",
      "        basename = os.path.basename(file)\n",
      "        self.parsed_filename = WHEEL_INFO_RE.match(basename)\n",
      "        if not basename.endswith(\".whl\") or self.parsed_filename is None:\n",
      "            raise WheelError(f\"Bad wheel filename {basename!r}\")\n",
      "\n",
      "        ZipFile.__init__(self, file, mode, compression=compression, allowZip64=True)\n",
      "\n",
      "        self.dist_info_path = \"{}.dist-info\".format(\n",
      "            self.parsed_filename.group(\"namever\")\n",
      "        )\n",
      "        self.record_path = self.dist_info_path + \"/RECORD\"\n",
      "        self._file_hashes: dict[str, tuple[None, None] | tuple[int, bytes]] = {}\n",
      "        self._file_sizes = {}\n",
      "        if mode == \"r\":\n",
      "            # Ignore RECORD and any embedded wheel signatures\n",
      "            self._file_hashes[self.record_path] = None, None\n",
      "            self._file_hashes[self.record_path + \".jws\"] = None, None\n",
      "            self._file_hashes[self.record_path + \".p7s\"] = None, None\n",
      "\n",
      "            # Fill in the expected hashes by reading them from RECORD\n",
      "            try:\n",
      "                record = self.open(self.record_path)\n",
      "            except KeyError:\n",
      "                raise WheelError(f\"Missing {self.record_path} file\") from None\n",
      "\n",
      "            with record:\n",
      "                for line in csv.reader(\n",
      "                    TextIOWrapper(record, newline=\"\", encoding=\"utf-8\")\n",
      "                ):\n",
      "                    path, hash_sum, size = line\n",
      "                    if not hash_sum:\n",
      "                        continue\n",
      "\n",
      "                    algorithm, hash_sum = hash_sum.split(\"=\")\n",
      "                    try:\n",
      "                        hashlib.new(algorithm)\n",
      "                    except ValueError:\n",
      "                        raise WheelError(\n",
      "                            f\"Unsupported hash algorithm: {algorithm}\"\n",
      "                        ) from None\n",
      "\n",
      "                    if algorithm.lower() in {\"md5\", \"sha1\"}:\n",
      "                        raise WheelError(\n",
      "                            f\"Weak hash algorithm ({algorithm}) is not permitted by \"\n",
      "                            f\"PEP 427\"\n",
      "                        )\n",
      "\n",
      "                    self._file_hashes[path] = (\n",
      "                        algorithm,\n",
      "                        urlsafe_b64decode(hash_sum.encode(\"ascii\")),\n",
      "                    )\n",
      "\n",
      "    def open(\n",
      "        self,\n",
      "        name_or_info: str | ZipInfo,\n",
      "        mode: Literal[\"r\", \"w\"] = \"r\",\n",
      "        pwd: bytes | None = None,\n",
      "    ) -> IO[bytes]:\n",
      "        def _update_crc(newdata: bytes) -> None:\n",
      "            eof = ef._eof\n",
      "            update_crc_orig(newdata)\n",
      "            running_hash.update(newdata)\n",
      "            if eof and running_hash.digest() != expected_hash:\n",
      "                raise WheelError(f\"Hash mismatch for file '{ef_name}'\")\n",
      "\n",
      "        ef_name = (\n",
      "            name_or_info.filename if isinstance(name_or_info, ZipInfo) else name_or_info\n",
      "        )\n",
      "        if (\n",
      "            mode == \"r\"\n",
      "            and not ef_name.endswith(\"/\")\n",
      "            and ef_name not in self._file_hashes\n",
      "        ):\n",
      "            raise WheelError(f\"No hash found for file '{ef_name}'\")\n",
      "\n",
      "        ef = ZipFile.open(self, name_or_info, mode, pwd)\n",
      "        if mode == \"r\" and not ef_name.endswith(\"/\"):\n",
      "            algorithm, expected_hash = self._file_hashes[ef_name]\n",
      "            if expected_hash is not None:\n",
      "                # Monkey patch the _update_crc method to also check for the hash from\n",
      "                # RECORD\n",
      "                running_hash = hashlib.new(algorithm)\n",
      "                update_crc_orig, ef._update_crc = ef._update_crc, _update_crc\n",
      "\n",
      "        return ef\n",
      "\n",
      "    def write_files(self, base_dir: str):\n",
      "        log.info(f\"creating '{self.filename}' and adding '{base_dir}' to it\")\n",
      "        deferred: list[tuple[str, str]] = []\n",
      "        for root, dirnames, filenames in os.walk(base_dir):\n",
      "            # Sort the directory names so that `os.walk` will walk them in a\n",
      "            # defined order on the next iteration.\n",
      "            dirnames.sort()\n",
      "            for name in sorted(filenames):\n",
      "                path = os.path.normpath(os.path.join(root, name))\n",
      "                if os.path.isfile(path):\n",
      "                    arcname = os.path.relpath(path, base_dir).replace(os.path.sep, \"/\")\n",
      "                    if arcname == self.record_path:\n",
      "                        pass\n",
      "                    elif root.endswith(\".dist-info\"):\n",
      "                        deferred.append((path, arcname))\n",
      "                    else:\n",
      "                        self.write(path, arcname)\n",
      "\n",
      "        deferred.sort()\n",
      "        for path, arcname in deferred:\n",
      "            self.write(path, arcname)\n",
      "\n",
      "    def write(\n",
      "        self,\n",
      "        filename: str,\n",
      "        arcname: str | None = None,\n",
      "        compress_type: int | None = None,\n",
      "    ) -> None:\n",
      "        with open(filename, \"rb\") as f:\n",
      "            st = os.fstat(f.fileno())\n",
      "            data = f.read()\n",
      "\n",
      "        zinfo = ZipInfo(\n",
      "            arcname or filename, date_time=get_zipinfo_datetime(st.st_mtime)\n",
      "        )\n",
      "        zinfo.external_attr = (stat.S_IMODE(st.st_mode) | stat.S_IFMT(st.st_mode)) << 16\n",
      "        zinfo.compress_type = compress_type or self.compression\n",
      "        self.writestr(zinfo, data, compress_type)\n",
      "\n",
      "    def writestr(\n",
      "        self,\n",
      "        zinfo_or_arcname: str | ZipInfo,\n",
      "        data: SizedBuffer | str,\n",
      "        compress_type: int | None = None,\n",
      "    ):\n",
      "        if isinstance(zinfo_or_arcname, str):\n",
      "            zinfo_or_arcname = ZipInfo(\n",
      "                zinfo_or_arcname, date_time=get_zipinfo_datetime()\n",
      "            )\n",
      "            zinfo_or_arcname.compress_type = self.compression\n",
      "            zinfo_or_arcname.external_attr = (0o664 | stat.S_IFREG) << 16\n",
      "\n",
      "        if isinstance(data, str):\n",
      "            data = data.encode(\"utf-8\")\n",
      "\n",
      "        ZipFile.writestr(self, zinfo_or_arcname, data, compress_type)\n",
      "        fname = (\n",
      "            zinfo_or_arcname.filename\n",
      "            if isinstance(zinfo_or_arcname, ZipInfo)\n",
      "            else zinfo_or_arcname\n",
      "        )\n",
      "        log.info(f\"adding '{fname}'\")\n",
      "        if fname != self.record_path:\n",
      "            hash_ = self._default_algorithm(data)\n",
      "            self._file_hashes[fname] = (\n",
      "                hash_.name,\n",
      "                urlsafe_b64encode(hash_.digest()).decode(\"ascii\"),\n",
      "            )\n",
      "            self._file_sizes[fname] = len(data)\n",
      "\n",
      "    def close(self):\n",
      "        # Write RECORD\n",
      "        if self.fp is not None and self.mode == \"w\" and self._file_hashes:\n",
      "            data = StringIO()\n",
      "            writer = csv.writer(data, delimiter=\",\", quotechar='\"', lineterminator=\"\\n\")\n",
      "            writer.writerows(\n",
      "                (\n",
      "                    (fname, algorithm + \"=\" + hash_, self._file_sizes[fname])\n",
      "                    for fname, (algorithm, hash_) in self._file_hashes.items()\n",
      "                )\n",
      "            )\n",
      "            writer.writerow((format(self.record_path), \"\", \"\"))\n",
      "            self.writestr(self.record_path, data.getvalue())\n",
      "\n",
      "        ZipFile.close(self)\n",
      "\n",
      "[end of src/wheel/wheelfile.py]\n",
      "[start of src/wheel/__init__.py]\n",
      "from __future__ import annotations\n",
      "\n",
      "__version__ = \"0.45.1\"\n",
      "\n",
      "[end of src/wheel/__init__.py]\n",
      "[start of src/wheel/util.py]\n",
      "from __future__ import annotations\n",
      "\n",
      "import base64\n",
      "import logging\n",
      "\n",
      "log = logging.getLogger(\"wheel\")\n",
      "\n",
      "\n",
      "def urlsafe_b64encode(data: bytes) -> bytes:\n",
      "    \"\"\"urlsafe_b64encode without padding\"\"\"\n",
      "    return base64.urlsafe_b64encode(data).rstrip(b\"=\")\n",
      "\n",
      "\n",
      "def urlsafe_b64decode(data: bytes) -> bytes:\n",
      "    \"\"\"urlsafe_b64decode without padding\"\"\"\n",
      "    pad = b\"=\" * (4 - (len(data) & 3))\n",
      "    return base64.urlsafe_b64decode(data + pad)\n",
      "\n",
      "[end of src/wheel/util.py]\n",
      "[start of src/wheel/_setuptools_logging.py]\n",
      "# copied from setuptools.logging, omitting monkeypatching\n",
      "from __future__ import annotations\n",
      "\n",
      "import logging\n",
      "import sys\n",
      "\n",
      "\n",
      "def _not_warning(record: logging.LogRecord) -> bool:\n",
      "    return record.levelno < logging.WARNING\n",
      "\n",
      "\n",
      "def configure() -> None:\n",
      "    \"\"\"\n",
      "    Configure logging to emit warning and above to stderr\n",
      "    and everything else to stdout. This behavior is provided\n",
      "    for compatibility with distutils.log but may change in\n",
      "    the future.\n",
      "    \"\"\"\n",
      "    err_handler = logging.StreamHandler()\n",
      "    err_handler.setLevel(logging.WARNING)\n",
      "    out_handler = logging.StreamHandler(sys.stdout)\n",
      "    out_handler.addFilter(_not_warning)\n",
      "    handlers = err_handler, out_handler\n",
      "    logging.basicConfig(\n",
      "        format=\"{message}\", style=\"{\", handlers=handlers, level=logging.DEBUG\n",
      "    )\n",
      "\n",
      "[end of src/wheel/_setuptools_logging.py]\n",
      "[start of src/wheel/macosx_libfile.py]\n",
      "\"\"\"\n",
      "This module contains function to analyse dynamic library\n",
      "headers to extract system information\n",
      "\n",
      "Currently only for MacOSX\n",
      "\n",
      "Library file on macosx system starts with Mach-O or Fat field.\n",
      "This can be distinguish by first 32 bites and it is called magic number.\n",
      "Proper value of magic number is with suffix _MAGIC. Suffix _CIGAM means\n",
      "reversed bytes order.\n",
      "Both fields can occur in two types: 32 and 64 bytes.\n",
      "\n",
      "FAT field inform that this library contains few version of library\n",
      "(typically for different types version). It contains\n",
      "information where Mach-O headers starts.\n",
      "\n",
      "Each section started with Mach-O header contains one library\n",
      "(So if file starts with this field it contains only one version).\n",
      "\n",
      "After filed Mach-O there are section fields.\n",
      "Each of them starts with two fields:\n",
      "cmd - magic number for this command\n",
      "cmdsize - total size occupied by this section information.\n",
      "\n",
      "In this case only sections LC_VERSION_MIN_MACOSX (for macosx 10.13 and earlier)\n",
      "and LC_BUILD_VERSION (for macosx 10.14 and newer) are interesting,\n",
      "because them contains information about minimal system version.\n",
      "\n",
      "Important remarks:\n",
      "- For fat files this implementation looks for maximum number version.\n",
      "  It not check if it is 32 or 64 and do not compare it with currently built package.\n",
      "  So it is possible to false report higher version that needed.\n",
      "- All structures signatures are taken form macosx header files.\n",
      "- I think that binary format will be more stable than `otool` output.\n",
      "  and if apple introduce some changes both implementation will need to be updated.\n",
      "- The system compile will set the deployment target no lower than\n",
      "  11.0 for arm64 builds. For \"Universal 2\" builds use the x86_64 deployment\n",
      "  target when the arm64 target is 11.0.\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import ctypes\n",
      "import os\n",
      "import sys\n",
      "from io import BufferedIOBase\n",
      "from typing import TYPE_CHECKING\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from typing import Union\n",
      "\n",
      "    StrPath = Union[str, os.PathLike[str]]\n",
      "\n",
      "\"\"\"here the needed const and struct from mach-o header files\"\"\"\n",
      "\n",
      "FAT_MAGIC = 0xCAFEBABE\n",
      "FAT_CIGAM = 0xBEBAFECA\n",
      "FAT_MAGIC_64 = 0xCAFEBABF\n",
      "FAT_CIGAM_64 = 0xBFBAFECA\n",
      "MH_MAGIC = 0xFEEDFACE\n",
      "MH_CIGAM = 0xCEFAEDFE\n",
      "MH_MAGIC_64 = 0xFEEDFACF\n",
      "MH_CIGAM_64 = 0xCFFAEDFE\n",
      "\n",
      "LC_VERSION_MIN_MACOSX = 0x24\n",
      "LC_BUILD_VERSION = 0x32\n",
      "\n",
      "CPU_TYPE_ARM64 = 0x0100000C\n",
      "\n",
      "mach_header_fields = [\n",
      "    (\"magic\", ctypes.c_uint32),\n",
      "    (\"cputype\", ctypes.c_int),\n",
      "    (\"cpusubtype\", ctypes.c_int),\n",
      "    (\"filetype\", ctypes.c_uint32),\n",
      "    (\"ncmds\", ctypes.c_uint32),\n",
      "    (\"sizeofcmds\", ctypes.c_uint32),\n",
      "    (\"flags\", ctypes.c_uint32),\n",
      "]\n",
      "\"\"\"\n",
      "struct mach_header {\n",
      "    uint32_t\tmagic;\t\t/* mach magic number identifier */\n",
      "    cpu_type_t\tcputype;\t/* cpu specifier */\n",
      "    cpu_subtype_t\tcpusubtype;\t/* machine specifier */\n",
      "    uint32_t\tfiletype;\t/* type of file */\n",
      "    uint32_t\tncmds;\t\t/* number of load commands */\n",
      "    uint32_t\tsizeofcmds;\t/* the size of all the load commands */\n",
      "    uint32_t\tflags;\t\t/* flags */\n",
      "};\n",
      "typedef integer_t cpu_type_t;\n",
      "typedef integer_t cpu_subtype_t;\n",
      "\"\"\"\n",
      "\n",
      "mach_header_fields_64 = mach_header_fields + [(\"reserved\", ctypes.c_uint32)]\n",
      "\"\"\"\n",
      "struct mach_header_64 {\n",
      "    uint32_t\tmagic;\t\t/* mach magic number identifier */\n",
      "    cpu_type_t\tcputype;\t/* cpu specifier */\n",
      "    cpu_subtype_t\tcpusubtype;\t/* machine specifier */\n",
      "    uint32_t\tfiletype;\t/* type of file */\n",
      "    uint32_t\tncmds;\t\t/* number of load commands */\n",
      "    uint32_t\tsizeofcmds;\t/* the size of all the load commands */\n",
      "    uint32_t\tflags;\t\t/* flags */\n",
      "    uint32_t\treserved;\t/* reserved */\n",
      "};\n",
      "\"\"\"\n",
      "\n",
      "fat_header_fields = [(\"magic\", ctypes.c_uint32), (\"nfat_arch\", ctypes.c_uint32)]\n",
      "\"\"\"\n",
      "struct fat_header {\n",
      "    uint32_t\tmagic;\t\t/* FAT_MAGIC or FAT_MAGIC_64 */\n",
      "    uint32_t\tnfat_arch;\t/* number of structs that follow */\n",
      "};\n",
      "\"\"\"\n",
      "\n",
      "fat_arch_fields = [\n",
      "    (\"cputype\", ctypes.c_int),\n",
      "    (\"cpusubtype\", ctypes.c_int),\n",
      "    (\"offset\", ctypes.c_uint32),\n",
      "    (\"size\", ctypes.c_uint32),\n",
      "    (\"align\", ctypes.c_uint32),\n",
      "]\n",
      "\"\"\"\n",
      "struct fat_arch {\n",
      "    cpu_type_t\tcputype;\t/* cpu specifier (int) */\n",
      "    cpu_subtype_t\tcpusubtype;\t/* machine specifier (int) */\n",
      "    uint32_t\toffset;\t\t/* file offset to this object file */\n",
      "    uint32_t\tsize;\t\t/* size of this object file */\n",
      "    uint32_t\talign;\t\t/* alignment as a power of 2 */\n",
      "};\n",
      "\"\"\"\n",
      "\n",
      "fat_arch_64_fields = [\n",
      "    (\"cputype\", ctypes.c_int),\n",
      "    (\"cpusubtype\", ctypes.c_int),\n",
      "    (\"offset\", ctypes.c_uint64),\n",
      "    (\"size\", ctypes.c_uint64),\n",
      "    (\"align\", ctypes.c_uint32),\n",
      "    (\"reserved\", ctypes.c_uint32),\n",
      "]\n",
      "\"\"\"\n",
      "struct fat_arch_64 {\n",
      "    cpu_type_t\tcputype;\t/* cpu specifier (int) */\n",
      "    cpu_subtype_t\tcpusubtype;\t/* machine specifier (int) */\n",
      "    uint64_t\toffset;\t\t/* file offset to this object file */\n",
      "    uint64_t\tsize;\t\t/* size of this object file */\n",
      "    uint32_t\talign;\t\t/* alignment as a power of 2 */\n",
      "    uint32_t\treserved;\t/* reserved */\n",
      "};\n",
      "\"\"\"\n",
      "\n",
      "segment_base_fields = [(\"cmd\", ctypes.c_uint32), (\"cmdsize\", ctypes.c_uint32)]\n",
      "\"\"\"base for reading segment info\"\"\"\n",
      "\n",
      "segment_command_fields = [\n",
      "    (\"cmd\", ctypes.c_uint32),\n",
      "    (\"cmdsize\", ctypes.c_uint32),\n",
      "    (\"segname\", ctypes.c_char * 16),\n",
      "    (\"vmaddr\", ctypes.c_uint32),\n",
      "    (\"vmsize\", ctypes.c_uint32),\n",
      "    (\"fileoff\", ctypes.c_uint32),\n",
      "    (\"filesize\", ctypes.c_uint32),\n",
      "    (\"maxprot\", ctypes.c_int),\n",
      "    (\"initprot\", ctypes.c_int),\n",
      "    (\"nsects\", ctypes.c_uint32),\n",
      "    (\"flags\", ctypes.c_uint32),\n",
      "]\n",
      "\"\"\"\n",
      "struct segment_command { /* for 32-bit architectures */\n",
      "    uint32_t\tcmd;\t\t/* LC_SEGMENT */\n",
      "    uint32_t\tcmdsize;\t/* includes sizeof section structs */\n",
      "    char\t\tsegname[16];\t/* segment name */\n",
      "    uint32_t\tvmaddr;\t\t/* memory address of this segment */\n",
      "    uint32_t\tvmsize;\t\t/* memory size of this segment */\n",
      "    uint32_t\tfileoff;\t/* file offset of this segment */\n",
      "    uint32_t\tfilesize;\t/* amount to map from the file */\n",
      "    vm_prot_t\tmaxprot;\t/* maximum VM protection */\n",
      "    vm_prot_t\tinitprot;\t/* initial VM protection */\n",
      "    uint32_t\tnsects;\t\t/* number of sections in segment */\n",
      "    uint32_t\tflags;\t\t/* flags */\n",
      "};\n",
      "typedef int vm_prot_t;\n",
      "\"\"\"\n",
      "\n",
      "segment_command_fields_64 = [\n",
      "    (\"cmd\", ctypes.c_uint32),\n",
      "    (\"cmdsize\", ctypes.c_uint32),\n",
      "    (\"segname\", ctypes.c_char * 16),\n",
      "    (\"vmaddr\", ctypes.c_uint64),\n",
      "    (\"vmsize\", ctypes.c_uint64),\n",
      "    (\"fileoff\", ctypes.c_uint64),\n",
      "    (\"filesize\", ctypes.c_uint64),\n",
      "    (\"maxprot\", ctypes.c_int),\n",
      "    (\"initprot\", ctypes.c_int),\n",
      "    (\"nsects\", ctypes.c_uint32),\n",
      "    (\"flags\", ctypes.c_uint32),\n",
      "]\n",
      "\"\"\"\n",
      "struct segment_command_64 { /* for 64-bit architectures */\n",
      "    uint32_t\tcmd;\t\t/* LC_SEGMENT_64 */\n",
      "    uint32_t\tcmdsize;\t/* includes sizeof section_64 structs */\n",
      "    char\t\tsegname[16];\t/* segment name */\n",
      "    uint64_t\tvmaddr;\t\t/* memory address of this segment */\n",
      "    uint64_t\tvmsize;\t\t/* memory size of this segment */\n",
      "    uint64_t\tfileoff;\t/* file offset of this segment */\n",
      "    uint64_t\tfilesize;\t/* amount to map from the file */\n",
      "    vm_prot_t\tmaxprot;\t/* maximum VM protection */\n",
      "    vm_prot_t\tinitprot;\t/* initial VM protection */\n",
      "    uint32_t\tnsects;\t\t/* number of sections in segment */\n",
      "    uint32_t\tflags;\t\t/* flags */\n",
      "};\n",
      "\"\"\"\n",
      "\n",
      "version_min_command_fields = segment_base_fields + [\n",
      "    (\"version\", ctypes.c_uint32),\n",
      "    (\"sdk\", ctypes.c_uint32),\n",
      "]\n",
      "\"\"\"\n",
      "struct version_min_command {\n",
      "    uint32_t\tcmd;\t\t/* LC_VERSION_MIN_MACOSX or\n",
      "                               LC_VERSION_MIN_IPHONEOS or\n",
      "                               LC_VERSION_MIN_WATCHOS or\n",
      "                               LC_VERSION_MIN_TVOS */\n",
      "    uint32_t\tcmdsize;\t/* sizeof(struct min_version_command) */\n",
      "    uint32_t\tversion;\t/* X.Y.Z is encoded in nibbles xxxx.yy.zz */\n",
      "    uint32_t\tsdk;\t\t/* X.Y.Z is encoded in nibbles xxxx.yy.zz */\n",
      "};\n",
      "\"\"\"\n",
      "\n",
      "build_version_command_fields = segment_base_fields + [\n",
      "    (\"platform\", ctypes.c_uint32),\n",
      "    (\"minos\", ctypes.c_uint32),\n",
      "    (\"sdk\", ctypes.c_uint32),\n",
      "    (\"ntools\", ctypes.c_uint32),\n",
      "]\n",
      "\"\"\"\n",
      "struct build_version_command {\n",
      "    uint32_t\tcmd;\t\t/* LC_BUILD_VERSION */\n",
      "    uint32_t\tcmdsize;\t/* sizeof(struct build_version_command) plus */\n",
      "                                /* ntools * sizeof(struct build_tool_version) */\n",
      "    uint32_t\tplatform;\t/* platform */\n",
      "    uint32_t\tminos;\t\t/* X.Y.Z is encoded in nibbles xxxx.yy.zz */\n",
      "    uint32_t\tsdk;\t\t/* X.Y.Z is encoded in nibbles xxxx.yy.zz */\n",
      "    uint32_t\tntools;\t\t/* number of tool entries following this */\n",
      "};\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def swap32(x: int) -> int:\n",
      "    return (\n",
      "        ((x << 24) & 0xFF000000)\n",
      "        | ((x << 8) & 0x00FF0000)\n",
      "        | ((x >> 8) & 0x0000FF00)\n",
      "        | ((x >> 24) & 0x000000FF)\n",
      "    )\n",
      "\n",
      "\n",
      "def get_base_class_and_magic_number(\n",
      "    lib_file: BufferedIOBase,\n",
      "    seek: int | None = None,\n",
      ") -> tuple[type[ctypes.Structure], int]:\n",
      "    if seek is None:\n",
      "        seek = lib_file.tell()\n",
      "    else:\n",
      "        lib_file.seek(seek)\n",
      "    magic_number = ctypes.c_uint32.from_buffer_copy(\n",
      "        lib_file.read(ctypes.sizeof(ctypes.c_uint32))\n",
      "    ).value\n",
      "\n",
      "    # Handle wrong byte order\n",
      "    if magic_number in [FAT_CIGAM, FAT_CIGAM_64, MH_CIGAM, MH_CIGAM_64]:\n",
      "        if sys.byteorder == \"little\":\n",
      "            BaseClass = ctypes.BigEndianStructure\n",
      "        else:\n",
      "            BaseClass = ctypes.LittleEndianStructure\n",
      "\n",
      "        magic_number = swap32(magic_number)\n",
      "    else:\n",
      "        BaseClass = ctypes.Structure\n",
      "\n",
      "    lib_file.seek(seek)\n",
      "    return BaseClass, magic_number\n",
      "\n",
      "\n",
      "def read_data(struct_class: type[ctypes.Structure], lib_file: BufferedIOBase):\n",
      "    return struct_class.from_buffer_copy(lib_file.read(ctypes.sizeof(struct_class)))\n",
      "\n",
      "\n",
      "def extract_macosx_min_system_version(path_to_lib: str):\n",
      "    with open(path_to_lib, \"rb\") as lib_file:\n",
      "        BaseClass, magic_number = get_base_class_and_magic_number(lib_file, 0)\n",
      "        if magic_number not in [FAT_MAGIC, FAT_MAGIC_64, MH_MAGIC, MH_MAGIC_64]:\n",
      "            return\n",
      "\n",
      "        if magic_number in [FAT_MAGIC, FAT_CIGAM_64]:\n",
      "\n",
      "            class FatHeader(BaseClass):\n",
      "                _fields_ = fat_header_fields\n",
      "\n",
      "            fat_header = read_data(FatHeader, lib_file)\n",
      "            if magic_number == FAT_MAGIC:\n",
      "\n",
      "                class FatArch(BaseClass):\n",
      "                    _fields_ = fat_arch_fields\n",
      "\n",
      "            else:\n",
      "\n",
      "                class FatArch(BaseClass):\n",
      "                    _fields_ = fat_arch_64_fields\n",
      "\n",
      "            fat_arch_list = [\n",
      "                read_data(FatArch, lib_file) for _ in range(fat_header.nfat_arch)\n",
      "            ]\n",
      "\n",
      "            versions_list: list[tuple[int, int, int]] = []\n",
      "            for el in fat_arch_list:\n",
      "                try:\n",
      "                    version = read_mach_header(lib_file, el.offset)\n",
      "                    if version is not None:\n",
      "                        if el.cputype == CPU_TYPE_ARM64 and len(fat_arch_list) != 1:\n",
      "                            # Xcode will not set the deployment target below 11.0.0\n",
      "                            # for the arm64 architecture. Ignore the arm64 deployment\n",
      "                            # in fat binaries when the target is 11.0.0, that way\n",
      "                            # the other architectures can select a lower deployment\n",
      "                            # target.\n",
      "                            # This is safe because there is no arm64 variant for\n",
      "                            # macOS 10.15 or earlier.\n",
      "                            if version == (11, 0, 0):\n",
      "                                continue\n",
      "                        versions_list.append(version)\n",
      "                except ValueError:\n",
      "                    pass\n",
      "\n",
      "            if len(versions_list) > 0:\n",
      "                return max(versions_list)\n",
      "            else:\n",
      "                return None\n",
      "\n",
      "        else:\n",
      "            try:\n",
      "                return read_mach_header(lib_file, 0)\n",
      "            except ValueError:\n",
      "                \"\"\"when some error during read library files\"\"\"\n",
      "                return None\n",
      "\n",
      "\n",
      "def read_mach_header(\n",
      "    lib_file: BufferedIOBase,\n",
      "    seek: int | None = None,\n",
      ") -> tuple[int, int, int] | None:\n",
      "    \"\"\"\n",
      "    This function parses a Mach-O header and extracts\n",
      "    information about the minimal macOS version.\n",
      "\n",
      "    :param lib_file: reference to opened library file with pointer\n",
      "    \"\"\"\n",
      "    base_class, magic_number = get_base_class_and_magic_number(lib_file, seek)\n",
      "    arch = \"32\" if magic_number == MH_MAGIC else \"64\"\n",
      "\n",
      "    class SegmentBase(base_class):\n",
      "        _fields_ = segment_base_fields\n",
      "\n",
      "    if arch == \"32\":\n",
      "\n",
      "        class MachHeader(base_class):\n",
      "            _fields_ = mach_header_fields\n",
      "\n",
      "    else:\n",
      "\n",
      "        class MachHeader(base_class):\n",
      "            _fields_ = mach_header_fields_64\n",
      "\n",
      "    mach_header = read_data(MachHeader, lib_file)\n",
      "    for _i in range(mach_header.ncmds):\n",
      "        pos = lib_file.tell()\n",
      "        segment_base = read_data(SegmentBase, lib_file)\n",
      "        lib_file.seek(pos)\n",
      "        if segment_base.cmd == LC_VERSION_MIN_MACOSX:\n",
      "\n",
      "            class VersionMinCommand(base_class):\n",
      "                _fields_ = version_min_command_fields\n",
      "\n",
      "            version_info = read_data(VersionMinCommand, lib_file)\n",
      "            return parse_version(version_info.version)\n",
      "        elif segment_base.cmd == LC_BUILD_VERSION:\n",
      "\n",
      "            class VersionBuild(base_class):\n",
      "                _fields_ = build_version_command_fields\n",
      "\n",
      "            version_info = read_data(VersionBuild, lib_file)\n",
      "            return parse_version(version_info.minos)\n",
      "        else:\n",
      "            lib_file.seek(pos + segment_base.cmdsize)\n",
      "            continue\n",
      "\n",
      "\n",
      "def parse_version(version: int) -> tuple[int, int, int]:\n",
      "    x = (version & 0xFFFF0000) >> 16\n",
      "    y = (version & 0x0000FF00) >> 8\n",
      "    z = version & 0x000000FF\n",
      "    return x, y, z\n",
      "\n",
      "\n",
      "def calculate_macosx_platform_tag(archive_root: StrPath, platform_tag: str) -> str:\n",
      "    \"\"\"\n",
      "    Calculate proper macosx platform tag basing on files which are included to wheel\n",
      "\n",
      "    Example platform tag `macosx-10.14-x86_64`\n",
      "    \"\"\"\n",
      "    prefix, base_version, suffix = platform_tag.split(\"-\")\n",
      "    base_version = tuple(int(x) for x in base_version.split(\".\"))\n",
      "    base_version = base_version[:2]\n",
      "    if base_version[0] > 10:\n",
      "        base_version = (base_version[0], 0)\n",
      "    assert len(base_version) == 2\n",
      "    if \"MACOSX_DEPLOYMENT_TARGET\" in os.environ:\n",
      "        deploy_target = tuple(\n",
      "            int(x) for x in os.environ[\"MACOSX_DEPLOYMENT_TARGET\"].split(\".\")\n",
      "        )\n",
      "        deploy_target = deploy_target[:2]\n",
      "        if deploy_target[0] > 10:\n",
      "            deploy_target = (deploy_target[0], 0)\n",
      "        if deploy_target < base_version:\n",
      "            sys.stderr.write(\n",
      "                \"[WARNING] MACOSX_DEPLOYMENT_TARGET is set to a lower value ({}) than \"\n",
      "                \"the version on which the Python interpreter was compiled ({}), and \"\n",
      "                \"will be ignored.\\n\".format(\n",
      "                    \".\".join(str(x) for x in deploy_target),\n",
      "                    \".\".join(str(x) for x in base_version),\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            base_version = deploy_target\n",
      "\n",
      "    assert len(base_version) == 2\n",
      "    start_version = base_version\n",
      "    versions_dict: dict[str, tuple[int, int]] = {}\n",
      "    for dirpath, _dirnames, filenames in os.walk(archive_root):\n",
      "        for filename in filenames:\n",
      "            if filename.endswith(\".dylib\") or filename.endswith(\".so\"):\n",
      "                lib_path = os.path.join(dirpath, filename)\n",
      "                min_ver = extract_macosx_min_system_version(lib_path)\n",
      "                if min_ver is not None:\n",
      "                    min_ver = min_ver[0:2]\n",
      "                    if min_ver[0] > 10:\n",
      "                        min_ver = (min_ver[0], 0)\n",
      "                    versions_dict[lib_path] = min_ver\n",
      "\n",
      "    if len(versions_dict) > 0:\n",
      "        base_version = max(base_version, max(versions_dict.values()))\n",
      "\n",
      "    # macosx platform tag do not support minor bugfix release\n",
      "    fin_base_version = \"_\".join([str(x) for x in base_version])\n",
      "    if start_version < base_version:\n",
      "        problematic_files = [k for k, v in versions_dict.items() if v > start_version]\n",
      "        problematic_files = \"\\n\".join(problematic_files)\n",
      "        if len(problematic_files) == 1:\n",
      "            files_form = \"this file\"\n",
      "        else:\n",
      "            files_form = \"these files\"\n",
      "        error_message = (\n",
      "            \"[WARNING] This wheel needs a higher macOS version than {}  \"\n",
      "            \"To silence this warning, set MACOSX_DEPLOYMENT_TARGET to at least \"\n",
      "            + fin_base_version\n",
      "            + \" or recreate \"\n",
      "            + files_form\n",
      "            + \" with lower \"\n",
      "            \"MACOSX_DEPLOYMENT_TARGET:  \\n\" + problematic_files\n",
      "        )\n",
      "\n",
      "        if \"MACOSX_DEPLOYMENT_TARGET\" in os.environ:\n",
      "            error_message = error_message.format(\n",
      "                \"is set in MACOSX_DEPLOYMENT_TARGET variable.\"\n",
      "            )\n",
      "        else:\n",
      "            error_message = error_message.format(\n",
      "                \"the version your Python interpreter is compiled against.\"\n",
      "            )\n",
      "\n",
      "        sys.stderr.write(error_message)\n",
      "\n",
      "    platform_tag = prefix + \"_\" + fin_base_version + \"_\" + suffix\n",
      "    return platform_tag\n",
      "\n",
      "[end of src/wheel/macosx_libfile.py]\n",
      "[start of src/wheel/_bdist_wheel.py]\n",
      "\"\"\"\n",
      "Create a wheel (.whl) distribution.\n",
      "\n",
      "A wheel is a built archive format.\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import os\n",
      "import re\n",
      "import shutil\n",
      "import stat\n",
      "import struct\n",
      "import sys\n",
      "import sysconfig\n",
      "import warnings\n",
      "from email.generator import BytesGenerator, Generator\n",
      "from email.policy import EmailPolicy\n",
      "from glob import iglob\n",
      "from shutil import rmtree\n",
      "from typing import TYPE_CHECKING, Callable, Iterable, Literal, Sequence, cast\n",
      "from zipfile import ZIP_DEFLATED, ZIP_STORED\n",
      "\n",
      "import setuptools\n",
      "from setuptools import Command\n",
      "\n",
      "from . import __version__ as wheel_version\n",
      "from .metadata import pkginfo_to_metadata\n",
      "from .util import log\n",
      "from .vendored.packaging import tags\n",
      "from .vendored.packaging import version as _packaging_version\n",
      "from .wheelfile import WheelFile\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    import types\n",
      "\n",
      "# ensure Python logging is configured\n",
      "try:\n",
      "    __import__(\"setuptools.logging\")\n",
      "except ImportError:\n",
      "    # setuptools < ??\n",
      "    from . import _setuptools_logging\n",
      "\n",
      "    _setuptools_logging.configure()\n",
      "\n",
      "\n",
      "def safe_name(name: str) -> str:\n",
      "    \"\"\"Convert an arbitrary string to a standard distribution name\n",
      "    Any runs of non-alphanumeric/. characters are replaced with a single '-'.\n",
      "    \"\"\"\n",
      "    return re.sub(\"[^A-Za-z0-9.]+\", \"-\", name)\n",
      "\n",
      "\n",
      "def safe_version(version: str) -> str:\n",
      "    \"\"\"\n",
      "    Convert an arbitrary string to a standard version string\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # normalize the version\n",
      "        return str(_packaging_version.Version(version))\n",
      "    except _packaging_version.InvalidVersion:\n",
      "        version = version.replace(\" \", \".\")\n",
      "        return re.sub(\"[^A-Za-z0-9.]+\", \"-\", version)\n",
      "\n",
      "\n",
      "setuptools_major_version = int(setuptools.__version__.split(\".\")[0])\n",
      "\n",
      "PY_LIMITED_API_PATTERN = r\"cp3\\d\"\n",
      "\n",
      "\n",
      "def _is_32bit_interpreter() -> bool:\n",
      "    return struct.calcsize(\"P\") == 4\n",
      "\n",
      "\n",
      "def python_tag() -> str:\n",
      "    return f\"py{sys.version_info[0]}\"\n",
      "\n",
      "\n",
      "def get_platform(archive_root: str | None) -> str:\n",
      "    \"\"\"Return our platform name 'win32', 'linux_x86_64'\"\"\"\n",
      "    result = sysconfig.get_platform()\n",
      "    if result.startswith(\"macosx\") and archive_root is not None:\n",
      "        from .macosx_libfile import calculate_macosx_platform_tag\n",
      "\n",
      "        result = calculate_macosx_platform_tag(archive_root, result)\n",
      "    elif _is_32bit_interpreter():\n",
      "        if result == \"linux-x86_64\":\n",
      "            # pip pull request #3497\n",
      "            result = \"linux-i686\"\n",
      "        elif result == \"linux-aarch64\":\n",
      "            # packaging pull request #234\n",
      "            # TODO armv8l, packaging pull request #690 => this did not land\n",
      "            # in pip/packaging yet\n",
      "            result = \"linux-armv7l\"\n",
      "\n",
      "    return result.replace(\"-\", \"_\")\n",
      "\n",
      "\n",
      "def get_flag(\n",
      "    var: str, fallback: bool, expected: bool = True, warn: bool = True\n",
      ") -> bool:\n",
      "    \"\"\"Use a fallback value for determining SOABI flags if the needed config\n",
      "    var is unset or unavailable.\"\"\"\n",
      "    val = sysconfig.get_config_var(var)\n",
      "    if val is None:\n",
      "        if warn:\n",
      "            warnings.warn(\n",
      "                f\"Config variable '{var}' is unset, Python ABI tag may be incorrect\",\n",
      "                RuntimeWarning,\n",
      "                stacklevel=2,\n",
      "            )\n",
      "        return fallback\n",
      "    return val == expected\n",
      "\n",
      "\n",
      "def get_abi_tag() -> str | None:\n",
      "    \"\"\"Return the ABI tag based on SOABI (if available) or emulate SOABI (PyPy2).\"\"\"\n",
      "    soabi: str = sysconfig.get_config_var(\"SOABI\")\n",
      "    impl = tags.interpreter_name()\n",
      "    if not soabi and impl in (\"cp\", \"pp\") and hasattr(sys, \"maxunicode\"):\n",
      "        d = \"\"\n",
      "        m = \"\"\n",
      "        u = \"\"\n",
      "        if get_flag(\"Py_DEBUG\", hasattr(sys, \"gettotalrefcount\"), warn=(impl == \"cp\")):\n",
      "            d = \"d\"\n",
      "\n",
      "        if get_flag(\n",
      "            \"WITH_PYMALLOC\",\n",
      "            impl == \"cp\",\n",
      "            warn=(impl == \"cp\" and sys.version_info < (3, 8)),\n",
      "        ) and sys.version_info < (3, 8):\n",
      "            m = \"m\"\n",
      "\n",
      "        abi = f\"{impl}{tags.interpreter_version()}{d}{m}{u}\"\n",
      "    elif soabi and impl == \"cp\" and soabi.startswith(\"cpython\"):\n",
      "        # non-Windows\n",
      "        abi = \"cp\" + soabi.split(\"-\")[1]\n",
      "    elif soabi and impl == \"cp\" and soabi.startswith(\"cp\"):\n",
      "        # Windows\n",
      "        abi = soabi.split(\"-\")[0]\n",
      "    elif soabi and impl == \"pp\":\n",
      "        # we want something like pypy36-pp73\n",
      "        abi = \"-\".join(soabi.split(\"-\")[:2])\n",
      "        abi = abi.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
      "    elif soabi and impl == \"graalpy\":\n",
      "        abi = \"-\".join(soabi.split(\"-\")[:3])\n",
      "        abi = abi.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
      "    elif soabi:\n",
      "        abi = soabi.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
      "    else:\n",
      "        abi = None\n",
      "\n",
      "    return abi\n",
      "\n",
      "\n",
      "def safer_name(name: str) -> str:\n",
      "    return safe_name(name).replace(\"-\", \"_\")\n",
      "\n",
      "\n",
      "def safer_version(version: str) -> str:\n",
      "    return safe_version(version).replace(\"-\", \"_\")\n",
      "\n",
      "\n",
      "def remove_readonly(\n",
      "    func: Callable[..., object],\n",
      "    path: str,\n",
      "    excinfo: tuple[type[Exception], Exception, types.TracebackType],\n",
      ") -> None:\n",
      "    remove_readonly_exc(func, path, excinfo[1])\n",
      "\n",
      "\n",
      "def remove_readonly_exc(func: Callable[..., object], path: str, exc: Exception) -> None:\n",
      "    os.chmod(path, stat.S_IWRITE)\n",
      "    func(path)\n",
      "\n",
      "\n",
      "class bdist_wheel(Command):\n",
      "    description = \"create a wheel distribution\"\n",
      "\n",
      "    supported_compressions = {\n",
      "        \"stored\": ZIP_STORED,\n",
      "        \"deflated\": ZIP_DEFLATED,\n",
      "    }\n",
      "\n",
      "    user_options = [\n",
      "        (\"bdist-dir=\", \"b\", \"temporary directory for creating the distribution\"),\n",
      "        (\n",
      "            \"plat-name=\",\n",
      "            \"p\",\n",
      "            \"platform name to embed in generated filenames \"\n",
      "            f\"(default: {get_platform(None)})\",\n",
      "        ),\n",
      "        (\n",
      "            \"keep-temp\",\n",
      "            \"k\",\n",
      "            \"keep the pseudo-installation tree around after \"\n",
      "            \"creating the distribution archive\",\n",
      "        ),\n",
      "        (\"dist-dir=\", \"d\", \"directory to put final built distributions in\"),\n",
      "        (\"skip-build\", None, \"skip rebuilding everything (for testing/debugging)\"),\n",
      "        (\n",
      "            \"relative\",\n",
      "            None,\n",
      "            \"build the archive using relative paths (default: false)\",\n",
      "        ),\n",
      "        (\n",
      "            \"owner=\",\n",
      "            \"u\",\n",
      "            \"Owner name used when creating a tar file [default: current user]\",\n",
      "        ),\n",
      "        (\n",
      "            \"group=\",\n",
      "            \"g\",\n",
      "            \"Group name used when creating a tar file [default: current group]\",\n",
      "        ),\n",
      "        (\"universal\", None, \"make a universal wheel (default: false)\"),\n",
      "        (\n",
      "            \"compression=\",\n",
      "            None,\n",
      "            \"zipfile compression (one of: {}) (default: 'deflated')\".format(\n",
      "                \", \".join(supported_compressions)\n",
      "            ),\n",
      "        ),\n",
      "        (\n",
      "            \"python-tag=\",\n",
      "            None,\n",
      "            f\"Python implementation compatibility tag (default: '{python_tag()}')\",\n",
      "        ),\n",
      "        (\n",
      "            \"build-number=\",\n",
      "            None,\n",
      "            \"Build number for this particular version. \"\n",
      "            \"As specified in PEP-0427, this must start with a digit. \"\n",
      "            \"[default: None]\",\n",
      "        ),\n",
      "        (\n",
      "            \"py-limited-api=\",\n",
      "            None,\n",
      "            \"Python tag (cp32|cp33|cpNN) for abi3 wheel tag (default: false)\",\n",
      "        ),\n",
      "    ]\n",
      "\n",
      "    boolean_options = [\"keep-temp\", \"skip-build\", \"relative\", \"universal\"]\n",
      "\n",
      "    def initialize_options(self):\n",
      "        self.bdist_dir: str = None\n",
      "        self.data_dir = None\n",
      "        self.plat_name: str | None = None\n",
      "        self.plat_tag = None\n",
      "        self.format = \"zip\"\n",
      "        self.keep_temp = False\n",
      "        self.dist_dir: str | None = None\n",
      "        self.egginfo_dir = None\n",
      "        self.root_is_pure: bool | None = None\n",
      "        self.skip_build = None\n",
      "        self.relative = False\n",
      "        self.owner = None\n",
      "        self.group = None\n",
      "        self.universal: bool = False\n",
      "        self.compression: str | int = \"deflated\"\n",
      "        self.python_tag: str = python_tag()\n",
      "        self.build_number: str | None = None\n",
      "        self.py_limited_api: str | Literal[False] = False\n",
      "        self.plat_name_supplied = False\n",
      "\n",
      "    def finalize_options(self):\n",
      "        if self.bdist_dir is None:\n",
      "            bdist_base = self.get_finalized_command(\"bdist\").bdist_base\n",
      "            self.bdist_dir = os.path.join(bdist_base, \"wheel\")\n",
      "\n",
      "        egg_info = self.distribution.get_command_obj(\"egg_info\")\n",
      "        egg_info.ensure_finalized()  # needed for correct `wheel_dist_name`\n",
      "\n",
      "        self.data_dir = self.wheel_dist_name + \".data\"\n",
      "        self.plat_name_supplied = self.plat_name is not None\n",
      "\n",
      "        try:\n",
      "            self.compression = self.supported_compressions[self.compression]\n",
      "        except KeyError:\n",
      "            raise ValueError(f\"Unsupported compression: {self.compression}\") from None\n",
      "\n",
      "        need_options = (\"dist_dir\", \"plat_name\", \"skip_build\")\n",
      "\n",
      "        self.set_undefined_options(\"bdist\", *zip(need_options, need_options))\n",
      "\n",
      "        self.root_is_pure = not (\n",
      "            self.distribution.has_ext_modules() or self.distribution.has_c_libraries()\n",
      "        )\n",
      "\n",
      "        if self.py_limited_api and not re.match(\n",
      "            PY_LIMITED_API_PATTERN, self.py_limited_api\n",
      "        ):\n",
      "            raise ValueError(f\"py-limited-api must match '{PY_LIMITED_API_PATTERN}'\")\n",
      "\n",
      "        # Support legacy [wheel] section for setting universal\n",
      "        wheel = self.distribution.get_option_dict(\"wheel\")\n",
      "        if \"universal\" in wheel:\n",
      "            # please don't define this in your global configs\n",
      "            log.warning(\n",
      "                \"The [wheel] section is deprecated. Use [bdist_wheel] instead.\",\n",
      "            )\n",
      "            val = wheel[\"universal\"][1].strip()\n",
      "            if val.lower() in (\"1\", \"true\", \"yes\"):\n",
      "                self.universal = True\n",
      "\n",
      "        if self.build_number is not None and not self.build_number[:1].isdigit():\n",
      "            raise ValueError(\"Build tag (build-number) must start with a digit.\")\n",
      "\n",
      "    @property\n",
      "    def wheel_dist_name(self):\n",
      "        \"\"\"Return distribution full name with - replaced with _\"\"\"\n",
      "        components = (\n",
      "            safer_name(self.distribution.get_name()),\n",
      "            safer_version(self.distribution.get_version()),\n",
      "        )\n",
      "        if self.build_number:\n",
      "            components += (self.build_number,)\n",
      "        return \"-\".join(components)\n",
      "\n",
      "    def get_tag(self) -> tuple[str, str, str]:\n",
      "        # bdist sets self.plat_name if unset, we should only use it for purepy\n",
      "        # wheels if the user supplied it.\n",
      "        if self.plat_name_supplied:\n",
      "            plat_name = cast(str, self.plat_name)\n",
      "        elif self.root_is_pure:\n",
      "            plat_name = \"any\"\n",
      "        else:\n",
      "            # macosx contains system version in platform name so need special handle\n",
      "            if self.plat_name and not self.plat_name.startswith(\"macosx\"):\n",
      "                plat_name = self.plat_name\n",
      "            else:\n",
      "                # on macosx always limit the platform name to comply with any\n",
      "                # c-extension modules in bdist_dir, since the user can specify\n",
      "                # a higher MACOSX_DEPLOYMENT_TARGET via tools like CMake\n",
      "\n",
      "                # on other platforms, and on macosx if there are no c-extension\n",
      "                # modules, use the default platform name.\n",
      "                plat_name = get_platform(self.bdist_dir)\n",
      "\n",
      "            if _is_32bit_interpreter():\n",
      "                if plat_name in (\"linux-x86_64\", \"linux_x86_64\"):\n",
      "                    plat_name = \"linux_i686\"\n",
      "                if plat_name in (\"linux-aarch64\", \"linux_aarch64\"):\n",
      "                    # TODO armv8l, packaging pull request #690 => this did not land\n",
      "                    # in pip/packaging yet\n",
      "                    plat_name = \"linux_armv7l\"\n",
      "\n",
      "        plat_name = (\n",
      "            plat_name.lower().replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\")\n",
      "        )\n",
      "\n",
      "        if self.root_is_pure:\n",
      "            if self.universal:\n",
      "                impl = \"py2.py3\"\n",
      "            else:\n",
      "                impl = self.python_tag\n",
      "            tag = (impl, \"none\", plat_name)\n",
      "        else:\n",
      "            impl_name = tags.interpreter_name()\n",
      "            impl_ver = tags.interpreter_version()\n",
      "            impl = impl_name + impl_ver\n",
      "            # We don't work on CPython 3.1, 3.0.\n",
      "            if self.py_limited_api and (impl_name + impl_ver).startswith(\"cp3\"):\n",
      "                impl = self.py_limited_api\n",
      "                abi_tag = \"abi3\"\n",
      "            else:\n",
      "                abi_tag = str(get_abi_tag()).lower()\n",
      "            tag = (impl, abi_tag, plat_name)\n",
      "            # issue gh-374: allow overriding plat_name\n",
      "            supported_tags = [\n",
      "                (t.interpreter, t.abi, plat_name) for t in tags.sys_tags()\n",
      "            ]\n",
      "            assert (\n",
      "                tag in supported_tags\n",
      "            ), f\"would build wheel with unsupported tag {tag}\"\n",
      "        return tag\n",
      "\n",
      "    def run(self):\n",
      "        build_scripts = self.reinitialize_command(\"build_scripts\")\n",
      "        build_scripts.executable = \"python\"\n",
      "        build_scripts.force = True\n",
      "\n",
      "        build_ext = self.reinitialize_command(\"build_ext\")\n",
      "        build_ext.inplace = False\n",
      "\n",
      "        if not self.skip_build:\n",
      "            self.run_command(\"build\")\n",
      "\n",
      "        install = self.reinitialize_command(\"install\", reinit_subcommands=True)\n",
      "        install.root = self.bdist_dir\n",
      "        install.compile = False\n",
      "        install.skip_build = self.skip_build\n",
      "        install.warn_dir = False\n",
      "\n",
      "        # A wheel without setuptools scripts is more cross-platform.\n",
      "        # Use the (undocumented) `no_ep` option to setuptools'\n",
      "        # install_scripts command to avoid creating entry point scripts.\n",
      "        install_scripts = self.reinitialize_command(\"install_scripts\")\n",
      "        install_scripts.no_ep = True\n",
      "\n",
      "        # Use a custom scheme for the archive, because we have to decide\n",
      "        # at installation time which scheme to use.\n",
      "        for key in (\"headers\", \"scripts\", \"data\", \"purelib\", \"platlib\"):\n",
      "            setattr(install, \"install_\" + key, os.path.join(self.data_dir, key))\n",
      "\n",
      "        basedir_observed = \"\"\n",
      "\n",
      "        if os.name == \"nt\":\n",
      "            # win32 barfs if any of these are ''; could be '.'?\n",
      "            # (distutils.command.install:change_roots bug)\n",
      "            basedir_observed = os.path.normpath(os.path.join(self.data_dir, \"..\"))\n",
      "            self.install_libbase = self.install_lib = basedir_observed\n",
      "\n",
      "        setattr(\n",
      "            install,\n",
      "            \"install_purelib\" if self.root_is_pure else \"install_platlib\",\n",
      "            basedir_observed,\n",
      "        )\n",
      "\n",
      "        log.info(f\"installing to {self.bdist_dir}\")\n",
      "\n",
      "        self.run_command(\"install\")\n",
      "\n",
      "        impl_tag, abi_tag, plat_tag = self.get_tag()\n",
      "        archive_basename = f\"{self.wheel_dist_name}-{impl_tag}-{abi_tag}-{plat_tag}\"\n",
      "        if not self.relative:\n",
      "            archive_root = self.bdist_dir\n",
      "        else:\n",
      "            archive_root = os.path.join(\n",
      "                self.bdist_dir, self._ensure_relative(install.install_base)\n",
      "            )\n",
      "\n",
      "        self.set_undefined_options(\"install_egg_info\", (\"target\", \"egginfo_dir\"))\n",
      "        distinfo_dirname = (\n",
      "            f\"{safer_name(self.distribution.get_name())}-\"\n",
      "            f\"{safer_version(self.distribution.get_version())}.dist-info\"\n",
      "        )\n",
      "        distinfo_dir = os.path.join(self.bdist_dir, distinfo_dirname)\n",
      "        self.egg2dist(self.egginfo_dir, distinfo_dir)\n",
      "\n",
      "        self.write_wheelfile(distinfo_dir)\n",
      "\n",
      "        # Make the archive\n",
      "        if not os.path.exists(self.dist_dir):\n",
      "            os.makedirs(self.dist_dir)\n",
      "\n",
      "        wheel_path = os.path.join(self.dist_dir, archive_basename + \".whl\")\n",
      "        with WheelFile(wheel_path, \"w\", self.compression) as wf:\n",
      "            wf.write_files(archive_root)\n",
      "\n",
      "        # Add to 'Distribution.dist_files' so that the \"upload\" command works\n",
      "        getattr(self.distribution, \"dist_files\", []).append(\n",
      "            (\n",
      "                \"bdist_wheel\",\n",
      "                \"{}.{}\".format(*sys.version_info[:2]),  # like 3.7\n",
      "                wheel_path,\n",
      "            )\n",
      "        )\n",
      "\n",
      "        if not self.keep_temp:\n",
      "            log.info(f\"removing {self.bdist_dir}\")\n",
      "            if not self.dry_run:\n",
      "                if sys.version_info < (3, 12):\n",
      "                    rmtree(self.bdist_dir, onerror=remove_readonly)\n",
      "                else:\n",
      "                    rmtree(self.bdist_dir, onexc=remove_readonly_exc)\n",
      "\n",
      "    def write_wheelfile(\n",
      "        self, wheelfile_base: str, generator: str = f\"bdist_wheel ({wheel_version})\"\n",
      "    ):\n",
      "        from email.message import Message\n",
      "\n",
      "        msg = Message()\n",
      "        msg[\"Wheel-Version\"] = \"1.0\"  # of the spec\n",
      "        msg[\"Generator\"] = generator\n",
      "        msg[\"Root-Is-Purelib\"] = str(self.root_is_pure).lower()\n",
      "        if self.build_number is not None:\n",
      "            msg[\"Build\"] = self.build_number\n",
      "\n",
      "        # Doesn't work for bdist_wininst\n",
      "        impl_tag, abi_tag, plat_tag = self.get_tag()\n",
      "        for impl in impl_tag.split(\".\"):\n",
      "            for abi in abi_tag.split(\".\"):\n",
      "                for plat in plat_tag.split(\".\"):\n",
      "                    msg[\"Tag\"] = \"-\".join((impl, abi, plat))\n",
      "\n",
      "        wheelfile_path = os.path.join(wheelfile_base, \"WHEEL\")\n",
      "        log.info(f\"creating {wheelfile_path}\")\n",
      "        with open(wheelfile_path, \"wb\") as f:\n",
      "            BytesGenerator(f, maxheaderlen=0).flatten(msg)\n",
      "\n",
      "    def _ensure_relative(self, path: str) -> str:\n",
      "        # copied from dir_util, deleted\n",
      "        drive, path = os.path.splitdrive(path)\n",
      "        if path[0:1] == os.sep:\n",
      "            path = drive + path[1:]\n",
      "        return path\n",
      "\n",
      "    @property\n",
      "    def license_paths(self) -> Iterable[str]:\n",
      "        if setuptools_major_version >= 57:\n",
      "            # Setuptools has resolved any patterns to actual file names\n",
      "            return self.distribution.metadata.license_files or ()\n",
      "\n",
      "        files: set[str] = set()\n",
      "        metadata = self.distribution.get_option_dict(\"metadata\")\n",
      "        if setuptools_major_version >= 42:\n",
      "            # Setuptools recognizes the license_files option but does not do globbing\n",
      "            patterns = cast(Sequence[str], self.distribution.metadata.license_files)\n",
      "        else:\n",
      "            # Prior to those, wheel is entirely responsible for handling license files\n",
      "            if \"license_files\" in metadata:\n",
      "                patterns = metadata[\"license_files\"][1].split()\n",
      "            else:\n",
      "                patterns = ()\n",
      "\n",
      "        if \"license_file\" in metadata:\n",
      "            warnings.warn(\n",
      "                'The \"license_file\" option is deprecated. Use \"license_files\" instead.',\n",
      "                DeprecationWarning,\n",
      "                stacklevel=2,\n",
      "            )\n",
      "            files.add(metadata[\"license_file\"][1])\n",
      "\n",
      "        if not files and not patterns and not isinstance(patterns, list):\n",
      "            patterns = (\"LICEN[CS]E*\", \"COPYING*\", \"NOTICE*\", \"AUTHORS*\")\n",
      "\n",
      "        for pattern in patterns:\n",
      "            for path in iglob(pattern):\n",
      "                if path.endswith(\"~\"):\n",
      "                    log.debug(\n",
      "                        f'ignoring license file \"{path}\" as it looks like a backup'\n",
      "                    )\n",
      "                    continue\n",
      "\n",
      "                if path not in files and os.path.isfile(path):\n",
      "                    log.info(\n",
      "                        f'adding license file \"{path}\" (matched pattern \"{pattern}\")'\n",
      "                    )\n",
      "                    files.add(path)\n",
      "\n",
      "        return files\n",
      "\n",
      "    def egg2dist(self, egginfo_path: str, distinfo_path: str):\n",
      "        \"\"\"Convert an .egg-info directory into a .dist-info directory\"\"\"\n",
      "\n",
      "        def adios(p: str) -> None:\n",
      "            \"\"\"Appropriately delete directory, file or link.\"\"\"\n",
      "            if os.path.exists(p) and not os.path.islink(p) and os.path.isdir(p):\n",
      "                shutil.rmtree(p)\n",
      "            elif os.path.exists(p):\n",
      "                os.unlink(p)\n",
      "\n",
      "        adios(distinfo_path)\n",
      "\n",
      "        if not os.path.exists(egginfo_path):\n",
      "            # There is no egg-info. This is probably because the egg-info\n",
      "            # file/directory is not named matching the distribution name used\n",
      "            # to name the archive file. Check for this case and report\n",
      "            # accordingly.\n",
      "            import glob\n",
      "\n",
      "            pat = os.path.join(os.path.dirname(egginfo_path), \"*.egg-info\")\n",
      "            possible = glob.glob(pat)\n",
      "            err = f\"Egg metadata expected at {egginfo_path} but not found\"\n",
      "            if possible:\n",
      "                alt = os.path.basename(possible[0])\n",
      "                err += f\" ({alt} found - possible misnamed archive file?)\"\n",
      "\n",
      "            raise ValueError(err)\n",
      "\n",
      "        if os.path.isfile(egginfo_path):\n",
      "            # .egg-info is a single file\n",
      "            pkg_info = pkginfo_to_metadata(egginfo_path, egginfo_path)\n",
      "            os.mkdir(distinfo_path)\n",
      "        else:\n",
      "            # .egg-info is a directory\n",
      "            pkginfo_path = os.path.join(egginfo_path, \"PKG-INFO\")\n",
      "            pkg_info = pkginfo_to_metadata(egginfo_path, pkginfo_path)\n",
      "\n",
      "            # ignore common egg metadata that is useless to wheel\n",
      "            shutil.copytree(\n",
      "                egginfo_path,\n",
      "                distinfo_path,\n",
      "                ignore=lambda x, y: {\n",
      "                    \"PKG-INFO\",\n",
      "                    \"requires.txt\",\n",
      "                    \"SOURCES.txt\",\n",
      "                    \"not-zip-safe\",\n",
      "                },\n",
      "            )\n",
      "\n",
      "            # delete dependency_links if it is only whitespace\n",
      "            dependency_links_path = os.path.join(distinfo_path, \"dependency_links.txt\")\n",
      "            with open(dependency_links_path, encoding=\"utf-8\") as dependency_links_file:\n",
      "                dependency_links = dependency_links_file.read().strip()\n",
      "            if not dependency_links:\n",
      "                adios(dependency_links_path)\n",
      "\n",
      "        pkg_info_path = os.path.join(distinfo_path, \"METADATA\")\n",
      "        serialization_policy = EmailPolicy(\n",
      "            utf8=True,\n",
      "            mangle_from_=False,\n",
      "            max_line_length=0,\n",
      "        )\n",
      "        with open(pkg_info_path, \"w\", encoding=\"utf-8\") as out:\n",
      "            Generator(out, policy=serialization_policy).flatten(pkg_info)\n",
      "\n",
      "        for license_path in self.license_paths:\n",
      "            filename = os.path.basename(license_path)\n",
      "            shutil.copy(license_path, os.path.join(distinfo_path, filename))\n",
      "\n",
      "        adios(egginfo_path)\n",
      "\n",
      "[end of src/wheel/_bdist_wheel.py]\n",
      "[start of src/wheel/__main__.py]\n",
      "\"\"\"\n",
      "Wheel command line tool (enable python -m wheel syntax)\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import sys\n",
      "\n",
      "\n",
      "def main():  # needed for console script\n",
      "    if __package__ == \"\":\n",
      "        # To be able to run 'python wheel-0.9.whl/wheel':\n",
      "        import os.path\n",
      "\n",
      "        path = os.path.dirname(os.path.dirname(__file__))\n",
      "        sys.path[0:0] = [path]\n",
      "    import wheel.cli\n",
      "\n",
      "    sys.exit(wheel.cli.main())\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    sys.exit(main())\n",
      "\n",
      "[end of src/wheel/__main__.py]\n",
      "[start of src/wheel/cli/convert.py]\n",
      "from __future__ import annotations\n",
      "\n",
      "import os.path\n",
      "import re\n",
      "from abc import ABCMeta, abstractmethod\n",
      "from collections import defaultdict\n",
      "from collections.abc import Iterator\n",
      "from email.message import Message\n",
      "from email.parser import Parser\n",
      "from email.policy import EmailPolicy\n",
      "from glob import iglob\n",
      "from pathlib import Path\n",
      "from textwrap import dedent\n",
      "from zipfile import ZipFile\n",
      "\n",
      "from .. import __version__\n",
      "from ..metadata import generate_requirements\n",
      "from ..vendored.packaging.tags import parse_tag\n",
      "from ..wheelfile import WheelFile\n",
      "\n",
      "egg_filename_re = re.compile(\n",
      "    r\"\"\"\n",
      "    (?P<name>.+?)-(?P<ver>.+?)\n",
      "    (-(?P<pyver>py\\d\\.\\d+)\n",
      "     (-(?P<arch>.+?))?\n",
      "    )?.egg$\"\"\",\n",
      "    re.VERBOSE,\n",
      ")\n",
      "egg_info_re = re.compile(\n",
      "    r\"\"\"\n",
      "    ^(?P<name>.+?)-(?P<ver>.+?)\n",
      "    (-(?P<pyver>py\\d\\.\\d+)\n",
      "    )?.egg-info/\"\"\",\n",
      "    re.VERBOSE,\n",
      ")\n",
      "wininst_re = re.compile(\n",
      "    r\"\\.(?P<platform>win32|win-amd64)(?:-(?P<pyver>py\\d\\.\\d))?\\.exe$\"\n",
      ")\n",
      "pyd_re = re.compile(r\"\\.(?P<abi>[a-z0-9]+)-(?P<platform>win32|win_amd64)\\.pyd$\")\n",
      "serialization_policy = EmailPolicy(\n",
      "    utf8=True,\n",
      "    mangle_from_=False,\n",
      "    max_line_length=0,\n",
      ")\n",
      "GENERATOR = f\"wheel {__version__}\"\n",
      "\n",
      "\n",
      "def convert_requires(requires: str, metadata: Message) -> None:\n",
      "    extra: str | None = None\n",
      "    requirements: dict[str | None, list[str]] = defaultdict(list)\n",
      "    for line in requires.splitlines():\n",
      "        line = line.strip()\n",
      "        if not line:\n",
      "            continue\n",
      "\n",
      "        if line.startswith(\"[\") and line.endswith(\"]\"):\n",
      "            extra = line[1:-1]\n",
      "            continue\n",
      "\n",
      "        requirements[extra].append(line)\n",
      "\n",
      "    for key, value in generate_requirements(requirements):\n",
      "        metadata.add_header(key, value)\n",
      "\n",
      "\n",
      "def convert_pkg_info(pkginfo: str, metadata: Message):\n",
      "    parsed_message = Parser().parsestr(pkginfo)\n",
      "    for key, value in parsed_message.items():\n",
      "        key_lower = key.lower()\n",
      "        if value == \"UNKNOWN\":\n",
      "            continue\n",
      "\n",
      "        if key_lower == \"description\":\n",
      "            description_lines = value.splitlines()\n",
      "            if description_lines:\n",
      "                value = \"\\n\".join(\n",
      "                    (\n",
      "                        description_lines[0].lstrip(),\n",
      "                        dedent(\"\\n\".join(description_lines[1:])),\n",
      "                        \"\\n\",\n",
      "                    )\n",
      "                )\n",
      "            else:\n",
      "                value = \"\\n\"\n",
      "\n",
      "            metadata.set_payload(value)\n",
      "        elif key_lower == \"home-page\":\n",
      "            metadata.add_header(\"Project-URL\", f\"Homepage, {value}\")\n",
      "        elif key_lower == \"download-url\":\n",
      "            metadata.add_header(\"Project-URL\", f\"Download, {value}\")\n",
      "        else:\n",
      "            metadata.add_header(key, value)\n",
      "\n",
      "    metadata.replace_header(\"Metadata-Version\", \"2.4\")\n",
      "\n",
      "\n",
      "def normalize(name: str) -> str:\n",
      "    return re.sub(r\"[-_.]+\", \"-\", name).lower().replace(\"-\", \"_\")\n",
      "\n",
      "\n",
      "class ConvertSource(metaclass=ABCMeta):\n",
      "    name: str\n",
      "    version: str\n",
      "    pyver: str = \"py2.py3\"\n",
      "    abi: str = \"none\"\n",
      "    platform: str = \"any\"\n",
      "    metadata: Message\n",
      "\n",
      "    @property\n",
      "    def dist_info_dir(self) -> str:\n",
      "        return f\"{self.name}-{self.version}.dist-info\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def generate_contents(self) -> Iterator[tuple[str, bytes]]:\n",
      "        pass\n",
      "\n",
      "\n",
      "class EggFileSource(ConvertSource):\n",
      "    def __init__(self, path: Path):\n",
      "        if not (match := egg_filename_re.match(path.name)):\n",
      "            raise ValueError(f\"Invalid egg file name: {path.name}\")\n",
      "\n",
      "        # Binary wheels are assumed to be for CPython\n",
      "        self.path = path\n",
      "        self.name = normalize(match.group(\"name\"))\n",
      "        self.version = match.group(\"ver\")\n",
      "        if pyver := match.group(\"pyver\"):\n",
      "            self.pyver = pyver.replace(\".\", \"\")\n",
      "            if arch := match.group(\"arch\"):\n",
      "                self.abi = self.pyver.replace(\"py\", \"cp\")\n",
      "                self.platform = normalize(arch)\n",
      "\n",
      "        self.metadata = Message()\n",
      "\n",
      "    def generate_contents(self) -> Iterator[tuple[str, bytes]]:\n",
      "        with ZipFile(self.path, \"r\") as zip_file:\n",
      "            for filename in sorted(zip_file.namelist()):\n",
      "                # Skip pure directory entries\n",
      "                if filename.endswith(\"/\"):\n",
      "                    continue\n",
      "\n",
      "                # Handle files in the egg-info directory specially, selectively moving\n",
      "                # them to the dist-info directory while converting as needed\n",
      "                if filename.startswith(\"EGG-INFO/\"):\n",
      "                    if filename == \"EGG-INFO/requires.txt\":\n",
      "                        requires = zip_file.read(filename).decode(\"utf-8\")\n",
      "                        convert_requires(requires, self.metadata)\n",
      "                    elif filename == \"EGG-INFO/PKG-INFO\":\n",
      "                        pkginfo = zip_file.read(filename).decode(\"utf-8\")\n",
      "                        convert_pkg_info(pkginfo, self.metadata)\n",
      "                    elif filename == \"EGG-INFO/entry_points.txt\":\n",
      "                        yield (\n",
      "                            f\"{self.dist_info_dir}/entry_points.txt\",\n",
      "                            zip_file.read(filename),\n",
      "                        )\n",
      "\n",
      "                    continue\n",
      "\n",
      "                # For any other file, just pass it through\n",
      "                yield filename, zip_file.read(filename)\n",
      "\n",
      "\n",
      "class EggDirectorySource(EggFileSource):\n",
      "    def generate_contents(self) -> Iterator[tuple[str, bytes]]:\n",
      "        for dirpath, _, filenames in os.walk(self.path):\n",
      "            for filename in sorted(filenames):\n",
      "                path = Path(dirpath, filename)\n",
      "                if path.parent.name == \"EGG-INFO\":\n",
      "                    if path.name == \"requires.txt\":\n",
      "                        requires = path.read_text(\"utf-8\")\n",
      "                        convert_requires(requires, self.metadata)\n",
      "                    elif path.name == \"PKG-INFO\":\n",
      "                        pkginfo = path.read_text(\"utf-8\")\n",
      "                        convert_pkg_info(pkginfo, self.metadata)\n",
      "                        if name := self.metadata.get(\"Name\"):\n",
      "                            self.name = normalize(name)\n",
      "\n",
      "                        if version := self.metadata.get(\"Version\"):\n",
      "                            self.version = version\n",
      "                    elif path.name == \"entry_points.txt\":\n",
      "                        yield (\n",
      "                            f\"{self.dist_info_dir}/entry_points.txt\",\n",
      "                            path.read_bytes(),\n",
      "                        )\n",
      "\n",
      "                    continue\n",
      "\n",
      "                # For any other file, just pass it through\n",
      "                yield str(path.relative_to(self.path)), path.read_bytes()\n",
      "\n",
      "\n",
      "class WininstFileSource(ConvertSource):\n",
      "    \"\"\"\n",
      "    Handles distributions created with ``bdist_wininst``.\n",
      "\n",
      "    The egginfo filename has the format::\n",
      "\n",
      "        name-ver(-pyver)(-arch).egg-info\n",
      "\n",
      "    The installer filename has the format::\n",
      "\n",
      "        name-ver.arch(-pyver).exe\n",
      "\n",
      "    Some things to note:\n",
      "\n",
      "    1. The installer filename is not definitive. An installer can be renamed\n",
      "       and work perfectly well as an installer. So more reliable data should\n",
      "       be used whenever possible.\n",
      "    2. The egg-info data should be preferred for the name and version, because\n",
      "       these come straight from the distutils metadata, and are mandatory.\n",
      "    3. The pyver from the egg-info data should be ignored, as it is\n",
      "       constructed from the version of Python used to build the installer,\n",
      "       which is irrelevant - the installer filename is correct here (even to\n",
      "       the point that when it's not there, any version is implied).\n",
      "    4. The architecture must be taken from the installer filename, as it is\n",
      "       not included in the egg-info data.\n",
      "    5. Architecture-neutral installers still have an architecture because the\n",
      "       installer format itself (being executable) is architecture-specific. We\n",
      "       should therefore ignore the architecture if the content is pure-python.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, path: Path):\n",
      "        self.path = path\n",
      "        self.metadata = Message()\n",
      "\n",
      "        # Determine the initial architecture and Python version from the file name\n",
      "        # (if possible)\n",
      "        if match := wininst_re.search(path.name):\n",
      "            self.platform = normalize(match.group(\"platform\"))\n",
      "            if pyver := match.group(\"pyver\"):\n",
      "                self.pyver = pyver.replace(\".\", \"\")\n",
      "\n",
      "        # Look for an .egg-info directory and any .pyd files for more precise info\n",
      "        egg_info_found = pyd_found = False\n",
      "        with ZipFile(self.path) as zip_file:\n",
      "            for filename in zip_file.namelist():\n",
      "                prefix, filename = filename.split(\"/\", 1)\n",
      "                if not egg_info_found and (match := egg_info_re.match(filename)):\n",
      "                    egg_info_found = True\n",
      "                    self.name = normalize(match.group(\"name\"))\n",
      "                    self.version = match.group(\"ver\")\n",
      "                    if pyver := match.group(\"pyver\"):\n",
      "                        self.pyver = pyver.replace(\".\", \"\")\n",
      "                elif not pyd_found and (match := pyd_re.search(filename)):\n",
      "                    pyd_found = True\n",
      "                    self.abi = match.group(\"abi\")\n",
      "                    self.platform = match.group(\"platform\")\n",
      "\n",
      "                if egg_info_found and pyd_found:\n",
      "                    break\n",
      "\n",
      "    def generate_contents(self) -> Iterator[tuple[str, bytes]]:\n",
      "        dist_info_dir = f\"{self.name}-{self.version}.dist-info\"\n",
      "        data_dir = f\"{self.name}-{self.version}.data\"\n",
      "        with ZipFile(self.path, \"r\") as zip_file:\n",
      "            for filename in sorted(zip_file.namelist()):\n",
      "                # Skip pure directory entries\n",
      "                if filename.endswith(\"/\"):\n",
      "                    continue\n",
      "\n",
      "                # Handle files in the egg-info directory specially, selectively moving\n",
      "                # them to the dist-info directory while converting as needed\n",
      "                prefix, target_filename = filename.split(\"/\", 1)\n",
      "                if egg_info_re.search(target_filename):\n",
      "                    basename = target_filename.rsplit(\"/\", 1)[-1]\n",
      "                    if basename == \"requires.txt\":\n",
      "                        requires = zip_file.read(filename).decode(\"utf-8\")\n",
      "                        convert_requires(requires, self.metadata)\n",
      "                    elif basename == \"PKG-INFO\":\n",
      "                        pkginfo = zip_file.read(filename).decode(\"utf-8\")\n",
      "                        convert_pkg_info(pkginfo, self.metadata)\n",
      "                    elif basename == \"entry_points.txt\":\n",
      "                        yield (\n",
      "                            f\"{dist_info_dir}/entry_points.txt\",\n",
      "                            zip_file.read(filename),\n",
      "                        )\n",
      "\n",
      "                    continue\n",
      "                elif prefix == \"SCRIPTS\":\n",
      "                    target_filename = f\"{data_dir}/scripts/{target_filename}\"\n",
      "\n",
      "                # For any other file, just pass it through\n",
      "                yield target_filename, zip_file.read(filename)\n",
      "\n",
      "\n",
      "def convert(files: list[str], dest_dir: str, verbose: bool) -> None:\n",
      "    for pat in files:\n",
      "        for archive in iglob(pat):\n",
      "            path = Path(archive)\n",
      "            if path.suffix == \".egg\":\n",
      "                if path.is_dir():\n",
      "                    source: ConvertSource = EggDirectorySource(path)\n",
      "                else:\n",
      "                    source = EggFileSource(path)\n",
      "            else:\n",
      "                source = WininstFileSource(path)\n",
      "\n",
      "            if verbose:\n",
      "                print(f\"{archive}...\", flush=True, end=\"\")\n",
      "\n",
      "            dest_path = Path(dest_dir) / (\n",
      "                f\"{source.name}-{source.version}-{source.pyver}-{source.abi}\"\n",
      "                f\"-{source.platform}.whl\"\n",
      "            )\n",
      "            with WheelFile(dest_path, \"w\") as wheelfile:\n",
      "                for name_or_zinfo, contents in source.generate_contents():\n",
      "                    wheelfile.writestr(name_or_zinfo, contents)\n",
      "\n",
      "                # Write the METADATA file\n",
      "                wheelfile.writestr(\n",
      "                    f\"{source.dist_info_dir}/METADATA\",\n",
      "                    source.metadata.as_string(policy=serialization_policy).encode(\n",
      "                        \"utf-8\"\n",
      "                    ),\n",
      "                )\n",
      "\n",
      "                # Write the WHEEL file\n",
      "                wheel_message = Message()\n",
      "                wheel_message.add_header(\"Wheel-Version\", \"1.0\")\n",
      "                wheel_message.add_header(\"Generator\", GENERATOR)\n",
      "                wheel_message.add_header(\n",
      "                    \"Root-Is-Purelib\", str(source.platform == \"any\").lower()\n",
      "                )\n",
      "                tags = parse_tag(f\"{source.pyver}-{source.abi}-{source.platform}\")\n",
      "                for tag in sorted(tags, key=lambda tag: tag.interpreter):\n",
      "                    wheel_message.add_header(\"Tag\", str(tag))\n",
      "\n",
      "                wheelfile.writestr(\n",
      "                    f\"{source.dist_info_dir}/WHEEL\",\n",
      "                    wheel_message.as_string(policy=serialization_policy).encode(\n",
      "                        \"utf-8\"\n",
      "                    ),\n",
      "                )\n",
      "\n",
      "            if verbose:\n",
      "                print(\"OK\")\n",
      "\n",
      "[end of src/wheel/cli/convert.py]\n",
      "[start of src/wheel/cli/pack.py]\n",
      "from __future__ import annotations\n",
      "\n",
      "import email.policy\n",
      "import os.path\n",
      "import re\n",
      "from email.generator import BytesGenerator\n",
      "from email.parser import BytesParser\n",
      "\n",
      "from wheel.cli import WheelError\n",
      "from wheel.wheelfile import WheelFile\n",
      "\n",
      "DIST_INFO_RE = re.compile(r\"^(?P<namever>(?P<name>.+?)-(?P<ver>\\d.*?))\\.dist-info$\")\n",
      "\n",
      "\n",
      "def pack(directory: str, dest_dir: str, build_number: str | None) -> None:\n",
      "    \"\"\"Repack a previously unpacked wheel directory into a new wheel file.\n",
      "\n",
      "    The .dist-info/WHEEL file must contain one or more tags so that the target\n",
      "    wheel file name can be determined.\n",
      "\n",
      "    :param directory: The unpacked wheel directory\n",
      "    :param dest_dir: Destination directory (defaults to the current directory)\n",
      "    \"\"\"\n",
      "    # Find the .dist-info directory\n",
      "    dist_info_dirs = [\n",
      "        fn\n",
      "        for fn in os.listdir(directory)\n",
      "        if os.path.isdir(os.path.join(directory, fn)) and DIST_INFO_RE.match(fn)\n",
      "    ]\n",
      "    if len(dist_info_dirs) > 1:\n",
      "        raise WheelError(f\"Multiple .dist-info directories found in {directory}\")\n",
      "    elif not dist_info_dirs:\n",
      "        raise WheelError(f\"No .dist-info directories found in {directory}\")\n",
      "\n",
      "    # Determine the target wheel filename\n",
      "    dist_info_dir = dist_info_dirs[0]\n",
      "    name_version = DIST_INFO_RE.match(dist_info_dir).group(\"namever\")\n",
      "\n",
      "    # Read the tags and the existing build number from .dist-info/WHEEL\n",
      "    wheel_file_path = os.path.join(directory, dist_info_dir, \"WHEEL\")\n",
      "    with open(wheel_file_path, \"rb\") as f:\n",
      "        info = BytesParser(policy=email.policy.compat32).parse(f)\n",
      "        tags: list[str] = info.get_all(\"Tag\", [])\n",
      "        existing_build_number = info.get(\"Build\")\n",
      "\n",
      "        if not tags:\n",
      "            raise WheelError(\n",
      "                f\"No tags present in {dist_info_dir}/WHEEL; cannot determine target \"\n",
      "                f\"wheel filename\"\n",
      "            )\n",
      "\n",
      "    # Set the wheel file name and add/replace/remove the Build tag in .dist-info/WHEEL\n",
      "    build_number = build_number if build_number is not None else existing_build_number\n",
      "    if build_number is not None:\n",
      "        del info[\"Build\"]\n",
      "        if build_number:\n",
      "            info[\"Build\"] = build_number\n",
      "            name_version += \"-\" + build_number\n",
      "\n",
      "        if build_number != existing_build_number:\n",
      "            with open(wheel_file_path, \"wb\") as f:\n",
      "                BytesGenerator(f, maxheaderlen=0).flatten(info)\n",
      "\n",
      "    # Reassemble the tags for the wheel file\n",
      "    tagline = compute_tagline(tags)\n",
      "\n",
      "    # Repack the wheel\n",
      "    wheel_path = os.path.join(dest_dir, f\"{name_version}-{tagline}.whl\")\n",
      "    with WheelFile(wheel_path, \"w\") as wf:\n",
      "        print(f\"Repacking wheel as {wheel_path}...\", end=\"\", flush=True)\n",
      "        wf.write_files(directory)\n",
      "\n",
      "    print(\"OK\")\n",
      "\n",
      "\n",
      "def compute_tagline(tags: list[str]) -> str:\n",
      "    \"\"\"Compute a tagline from a list of tags.\n",
      "\n",
      "    :param tags: A list of tags\n",
      "    :return: A tagline\n",
      "    \"\"\"\n",
      "    impls = sorted({tag.split(\"-\")[0] for tag in tags})\n",
      "    abivers = sorted({tag.split(\"-\")[1] for tag in tags})\n",
      "    platforms = sorted({tag.split(\"-\")[2] for tag in tags})\n",
      "    return \"-\".join([\".\".join(impls), \".\".join(abivers), \".\".join(platforms)])\n",
      "\n",
      "[end of src/wheel/cli/pack.py]\n",
      "[start of src/wheel/cli/__init__.py]\n",
      "\"\"\"\n",
      "Wheel command-line utility.\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import sys\n",
      "from argparse import ArgumentTypeError\n",
      "\n",
      "\n",
      "class WheelError(Exception):\n",
      "    pass\n",
      "\n",
      "\n",
      "def unpack_f(args: argparse.Namespace) -> None:\n",
      "    from .unpack import unpack\n",
      "\n",
      "    unpack(args.wheelfile, args.dest)\n",
      "\n",
      "\n",
      "def pack_f(args: argparse.Namespace) -> None:\n",
      "    from .pack import pack\n",
      "\n",
      "    pack(args.directory, args.dest_dir, args.build_number)\n",
      "\n",
      "\n",
      "def convert_f(args: argparse.Namespace) -> None:\n",
      "    from .convert import convert\n",
      "\n",
      "    convert(args.files, args.dest_dir, args.verbose)\n",
      "\n",
      "\n",
      "def tags_f(args: argparse.Namespace) -> None:\n",
      "    from .tags import tags\n",
      "\n",
      "    names = (\n",
      "        tags(\n",
      "            wheel,\n",
      "            args.python_tag,\n",
      "            args.abi_tag,\n",
      "            args.platform_tag,\n",
      "            args.build,\n",
      "            args.remove,\n",
      "        )\n",
      "        for wheel in args.wheel\n",
      "    )\n",
      "\n",
      "    for name in names:\n",
      "        print(name)\n",
      "\n",
      "\n",
      "def version_f(args: argparse.Namespace) -> None:\n",
      "    from .. import __version__\n",
      "\n",
      "    print(f\"wheel {__version__}\")\n",
      "\n",
      "\n",
      "def parse_build_tag(build_tag: str) -> str:\n",
      "    if build_tag and not build_tag[0].isdigit():\n",
      "        raise ArgumentTypeError(\"build tag must begin with a digit\")\n",
      "    elif \"-\" in build_tag:\n",
      "        raise ArgumentTypeError(\"invalid character ('-') in build tag\")\n",
      "\n",
      "    return build_tag\n",
      "\n",
      "\n",
      "TAGS_HELP = \"\"\"\\\n",
      "Make a new wheel with given tags. Any tags unspecified will remain the same.\n",
      "Starting the tags with a \"+\" will append to the existing tags. Starting with a\n",
      "\"-\" will remove a tag (use --option=-TAG syntax). Multiple tags can be\n",
      "separated by \".\". The original file will remain unless --remove is given.  The\n",
      "output filename(s) will be displayed on stdout for further processing.\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def parser():\n",
      "    p = argparse.ArgumentParser()\n",
      "    s = p.add_subparsers(help=\"commands\")\n",
      "\n",
      "    unpack_parser = s.add_parser(\"unpack\", help=\"Unpack wheel\")\n",
      "    unpack_parser.add_argument(\n",
      "        \"--dest\", \"-d\", help=\"Destination directory\", default=\".\"\n",
      "    )\n",
      "    unpack_parser.add_argument(\"wheelfile\", help=\"Wheel file\")\n",
      "    unpack_parser.set_defaults(func=unpack_f)\n",
      "\n",
      "    repack_parser = s.add_parser(\"pack\", help=\"Repack wheel\")\n",
      "    repack_parser.add_argument(\"directory\", help=\"Root directory of the unpacked wheel\")\n",
      "    repack_parser.add_argument(\n",
      "        \"--dest-dir\",\n",
      "        \"-d\",\n",
      "        default=os.path.curdir,\n",
      "        help=\"Directory to store the wheel (default %(default)s)\",\n",
      "    )\n",
      "    repack_parser.add_argument(\n",
      "        \"--build-number\", help=\"Build tag to use in the wheel name\"\n",
      "    )\n",
      "    repack_parser.set_defaults(func=pack_f)\n",
      "\n",
      "    convert_parser = s.add_parser(\"convert\", help=\"Convert egg or wininst to wheel\")\n",
      "    convert_parser.add_argument(\"files\", nargs=\"*\", help=\"Files to convert\")\n",
      "    convert_parser.add_argument(\n",
      "        \"--dest-dir\",\n",
      "        \"-d\",\n",
      "        default=os.path.curdir,\n",
      "        help=\"Directory to store wheels (default %(default)s)\",\n",
      "    )\n",
      "    convert_parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\")\n",
      "    convert_parser.set_defaults(func=convert_f)\n",
      "\n",
      "    tags_parser = s.add_parser(\n",
      "        \"tags\", help=\"Add or replace the tags on a wheel\", description=TAGS_HELP\n",
      "    )\n",
      "    tags_parser.add_argument(\"wheel\", nargs=\"*\", help=\"Existing wheel(s) to retag\")\n",
      "    tags_parser.add_argument(\n",
      "        \"--remove\",\n",
      "        action=\"store_true\",\n",
      "        help=\"Remove the original files, keeping only the renamed ones\",\n",
      "    )\n",
      "    tags_parser.add_argument(\n",
      "        \"--python-tag\", metavar=\"TAG\", help=\"Specify an interpreter tag(s)\"\n",
      "    )\n",
      "    tags_parser.add_argument(\"--abi-tag\", metavar=\"TAG\", help=\"Specify an ABI tag(s)\")\n",
      "    tags_parser.add_argument(\n",
      "        \"--platform-tag\", metavar=\"TAG\", help=\"Specify a platform tag(s)\"\n",
      "    )\n",
      "    tags_parser.add_argument(\n",
      "        \"--build\", type=parse_build_tag, metavar=\"BUILD\", help=\"Specify a build tag\"\n",
      "    )\n",
      "    tags_parser.set_defaults(func=tags_f)\n",
      "\n",
      "    version_parser = s.add_parser(\"version\", help=\"Print version and exit\")\n",
      "    version_parser.set_defaults(func=version_f)\n",
      "\n",
      "    help_parser = s.add_parser(\"help\", help=\"Show this help\")\n",
      "    help_parser.set_defaults(func=lambda args: p.print_help())\n",
      "\n",
      "    return p\n",
      "\n",
      "\n",
      "def main():\n",
      "    p = parser()\n",
      "    args = p.parse_args()\n",
      "    if not hasattr(args, \"func\"):\n",
      "        p.print_help()\n",
      "    else:\n",
      "        try:\n",
      "            args.func(args)\n",
      "            return 0\n",
      "        except WheelError as e:\n",
      "            print(e, file=sys.stderr)\n",
      "\n",
      "    return 1\n",
      "\n",
      "[end of src/wheel/cli/__init__.py]\n",
      "[start of src/wheel/cli/tags.py]\n",
      "from __future__ import annotations\n",
      "\n",
      "import email.policy\n",
      "import itertools\n",
      "import os\n",
      "from collections.abc import Iterable\n",
      "from email.parser import BytesParser\n",
      "\n",
      "from ..wheelfile import WheelFile\n",
      "\n",
      "\n",
      "def _compute_tags(original_tags: Iterable[str], new_tags: str | None) -> set[str]:\n",
      "    \"\"\"Add or replace tags. Supports dot-separated tags\"\"\"\n",
      "    if new_tags is None:\n",
      "        return set(original_tags)\n",
      "\n",
      "    if new_tags.startswith(\"+\"):\n",
      "        return {*original_tags, *new_tags[1:].split(\".\")}\n",
      "\n",
      "    if new_tags.startswith(\"-\"):\n",
      "        return set(original_tags) - set(new_tags[1:].split(\".\"))\n",
      "\n",
      "    return set(new_tags.split(\".\"))\n",
      "\n",
      "\n",
      "def tags(\n",
      "    wheel: str,\n",
      "    python_tags: str | None = None,\n",
      "    abi_tags: str | None = None,\n",
      "    platform_tags: str | None = None,\n",
      "    build_tag: str | None = None,\n",
      "    remove: bool = False,\n",
      ") -> str:\n",
      "    \"\"\"Change the tags on a wheel file.\n",
      "\n",
      "    The tags are left unchanged if they are not specified. To specify \"none\",\n",
      "    use [\"none\"]. To append to the previous tags, a tag should start with a\n",
      "    \"+\".  If a tag starts with \"-\", it will be removed from existing tags.\n",
      "    Processing is done left to right.\n",
      "\n",
      "    :param wheel: The paths to the wheels\n",
      "    :param python_tags: The Python tags to set\n",
      "    :param abi_tags: The ABI tags to set\n",
      "    :param platform_tags: The platform tags to set\n",
      "    :param build_tag: The build tag to set\n",
      "    :param remove: Remove the original wheel\n",
      "    \"\"\"\n",
      "    with WheelFile(wheel, \"r\") as f:\n",
      "        assert f.filename, f\"{f.filename} must be available\"\n",
      "\n",
      "        wheel_info = f.read(f.dist_info_path + \"/WHEEL\")\n",
      "        info = BytesParser(policy=email.policy.compat32).parsebytes(wheel_info)\n",
      "\n",
      "        original_wheel_name = os.path.basename(f.filename)\n",
      "        namever = f.parsed_filename.group(\"namever\")\n",
      "        build = f.parsed_filename.group(\"build\")\n",
      "        original_python_tags = f.parsed_filename.group(\"pyver\").split(\".\")\n",
      "        original_abi_tags = f.parsed_filename.group(\"abi\").split(\".\")\n",
      "        original_plat_tags = f.parsed_filename.group(\"plat\").split(\".\")\n",
      "\n",
      "    tags: list[str] = info.get_all(\"Tag\", [])\n",
      "    existing_build_tag = info.get(\"Build\")\n",
      "\n",
      "    impls = {tag.split(\"-\")[0] for tag in tags}\n",
      "    abivers = {tag.split(\"-\")[1] for tag in tags}\n",
      "    platforms = {tag.split(\"-\")[2] for tag in tags}\n",
      "\n",
      "    if impls != set(original_python_tags):\n",
      "        msg = f\"Wheel internal tags {impls!r} != filename tags {original_python_tags!r}\"\n",
      "        raise AssertionError(msg)\n",
      "\n",
      "    if abivers != set(original_abi_tags):\n",
      "        msg = f\"Wheel internal tags {abivers!r} != filename tags {original_abi_tags!r}\"\n",
      "        raise AssertionError(msg)\n",
      "\n",
      "    if platforms != set(original_plat_tags):\n",
      "        msg = (\n",
      "            f\"Wheel internal tags {platforms!r} != filename tags {original_plat_tags!r}\"\n",
      "        )\n",
      "        raise AssertionError(msg)\n",
      "\n",
      "    if existing_build_tag != build:\n",
      "        msg = (\n",
      "            f\"Incorrect filename '{build}' \"\n",
      "            f\"& *.dist-info/WHEEL '{existing_build_tag}' build numbers\"\n",
      "        )\n",
      "        raise AssertionError(msg)\n",
      "\n",
      "    # Start changing as needed\n",
      "    if build_tag is not None:\n",
      "        build = build_tag\n",
      "\n",
      "    final_python_tags = sorted(_compute_tags(original_python_tags, python_tags))\n",
      "    final_abi_tags = sorted(_compute_tags(original_abi_tags, abi_tags))\n",
      "    final_plat_tags = sorted(_compute_tags(original_plat_tags, platform_tags))\n",
      "\n",
      "    final_tags = [\n",
      "        namever,\n",
      "        \".\".join(final_python_tags),\n",
      "        \".\".join(final_abi_tags),\n",
      "        \".\".join(final_plat_tags),\n",
      "    ]\n",
      "    if build:\n",
      "        final_tags.insert(1, build)\n",
      "\n",
      "    final_wheel_name = \"-\".join(final_tags) + \".whl\"\n",
      "\n",
      "    if original_wheel_name != final_wheel_name:\n",
      "        del info[\"Tag\"], info[\"Build\"]\n",
      "        for a, b, c in itertools.product(\n",
      "            final_python_tags, final_abi_tags, final_plat_tags\n",
      "        ):\n",
      "            info[\"Tag\"] = f\"{a}-{b}-{c}\"\n",
      "        if build:\n",
      "            info[\"Build\"] = build\n",
      "\n",
      "        original_wheel_path = os.path.join(\n",
      "            os.path.dirname(f.filename), original_wheel_name\n",
      "        )\n",
      "        final_wheel_path = os.path.join(os.path.dirname(f.filename), final_wheel_name)\n",
      "\n",
      "        with WheelFile(original_wheel_path, \"r\") as fin, WheelFile(\n",
      "            final_wheel_path, \"w\"\n",
      "        ) as fout:\n",
      "            fout.comment = fin.comment  # preserve the comment\n",
      "            for item in fin.infolist():\n",
      "                if item.is_dir():\n",
      "                    continue\n",
      "                if item.filename == f.dist_info_path + \"/RECORD\":\n",
      "                    continue\n",
      "                if item.filename == f.dist_info_path + \"/WHEEL\":\n",
      "                    fout.writestr(item, info.as_bytes())\n",
      "                else:\n",
      "                    fout.writestr(item, fin.read(item))\n",
      "\n",
      "        if remove:\n",
      "            os.remove(original_wheel_path)\n",
      "\n",
      "    return final_wheel_name\n",
      "\n",
      "[end of src/wheel/cli/tags.py]\n",
      "[start of src/wheel/cli/unpack.py]\n",
      "from __future__ import annotations\n",
      "\n",
      "from pathlib import Path\n",
      "\n",
      "from ..wheelfile import WheelFile\n",
      "\n",
      "\n",
      "def unpack(path: str, dest: str = \".\") -> None:\n",
      "    \"\"\"Unpack a wheel.\n",
      "\n",
      "    Wheel content will be unpacked to {dest}/{name}-{ver}, where {name}\n",
      "    is the package name and {ver} its version.\n",
      "\n",
      "    :param path: The path to the wheel.\n",
      "    :param dest: Destination directory (default to current directory).\n",
      "    \"\"\"\n",
      "    with WheelFile(path) as wf:\n",
      "        namever = wf.parsed_filename.group(\"namever\")\n",
      "        destination = Path(dest) / namever\n",
      "        print(f\"Unpacking to: {destination}...\", end=\"\", flush=True)\n",
      "        for zinfo in wf.filelist:\n",
      "            wf.extract(zinfo, destination)\n",
      "\n",
      "            # Set permissions to the same values as they were set in the archive\n",
      "            # We have to do this manually due to\n",
      "            # https://github.com/python/cpython/issues/59999\n",
      "            permissions = zinfo.external_attr >> 16 & 0o777\n",
      "            destination.joinpath(zinfo.filename).chmod(permissions)\n",
      "\n",
      "    print(\"OK\")\n",
      "\n",
      "[end of src/wheel/cli/unpack.py]\n",
      "[start of src/wheel/vendored/__init__.py]\n",
      "\n",
      "[end of src/wheel/vendored/__init__.py]\n",
      "[start of src/wheel/vendored/vendor.txt]\n",
      "packaging==24.0\n",
      "\n",
      "[end of src/wheel/vendored/vendor.txt]\n",
      "[start of src/wheel/vendored/packaging/_structures.py]\n",
      "# This file is dual licensed under the terms of the Apache License, Version\n",
      "# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n",
      "# for complete details.\n",
      "\n",
      "\n",
      "class InfinityType:\n",
      "    def __repr__(self) -> str:\n",
      "        return \"Infinity\"\n",
      "\n",
      "    def __hash__(self) -> int:\n",
      "        return hash(repr(self))\n",
      "\n",
      "    def __lt__(self, other: object) -> bool:\n",
      "        return False\n",
      "\n",
      "    def __le__(self, other: object) -> bool:\n",
      "        return False\n",
      "\n",
      "    def __eq__(self, other: object) -> bool:\n",
      "        return isinstance(other, self.__class__)\n",
      "\n",
      "    def __gt__(self, other: object) -> bool:\n",
      "        return True\n",
      "\n",
      "    def __ge__(self, other: object) -> bool:\n",
      "        return True\n",
      "\n",
      "    def __neg__(self: object) -> \"NegativeInfinityType\":\n",
      "        return NegativeInfinity\n",
      "\n",
      "\n",
      "Infinity = InfinityType()\n",
      "\n",
      "\n",
      "class NegativeInfinityType:\n",
      "    def __repr__(self) -> str:\n",
      "        return \"-Infinity\"\n",
      "\n",
      "    def __hash__(self) -> int:\n",
      "        return hash(repr(self))\n",
      "\n",
      "    def __lt__(self, other: object) -> bool:\n",
      "        return True\n",
      "\n",
      "    def __le__(self, other: object) -> bool:\n",
      "        return True\n",
      "\n",
      "    def __eq__(self, other: object) -> bool:\n",
      "        return isinstance(other, self.__class__)\n",
      "\n",
      "    def __gt__(self, other: object) -> bool:\n",
      "        return False\n",
      "\n",
      "    def __ge__(self, other: object) -> bool:\n",
      "        return False\n",
      "\n",
      "    def __neg__(self: object) -> InfinityType:\n",
      "        return Infinity\n",
      "\n",
      "\n",
      "NegativeInfinity = NegativeInfinityType()\n",
      "\n",
      "[end of src/wheel/vendored/packaging/_structures.py]\n",
      "[start of src/wheel/vendored/packaging/version.py]\n",
      "# This file is dual licensed under the terms of the Apache License, Version\n",
      "# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n",
      "# for complete details.\n",
      "\"\"\"\n",
      ".. testsetup::\n",
      "\n",
      "    from packaging.version import parse, Version\n",
      "\"\"\"\n",
      "\n",
      "import itertools\n",
      "import re\n",
      "from typing import Any, Callable, NamedTuple, Optional, SupportsInt, Tuple, Union\n",
      "\n",
      "from ._structures import Infinity, InfinityType, NegativeInfinity, NegativeInfinityType\n",
      "\n",
      "__all__ = [\"VERSION_PATTERN\", \"parse\", \"Version\", \"InvalidVersion\"]\n",
      "\n",
      "LocalType = Tuple[Union[int, str], ...]\n",
      "\n",
      "CmpPrePostDevType = Union[InfinityType, NegativeInfinityType, Tuple[str, int]]\n",
      "CmpLocalType = Union[\n",
      "    NegativeInfinityType,\n",
      "    Tuple[Union[Tuple[int, str], Tuple[NegativeInfinityType, Union[int, str]]], ...],\n",
      "]\n",
      "CmpKey = Tuple[\n",
      "    int,\n",
      "    Tuple[int, ...],\n",
      "    CmpPrePostDevType,\n",
      "    CmpPrePostDevType,\n",
      "    CmpPrePostDevType,\n",
      "    CmpLocalType,\n",
      "]\n",
      "VersionComparisonMethod = Callable[[CmpKey, CmpKey], bool]\n",
      "\n",
      "\n",
      "class _Version(NamedTuple):\n",
      "    epoch: int\n",
      "    release: Tuple[int, ...]\n",
      "    dev: Optional[Tuple[str, int]]\n",
      "    pre: Optional[Tuple[str, int]]\n",
      "    post: Optional[Tuple[str, int]]\n",
      "    local: Optional[LocalType]\n",
      "\n",
      "\n",
      "def parse(version: str) -> \"Version\":\n",
      "    \"\"\"Parse the given version string.\n",
      "\n",
      "    >>> parse('1.0.dev1')\n",
      "    <Version('1.0.dev1')>\n",
      "\n",
      "    :param version: The version string to parse.\n",
      "    :raises InvalidVersion: When the version string is not a valid version.\n",
      "    \"\"\"\n",
      "    return Version(version)\n",
      "\n",
      "\n",
      "class InvalidVersion(ValueError):\n",
      "    \"\"\"Raised when a version string is not a valid version.\n",
      "\n",
      "    >>> Version(\"invalid\")\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    packaging.version.InvalidVersion: Invalid version: 'invalid'\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "class _BaseVersion:\n",
      "    _key: Tuple[Any, ...]\n",
      "\n",
      "    def __hash__(self) -> int:\n",
      "        return hash(self._key)\n",
      "\n",
      "    # Please keep the duplicated `isinstance` check\n",
      "    # in the six comparisons hereunder\n",
      "    # unless you find a way to avoid adding overhead function calls.\n",
      "    def __lt__(self, other: \"_BaseVersion\") -> bool:\n",
      "        if not isinstance(other, _BaseVersion):\n",
      "            return NotImplemented\n",
      "\n",
      "        return self._key < other._key\n",
      "\n",
      "    def __le__(self, other: \"_BaseVersion\") -> bool:\n",
      "        if not isinstance(other, _BaseVersion):\n",
      "            return NotImplemented\n",
      "\n",
      "        return self._key <= other._key\n",
      "\n",
      "    def __eq__(self, other: object) -> bool:\n",
      "        if not isinstance(other, _BaseVersion):\n",
      "            return NotImplemented\n",
      "\n",
      "        return self._key == other._key\n",
      "\n",
      "    def __ge__(self, other: \"_BaseVersion\") -> bool:\n",
      "        if not isinstance(other, _BaseVersion):\n",
      "            return NotImplemented\n",
      "\n",
      "        return self._key >= other._key\n",
      "\n",
      "    def __gt__(self, other: \"_BaseVersion\") -> bool:\n",
      "        if not isinstance(other, _BaseVersion):\n",
      "            return NotImplemented\n",
      "\n",
      "        return self._key > other._key\n",
      "\n",
      "    def __ne__(self, other: object) -> bool:\n",
      "        if not isinstance(other, _BaseVersion):\n",
      "            return NotImplemented\n",
      "\n",
      "        return self._key != other._key\n",
      "\n",
      "\n",
      "# Deliberately not anchored to the start and end of the string, to make it\n",
      "# easier for 3rd party code to reuse\n",
      "_VERSION_PATTERN = r\"\"\"\n",
      "    v?\n",
      "    (?:\n",
      "        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n",
      "        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n",
      "        (?P<pre>                                          # pre-release\n",
      "            [-_\\.]?\n",
      "            (?P<pre_l>alpha|a|beta|b|preview|pre|c|rc)\n",
      "            [-_\\.]?\n",
      "            (?P<pre_n>[0-9]+)?\n",
      "        )?\n",
      "        (?P<post>                                         # post release\n",
      "            (?:-(?P<post_n1>[0-9]+))\n",
      "            |\n",
      "            (?:\n",
      "                [-_\\.]?\n",
      "                (?P<post_l>post|rev|r)\n",
      "                [-_\\.]?\n",
      "                (?P<post_n2>[0-9]+)?\n",
      "            )\n",
      "        )?\n",
      "        (?P<dev>                                          # dev release\n",
      "            [-_\\.]?\n",
      "            (?P<dev_l>dev)\n",
      "            [-_\\.]?\n",
      "            (?P<dev_n>[0-9]+)?\n",
      "        )?\n",
      "    )\n",
      "    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n",
      "\"\"\"\n",
      "\n",
      "VERSION_PATTERN = _VERSION_PATTERN\n",
      "\"\"\"\n",
      "A string containing the regular expression used to match a valid version.\n",
      "\n",
      "The pattern is not anchored at either end, and is intended for embedding in larger\n",
      "expressions (for example, matching a version number as part of a file name). The\n",
      "regular expression should be compiled with the ``re.VERBOSE`` and ``re.IGNORECASE``\n",
      "flags set.\n",
      "\n",
      ":meta hide-value:\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "class Version(_BaseVersion):\n",
      "    \"\"\"This class abstracts handling of a project's versions.\n",
      "\n",
      "    A :class:`Version` instance is comparison aware and can be compared and\n",
      "    sorted using the standard Python interfaces.\n",
      "\n",
      "    >>> v1 = Version(\"1.0a5\")\n",
      "    >>> v2 = Version(\"1.0\")\n",
      "    >>> v1\n",
      "    <Version('1.0a5')>\n",
      "    >>> v2\n",
      "    <Version('1.0')>\n",
      "    >>> v1 < v2\n",
      "    True\n",
      "    >>> v1 == v2\n",
      "    False\n",
      "    >>> v1 > v2\n",
      "    False\n",
      "    >>> v1 >= v2\n",
      "    False\n",
      "    >>> v1 <= v2\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "    _regex = re.compile(r\"^\\s*\" + VERSION_PATTERN + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n",
      "    _key: CmpKey\n",
      "\n",
      "    def __init__(self, version: str) -> None:\n",
      "        \"\"\"Initialize a Version object.\n",
      "\n",
      "        :param version:\n",
      "            The string representation of a version which will be parsed and normalized\n",
      "            before use.\n",
      "        :raises InvalidVersion:\n",
      "            If the ``version`` does not conform to PEP 440 in any way then this\n",
      "            exception will be raised.\n",
      "        \"\"\"\n",
      "\n",
      "        # Validate the version and parse it into pieces\n",
      "        match = self._regex.search(version)\n",
      "        if not match:\n",
      "            raise InvalidVersion(f\"Invalid version: '{version}'\")\n",
      "\n",
      "        # Store the parsed out pieces of the version\n",
      "        self._version = _Version(\n",
      "            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n",
      "            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n",
      "            pre=_parse_letter_version(match.group(\"pre_l\"), match.group(\"pre_n\")),\n",
      "            post=_parse_letter_version(\n",
      "                match.group(\"post_l\"), match.group(\"post_n1\") or match.group(\"post_n2\")\n",
      "            ),\n",
      "            dev=_parse_letter_version(match.group(\"dev_l\"), match.group(\"dev_n\")),\n",
      "            local=_parse_local_version(match.group(\"local\")),\n",
      "        )\n",
      "\n",
      "        # Generate a key which will be used for sorting\n",
      "        self._key = _cmpkey(\n",
      "            self._version.epoch,\n",
      "            self._version.release,\n",
      "            self._version.pre,\n",
      "            self._version.post,\n",
      "            self._version.dev,\n",
      "            self._version.local,\n",
      "        )\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        \"\"\"A representation of the Version that shows all internal state.\n",
      "\n",
      "        >>> Version('1.0.0')\n",
      "        <Version('1.0.0')>\n",
      "        \"\"\"\n",
      "        return f\"<Version('{self}')>\"\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        \"\"\"A string representation of the version that can be rounded-tripped.\n",
      "\n",
      "        >>> str(Version(\"1.0a5\"))\n",
      "        '1.0a5'\n",
      "        \"\"\"\n",
      "        parts = []\n",
      "\n",
      "        # Epoch\n",
      "        if self.epoch != 0:\n",
      "            parts.append(f\"{self.epoch}!\")\n",
      "\n",
      "        # Release segment\n",
      "        parts.append(\".\".join(str(x) for x in self.release))\n",
      "\n",
      "        # Pre-release\n",
      "        if self.pre is not None:\n",
      "            parts.append(\"\".join(str(x) for x in self.pre))\n",
      "\n",
      "        # Post-release\n",
      "        if self.post is not None:\n",
      "            parts.append(f\".post{self.post}\")\n",
      "\n",
      "        # Development release\n",
      "        if self.dev is not None:\n",
      "            parts.append(f\".dev{self.dev}\")\n",
      "\n",
      "        # Local version segment\n",
      "        if self.local is not None:\n",
      "            parts.append(f\"+{self.local}\")\n",
      "\n",
      "        return \"\".join(parts)\n",
      "\n",
      "    @property\n",
      "    def epoch(self) -> int:\n",
      "        \"\"\"The epoch of the version.\n",
      "\n",
      "        >>> Version(\"2.0.0\").epoch\n",
      "        0\n",
      "        >>> Version(\"1!2.0.0\").epoch\n",
      "        1\n",
      "        \"\"\"\n",
      "        return self._version.epoch\n",
      "\n",
      "    @property\n",
      "    def release(self) -> Tuple[int, ...]:\n",
      "        \"\"\"The components of the \"release\" segment of the version.\n",
      "\n",
      "        >>> Version(\"1.2.3\").release\n",
      "        (1, 2, 3)\n",
      "        >>> Version(\"2.0.0\").release\n",
      "        (2, 0, 0)\n",
      "        >>> Version(\"1!2.0.0.post0\").release\n",
      "        (2, 0, 0)\n",
      "\n",
      "        Includes trailing zeroes but not the epoch or any pre-release / development /\n",
      "        post-release suffixes.\n",
      "        \"\"\"\n",
      "        return self._version.release\n",
      "\n",
      "    @property\n",
      "    def pre(self) -> Optional[Tuple[str, int]]:\n",
      "        \"\"\"The pre-release segment of the version.\n",
      "\n",
      "        >>> print(Version(\"1.2.3\").pre)\n",
      "        None\n",
      "        >>> Version(\"1.2.3a1\").pre\n",
      "        ('a', 1)\n",
      "        >>> Version(\"1.2.3b1\").pre\n",
      "        ('b', 1)\n",
      "        >>> Version(\"1.2.3rc1\").pre\n",
      "        ('rc', 1)\n",
      "        \"\"\"\n",
      "        return self._version.pre\n",
      "\n",
      "    @property\n",
      "    def post(self) -> Optional[int]:\n",
      "        \"\"\"The post-release number of the version.\n",
      "\n",
      "        >>> print(Version(\"1.2.3\").post)\n",
      "        None\n",
      "        >>> Version(\"1.2.3.post1\").post\n",
      "        1\n",
      "        \"\"\"\n",
      "        return self._version.post[1] if self._version.post else None\n",
      "\n",
      "    @property\n",
      "    def dev(self) -> Optional[int]:\n",
      "        \"\"\"The development number of the version.\n",
      "\n",
      "        >>> print(Version(\"1.2.3\").dev)\n",
      "        None\n",
      "        >>> Version(\"1.2.3.dev1\").dev\n",
      "        1\n",
      "        \"\"\"\n",
      "        return self._version.dev[1] if self._version.dev else None\n",
      "\n",
      "    @property\n",
      "    def local(self) -> Optional[str]:\n",
      "        \"\"\"The local version segment of the version.\n",
      "\n",
      "        >>> print(Version(\"1.2.3\").local)\n",
      "        None\n",
      "        >>> Version(\"1.2.3+abc\").local\n",
      "        'abc'\n",
      "        \"\"\"\n",
      "        if self._version.local:\n",
      "            return \".\".join(str(x) for x in self._version.local)\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    @property\n",
      "    def public(self) -> str:\n",
      "        \"\"\"The public portion of the version.\n",
      "\n",
      "        >>> Version(\"1.2.3\").public\n",
      "        '1.2.3'\n",
      "        >>> Version(\"1.2.3+abc\").public\n",
      "        '1.2.3'\n",
      "        >>> Version(\"1.2.3+abc.dev1\").public\n",
      "        '1.2.3'\n",
      "        \"\"\"\n",
      "        return str(self).split(\"+\", 1)[0]\n",
      "\n",
      "    @property\n",
      "    def base_version(self) -> str:\n",
      "        \"\"\"The \"base version\" of the version.\n",
      "\n",
      "        >>> Version(\"1.2.3\").base_version\n",
      "        '1.2.3'\n",
      "        >>> Version(\"1.2.3+abc\").base_version\n",
      "        '1.2.3'\n",
      "        >>> Version(\"1!1.2.3+abc.dev1\").base_version\n",
      "        '1!1.2.3'\n",
      "\n",
      "        The \"base version\" is the public version of the project without any pre or post\n",
      "        release markers.\n",
      "        \"\"\"\n",
      "        parts = []\n",
      "\n",
      "        # Epoch\n",
      "        if self.epoch != 0:\n",
      "            parts.append(f\"{self.epoch}!\")\n",
      "\n",
      "        # Release segment\n",
      "        parts.append(\".\".join(str(x) for x in self.release))\n",
      "\n",
      "        return \"\".join(parts)\n",
      "\n",
      "    @property\n",
      "    def is_prerelease(self) -> bool:\n",
      "        \"\"\"Whether this version is a pre-release.\n",
      "\n",
      "        >>> Version(\"1.2.3\").is_prerelease\n",
      "        False\n",
      "        >>> Version(\"1.2.3a1\").is_prerelease\n",
      "        True\n",
      "        >>> Version(\"1.2.3b1\").is_prerelease\n",
      "        True\n",
      "        >>> Version(\"1.2.3rc1\").is_prerelease\n",
      "        True\n",
      "        >>> Version(\"1.2.3dev1\").is_prerelease\n",
      "        True\n",
      "        \"\"\"\n",
      "        return self.dev is not None or self.pre is not None\n",
      "\n",
      "    @property\n",
      "    def is_postrelease(self) -> bool:\n",
      "        \"\"\"Whether this version is a post-release.\n",
      "\n",
      "        >>> Version(\"1.2.3\").is_postrelease\n",
      "        False\n",
      "        >>> Version(\"1.2.3.post1\").is_postrelease\n",
      "        True\n",
      "        \"\"\"\n",
      "        return self.post is not None\n",
      "\n",
      "    @property\n",
      "    def is_devrelease(self) -> bool:\n",
      "        \"\"\"Whether this version is a development release.\n",
      "\n",
      "        >>> Version(\"1.2.3\").is_devrelease\n",
      "        False\n",
      "        >>> Version(\"1.2.3.dev1\").is_devrelease\n",
      "        True\n",
      "        \"\"\"\n",
      "        return self.dev is not None\n",
      "\n",
      "    @property\n",
      "    def major(self) -> int:\n",
      "        \"\"\"The first item of :attr:`release` or ``0`` if unavailable.\n",
      "\n",
      "        >>> Version(\"1.2.3\").major\n",
      "        1\n",
      "        \"\"\"\n",
      "        return self.release[0] if len(self.release) >= 1 else 0\n",
      "\n",
      "    @property\n",
      "    def minor(self) -> int:\n",
      "        \"\"\"The second item of :attr:`release` or ``0`` if unavailable.\n",
      "\n",
      "        >>> Version(\"1.2.3\").minor\n",
      "        2\n",
      "        >>> Version(\"1\").minor\n",
      "        0\n",
      "        \"\"\"\n",
      "        return self.release[1] if len(self.release) >= 2 else 0\n",
      "\n",
      "    @property\n",
      "    def micro(self) -> int:\n",
      "        \"\"\"The third item of :attr:`release` or ``0`` if unavailable.\n",
      "\n",
      "        >>> Version(\"1.2.3\").micro\n",
      "        3\n",
      "        >>> Version(\"1\").micro\n",
      "        0\n",
      "        \"\"\"\n",
      "        return self.release[2] if len(self.release) >= 3 else 0\n",
      "\n",
      "\n",
      "def _parse_letter_version(\n",
      "    letter: Optional[str], number: Union[str, bytes, SupportsInt, None]\n",
      ") -> Optional[Tuple[str, int]]:\n",
      "    if letter:\n",
      "        # We consider there to be an implicit 0 in a pre-release if there is\n",
      "        # not a numeral associated with it.\n",
      "        if number is None:\n",
      "            number = 0\n",
      "\n",
      "        # We normalize any letters to their lower case form\n",
      "        letter = letter.lower()\n",
      "\n",
      "        # We consider some words to be alternate spellings of other words and\n",
      "        # in those cases we want to normalize the spellings to our preferred\n",
      "        # spelling.\n",
      "        if letter == \"alpha\":\n",
      "            letter = \"a\"\n",
      "        elif letter == \"beta\":\n",
      "            letter = \"b\"\n",
      "        elif letter in [\"c\", \"pre\", \"preview\"]:\n",
      "            letter = \"rc\"\n",
      "        elif letter in [\"rev\", \"r\"]:\n",
      "            letter = \"post\"\n",
      "\n",
      "        return letter, int(number)\n",
      "    if not letter and number:\n",
      "        # We assume if we are given a number, but we are not given a letter\n",
      "        # then this is using the implicit post release syntax (e.g. 1.0-1)\n",
      "        letter = \"post\"\n",
      "\n",
      "        return letter, int(number)\n",
      "\n",
      "    return None\n",
      "\n",
      "\n",
      "_local_version_separators = re.compile(r\"[\\._-]\")\n",
      "\n",
      "\n",
      "def _parse_local_version(local: Optional[str]) -> Optional[LocalType]:\n",
      "    \"\"\"\n",
      "    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n",
      "    \"\"\"\n",
      "    if local is not None:\n",
      "        return tuple(\n",
      "            part.lower() if not part.isdigit() else int(part)\n",
      "            for part in _local_version_separators.split(local)\n",
      "        )\n",
      "    return None\n",
      "\n",
      "\n",
      "def _cmpkey(\n",
      "    epoch: int,\n",
      "    release: Tuple[int, ...],\n",
      "    pre: Optional[Tuple[str, int]],\n",
      "    post: Optional[Tuple[str, int]],\n",
      "    dev: Optional[Tuple[str, int]],\n",
      "    local: Optional[LocalType],\n",
      ") -> CmpKey:\n",
      "    # When we compare a release version, we want to compare it with all of the\n",
      "    # trailing zeros removed. So we'll use a reverse the list, drop all the now\n",
      "    # leading zeros until we come to something non zero, then take the rest\n",
      "    # re-reverse it back into the correct order and make it a tuple and use\n",
      "    # that for our sorting key.\n",
      "    _release = tuple(\n",
      "        reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))\n",
      "    )\n",
      "\n",
      "    # We need to \"trick\" the sorting algorithm to put 1.0.dev0 before 1.0a0.\n",
      "    # We'll do this by abusing the pre segment, but we _only_ want to do this\n",
      "    # if there is not a pre or a post segment. If we have one of those then\n",
      "    # the normal sorting rules will handle this case correctly.\n",
      "    if pre is None and post is None and dev is not None:\n",
      "        _pre: CmpPrePostDevType = NegativeInfinity\n",
      "    # Versions without a pre-release (except as noted above) should sort after\n",
      "    # those with one.\n",
      "    elif pre is None:\n",
      "        _pre = Infinity\n",
      "    else:\n",
      "        _pre = pre\n",
      "\n",
      "    # Versions without a post segment should sort before those with one.\n",
      "    if post is None:\n",
      "        _post: CmpPrePostDevType = NegativeInfinity\n",
      "\n",
      "    else:\n",
      "        _post = post\n",
      "\n",
      "    # Versions without a development segment should sort after those with one.\n",
      "    if dev is None:\n",
      "        _dev: CmpPrePostDevType = Infinity\n",
      "\n",
      "    else:\n",
      "        _dev = dev\n",
      "\n",
      "    if local is None:\n",
      "        # Versions without a local segment should sort before those with one.\n",
      "        _local: CmpLocalType = NegativeInfinity\n",
      "    else:\n",
      "        # Versions with a local segment need that segment parsed to implement\n",
      "        # the sorting rules in PEP440.\n",
      "        # - Alpha numeric segments sort before numeric segments\n",
      "        # - Alpha numeric segments sort lexicographically\n",
      "        # - Numeric segments sort numerically\n",
      "        # - Shorter versions sort before longer versions when the prefixes\n",
      "        #   match exactly\n",
      "        _local = tuple(\n",
      "            (i, \"\") if isinstance(i, int) else (NegativeInfinity, i) for i in local\n",
      "        )\n",
      "\n",
      "    return epoch, _release, _pre, _post, _dev, _local\n",
      "\n",
      "[end of src/wheel/vendored/packaging/version.py]\n",
      "[start of src/wheel/vendored/packaging/LICENSE.APACHE]\n",
      "\n",
      "                                 Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "      and distribution as defined by Sections 1 through 9 of this document.\n",
      "\n",
      "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
      "      the copyright owner that is granting the License.\n",
      "\n",
      "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
      "      other entities that control, are controlled by, or are under common\n",
      "      control with that entity. For the purposes of this definition,\n",
      "      \"control\" means (i) the power, direct or indirect, to cause the\n",
      "      direction or management of such entity, whether by contract or\n",
      "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "\n",
      "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
      "      exercising permissions granted by this License.\n",
      "\n",
      "      \"Source\" form shall mean the preferred form for making modifications,\n",
      "      including but not limited to software source code, documentation\n",
      "      source, and configuration files.\n",
      "\n",
      "      \"Object\" form shall mean any form resulting from mechanical\n",
      "      transformation or translation of a Source form, including but\n",
      "      not limited to compiled object code, generated documentation,\n",
      "      and conversions to other media types.\n",
      "\n",
      "      \"Work\" shall mean the work of authorship, whether in Source or\n",
      "      Object form, made available under the License, as indicated by a\n",
      "      copyright notice that is included in or attached to the work\n",
      "      (an example is provided in the Appendix below).\n",
      "\n",
      "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
      "      form, that is based on (or derived from) the Work and for which the\n",
      "      editorial revisions, annotations, elaborations, or other modifications\n",
      "      represent, as a whole, an original work of authorship. For the purposes\n",
      "      of this License, Derivative Works shall not include works that remain\n",
      "      separable from, or merely link (or bind by name) to the interfaces of,\n",
      "      the Work and Derivative Works thereof.\n",
      "\n",
      "      \"Contribution\" shall mean any work of authorship, including\n",
      "      the original version of the Work and any modifications or additions\n",
      "      to that Work or Derivative Works thereof, that is intentionally\n",
      "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
      "      or by an individual or Legal Entity authorized to submit on behalf of\n",
      "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
      "      means any form of electronic, verbal, or written communication sent\n",
      "      to the Licensor or its representatives, including but not limited to\n",
      "      communication on electronic mailing lists, source code control systems,\n",
      "      and issue tracking systems that are managed by, or on behalf of, the\n",
      "      Licensor for the purpose of discussing and improving the Work, but\n",
      "      excluding communication that is conspicuously marked or otherwise\n",
      "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
      "\n",
      "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
      "      on behalf of whom a Contribution has been received by Licensor and\n",
      "      subsequently incorporated within the Work.\n",
      "\n",
      "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
      "      this License, each Contributor hereby grants to You a perpetual,\n",
      "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "      copyright license to reproduce, prepare Derivative Works of,\n",
      "      publicly display, publicly perform, sublicense, and distribute the\n",
      "      Work and such Derivative Works in Source or Object form.\n",
      "\n",
      "   3. Grant of Patent License. Subject to the terms and conditions of\n",
      "      this License, each Contributor hereby grants to You a perpetual,\n",
      "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "      (except as stated in this section) patent license to make, have made,\n",
      "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
      "      where such license applies only to those patent claims licensable\n",
      "      by such Contributor that are necessarily infringed by their\n",
      "      Contribution(s) alone or by combination of their Contribution(s)\n",
      "      with the Work to which such Contribution(s) was submitted. If You\n",
      "      institute patent litigation against any entity (including a\n",
      "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
      "      or a Contribution incorporated within the Work constitutes direct\n",
      "      or contributory patent infringement, then any patent licenses\n",
      "      granted to You under this License for that Work shall terminate\n",
      "      as of the date such litigation is filed.\n",
      "\n",
      "   4. Redistribution. You may reproduce and distribute copies of the\n",
      "      Work or Derivative Works thereof in any medium, with or without\n",
      "      modifications, and in Source or Object form, provided that You\n",
      "      meet the following conditions:\n",
      "\n",
      "      (a) You must give any other recipients of the Work or\n",
      "          Derivative Works a copy of this License; and\n",
      "\n",
      "      (b) You must cause any modified files to carry prominent notices\n",
      "          stating that You changed the files; and\n",
      "\n",
      "      (c) You must retain, in the Source form of any Derivative Works\n",
      "          that You distribute, all copyright, patent, trademark, and\n",
      "          attribution notices from the Source form of the Work,\n",
      "          excluding those notices that do not pertain to any part of\n",
      "          the Derivative Works; and\n",
      "\n",
      "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
      "          distribution, then any Derivative Works that You distribute must\n",
      "          include a readable copy of the attribution notices contained\n",
      "          within such NOTICE file, excluding those notices that do not\n",
      "          pertain to any part of the Derivative Works, in at least one\n",
      "          of the following places: within a NOTICE text file distributed\n",
      "          as part of the Derivative Works; within the Source form or\n",
      "          documentation, if provided along with the Derivative Works; or,\n",
      "          within a display generated by the Derivative Works, if and\n",
      "          wherever such third-party notices normally appear. The contents\n",
      "          of the NOTICE file are for informational purposes only and\n",
      "          do not modify the License. You may add Your own attribution\n",
      "          notices within Derivative Works that You distribute, alongside\n",
      "          or as an addendum to the NOTICE text from the Work, provided\n",
      "          that such additional attribution notices cannot be construed\n",
      "          as modifying the License.\n",
      "\n",
      "      You may add Your own copyright statement to Your modifications and\n",
      "      may provide additional or different license terms and conditions\n",
      "      for use, reproduction, or distribution of Your modifications, or\n",
      "      for any such Derivative Works as a whole, provided Your use,\n",
      "      reproduction, and distribution of the Work otherwise complies with\n",
      "      the conditions stated in this License.\n",
      "\n",
      "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
      "      any Contribution intentionally submitted for inclusion in the Work\n",
      "      by You to the Licensor shall be under the terms and conditions of\n",
      "      this License, without any additional terms or conditions.\n",
      "      Notwithstanding the above, nothing herein shall supersede or modify\n",
      "      the terms of any separate license agreement you may have executed\n",
      "      with Licensor regarding such Contributions.\n",
      "\n",
      "   6. Trademarks. This License does not grant permission to use the trade\n",
      "      names, trademarks, service marks, or product names of the Licensor,\n",
      "      except as required for reasonable and customary use in describing the\n",
      "      origin of the Work and reproducing the content of the NOTICE file.\n",
      "\n",
      "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
      "      agreed to in writing, Licensor provides the Work (and each\n",
      "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
      "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
      "      implied, including, without limitation, any warranties or conditions\n",
      "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
      "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
      "      appropriateness of using or redistributing the Work and assume any\n",
      "      risks associated with Your exercise of permissions under this License.\n",
      "\n",
      "   8. Limitation of Liability. In no event and under no legal theory,\n",
      "      whether in tort (including negligence), contract, or otherwise,\n",
      "      unless required by applicable law (such as deliberate and grossly\n",
      "      negligent acts) or agreed to in writing, shall any Contributor be\n",
      "      liable to You for damages, including any direct, indirect, special,\n",
      "      incidental, or consequential damages of any character arising as a\n",
      "      result of this License or out of the use or inability to use the\n",
      "      Work (including but not limited to damages for loss of goodwill,\n",
      "      work stoppage, computer failure or malfunction, or any and all\n",
      "      other commercial damages or losses), even if such Contributor\n",
      "      has been advised of the possibility of such damages.\n",
      "\n",
      "   9. Accepting Warranty or Additional Liability. While redistributing\n",
      "      the Work or Derivative Works thereof, You may choose to offer,\n",
      "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
      "      or other liability obligations and/or rights consistent with this\n",
      "      License. However, in accepting such obligations, You may act only\n",
      "      on Your own behalf and on Your sole responsibility, not on behalf\n",
      "      of any other Contributor, and only if You agree to indemnify,\n",
      "      defend, and hold each Contributor harmless for any liability\n",
      "      incurred by, or claims asserted against, such Contributor by reason\n",
      "      of your accepting any such warranty or additional liability.\n",
      "\n",
      "   END OF TERMS AND CONDITIONS\n",
      "\n",
      "[end of src/wheel/vendored/packaging/LICENSE.APACHE]\n",
      "[start of src/wheel/vendored/packaging/_parser.py]\n",
      "\"\"\"Handwritten parser of dependency specifiers.\n",
      "\n",
      "The docstring for each __parse_* function contains EBNF-inspired grammar representing\n",
      "the implementation.\n",
      "\"\"\"\n",
      "\n",
      "import ast\n",
      "from typing import Any, List, NamedTuple, Optional, Tuple, Union\n",
      "\n",
      "from ._tokenizer import DEFAULT_RULES, Tokenizer\n",
      "\n",
      "\n",
      "class Node:\n",
      "    def __init__(self, value: str) -> None:\n",
      "        self.value = value\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        return self.value\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        return f\"<{self.__class__.__name__}('{self}')>\"\n",
      "\n",
      "    def serialize(self) -> str:\n",
      "        raise NotImplementedError\n",
      "\n",
      "\n",
      "class Variable(Node):\n",
      "    def serialize(self) -> str:\n",
      "        return str(self)\n",
      "\n",
      "\n",
      "class Value(Node):\n",
      "    def serialize(self) -> str:\n",
      "        return f'\"{self}\"'\n",
      "\n",
      "\n",
      "class Op(Node):\n",
      "    def serialize(self) -> str:\n",
      "        return str(self)\n",
      "\n",
      "\n",
      "MarkerVar = Union[Variable, Value]\n",
      "MarkerItem = Tuple[MarkerVar, Op, MarkerVar]\n",
      "# MarkerAtom = Union[MarkerItem, List[\"MarkerAtom\"]]\n",
      "# MarkerList = List[Union[\"MarkerList\", MarkerAtom, str]]\n",
      "# mypy does not support recursive type definition\n",
      "# https://github.com/python/mypy/issues/731\n",
      "MarkerAtom = Any\n",
      "MarkerList = List[Any]\n",
      "\n",
      "\n",
      "class ParsedRequirement(NamedTuple):\n",
      "    name: str\n",
      "    url: str\n",
      "    extras: List[str]\n",
      "    specifier: str\n",
      "    marker: Optional[MarkerList]\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------------------------------\n",
      "# Recursive descent parser for dependency specifier\n",
      "# --------------------------------------------------------------------------------------\n",
      "def parse_requirement(source: str) -> ParsedRequirement:\n",
      "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "\n",
      "\n",
      "def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n",
      "    \"\"\"\n",
      "    requirement = WS? IDENTIFIER WS? extras WS? requirement_details\n",
      "    \"\"\"\n",
      "    tokenizer.consume(\"WS\")\n",
      "\n",
      "    name_token = tokenizer.expect(\n",
      "        \"IDENTIFIER\", expected=\"package name at the start of dependency specifier\"\n",
      "    )\n",
      "    name = name_token.text\n",
      "    tokenizer.consume(\"WS\")\n",
      "\n",
      "    extras = _parse_extras(tokenizer)\n",
      "    tokenizer.consume(\"WS\")\n",
      "\n",
      "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "    tokenizer.expect(\"END\", expected=\"end of dependency specifier\")\n",
      "\n",
      "    return ParsedRequirement(name, url, extras, specifier, marker)\n",
      "\n",
      "\n",
      "def _parse_requirement_details(\n",
      "    tokenizer: Tokenizer,\n",
      ") -> Tuple[str, str, Optional[MarkerList]]:\n",
      "    \"\"\"\n",
      "    requirement_details = AT URL (WS requirement_marker?)?\n",
      "                        | specifier WS? (requirement_marker)?\n",
      "    \"\"\"\n",
      "\n",
      "    specifier = \"\"\n",
      "    url = \"\"\n",
      "    marker = None\n",
      "\n",
      "    if tokenizer.check(\"AT\"):\n",
      "        tokenizer.read()\n",
      "        tokenizer.consume(\"WS\")\n",
      "\n",
      "        url_start = tokenizer.position\n",
      "        url = tokenizer.expect(\"URL\", expected=\"URL after @\").text\n",
      "        if tokenizer.check(\"END\", peek=True):\n",
      "            return (url, specifier, marker)\n",
      "\n",
      "        tokenizer.expect(\"WS\", expected=\"whitespace after URL\")\n",
      "\n",
      "        # The input might end after whitespace.\n",
      "        if tokenizer.check(\"END\", peek=True):\n",
      "            return (url, specifier, marker)\n",
      "\n",
      "        marker = _parse_requirement_marker(\n",
      "            tokenizer, span_start=url_start, after=\"URL and whitespace\"\n",
      "        )\n",
      "    else:\n",
      "        specifier_start = tokenizer.position\n",
      "        specifier = _parse_specifier(tokenizer)\n",
      "        tokenizer.consume(\"WS\")\n",
      "\n",
      "        if tokenizer.check(\"END\", peek=True):\n",
      "            return (url, specifier, marker)\n",
      "\n",
      "        marker = _parse_requirement_marker(\n",
      "            tokenizer,\n",
      "            span_start=specifier_start,\n",
      "            after=(\n",
      "                \"version specifier\"\n",
      "                if specifier\n",
      "                else \"name and no valid version specifier\"\n",
      "            ),\n",
      "        )\n",
      "\n",
      "    return (url, specifier, marker)\n",
      "\n",
      "\n",
      "def _parse_requirement_marker(\n",
      "    tokenizer: Tokenizer, *, span_start: int, after: str\n",
      ") -> MarkerList:\n",
      "    \"\"\"\n",
      "    requirement_marker = SEMICOLON marker WS?\n",
      "    \"\"\"\n",
      "\n",
      "    if not tokenizer.check(\"SEMICOLON\"):\n",
      "        tokenizer.raise_syntax_error(\n",
      "            f\"Expected end or semicolon (after {after})\",\n",
      "            span_start=span_start,\n",
      "        )\n",
      "    tokenizer.read()\n",
      "\n",
      "    marker = _parse_marker(tokenizer)\n",
      "    tokenizer.consume(\"WS\")\n",
      "\n",
      "    return marker\n",
      "\n",
      "\n",
      "def _parse_extras(tokenizer: Tokenizer) -> List[str]:\n",
      "    \"\"\"\n",
      "    extras = (LEFT_BRACKET wsp* extras_list? wsp* RIGHT_BRACKET)?\n",
      "    \"\"\"\n",
      "    if not tokenizer.check(\"LEFT_BRACKET\", peek=True):\n",
      "        return []\n",
      "\n",
      "    with tokenizer.enclosing_tokens(\n",
      "        \"LEFT_BRACKET\",\n",
      "        \"RIGHT_BRACKET\",\n",
      "        around=\"extras\",\n",
      "    ):\n",
      "        tokenizer.consume(\"WS\")\n",
      "        extras = _parse_extras_list(tokenizer)\n",
      "        tokenizer.consume(\"WS\")\n",
      "\n",
      "    return extras\n",
      "\n",
      "\n",
      "def _parse_extras_list(tokenizer: Tokenizer) -> List[str]:\n",
      "    \"\"\"\n",
      "    extras_list = identifier (wsp* ',' wsp* identifier)*\n",
      "    \"\"\"\n",
      "    extras: List[str] = []\n",
      "\n",
      "    if not tokenizer.check(\"IDENTIFIER\"):\n",
      "        return extras\n",
      "\n",
      "    extras.append(tokenizer.read().text)\n",
      "\n",
      "    while True:\n",
      "        tokenizer.consume(\"WS\")\n",
      "        if tokenizer.check(\"IDENTIFIER\", peek=True):\n",
      "            tokenizer.raise_syntax_error(\"Expected comma between extra names\")\n",
      "        elif not tokenizer.check(\"COMMA\"):\n",
      "            break\n",
      "\n",
      "        tokenizer.read()\n",
      "        tokenizer.consume(\"WS\")\n",
      "\n",
      "        extra_token = tokenizer.expect(\"IDENTIFIER\", expected=\"extra name after comma\")\n",
      "        extras.append(extra_token.text)\n",
      "\n",
      "    return extras\n",
      "\n",
      "\n",
      "def _parse_specifier(tokenizer: Tokenizer) -> str:\n",
      "    \"\"\"\n",
      "    specifier = LEFT_PARENTHESIS WS? version_many WS? RIGHT_PARENTHESIS\n",
      "              | WS? version_many WS?\n",
      "    \"\"\"\n",
      "    with tokenizer.enclosing_tokens(\n",
      "        \"LEFT_PARENTHESIS\",\n",
      "        \"RIGHT_PARENTHESIS\",\n",
      "        around=\"version specifier\",\n",
      "    ):\n",
      "        tokenizer.consume(\"WS\")\n",
      "        parsed_specifiers = _parse_version_many(tokenizer)\n",
      "        tokenizer.consume(\"WS\")\n",
      "\n",
      "    return parsed_specifiers\n",
      "\n",
      "\n",
      "def _parse_version_many(tokenizer: Tokenizer) -> str:\n",
      "    \"\"\"\n",
      "    version_many = (SPECIFIER (WS? COMMA WS? SPECIFIER)*)?\n",
      "    \"\"\"\n",
      "    parsed_specifiers = \"\"\n",
      "    while tokenizer.check(\"SPECIFIER\"):\n",
      "        span_start = tokenizer.position\n",
      "        parsed_specifiers += tokenizer.read().text\n",
      "        if tokenizer.check(\"VERSION_PREFIX_TRAIL\", peek=True):\n",
      "            tokenizer.raise_syntax_error(\n",
      "                \".* suffix can only be used with `==` or `!=` operators\",\n",
      "                span_start=span_start,\n",
      "                span_end=tokenizer.position + 1,\n",
      "            )\n",
      "        if tokenizer.check(\"VERSION_LOCAL_LABEL_TRAIL\", peek=True):\n",
      "            tokenizer.raise_syntax_error(\n",
      "                \"Local version label can only be used with `==` or `!=` operators\",\n",
      "                span_start=span_start,\n",
      "                span_end=tokenizer.position,\n",
      "            )\n",
      "        tokenizer.consume(\"WS\")\n",
      "        if not tokenizer.check(\"COMMA\"):\n",
      "            break\n",
      "        parsed_specifiers += tokenizer.read().text\n",
      "        tokenizer.consume(\"WS\")\n",
      "\n",
      "    return parsed_specifiers\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------------------------------\n",
      "# Recursive descent parser for marker expression\n",
      "# --------------------------------------------------------------------------------------\n",
      "def parse_marker(source: str) -> MarkerList:\n",
      "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "\n",
      "\n",
      "def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:\n",
      "    retval = _parse_marker(tokenizer)\n",
      "    tokenizer.expect(\"END\", expected=\"end of marker expression\")\n",
      "    return retval\n",
      "\n",
      "\n",
      "def _parse_marker(tokenizer: Tokenizer) -> MarkerList:\n",
      "    \"\"\"\n",
      "    marker = marker_atom (BOOLOP marker_atom)+\n",
      "    \"\"\"\n",
      "    expression = [_parse_marker_atom(tokenizer)]\n",
      "    while tokenizer.check(\"BOOLOP\"):\n",
      "        token = tokenizer.read()\n",
      "        expr_right = _parse_marker_atom(tokenizer)\n",
      "        expression.extend((token.text, expr_right))\n",
      "    return expression\n",
      "\n",
      "\n",
      "def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:\n",
      "    \"\"\"\n",
      "    marker_atom = WS? LEFT_PARENTHESIS WS? marker WS? RIGHT_PARENTHESIS WS?\n",
      "                | WS? marker_item WS?\n",
      "    \"\"\"\n",
      "\n",
      "    tokenizer.consume(\"WS\")\n",
      "    if tokenizer.check(\"LEFT_PARENTHESIS\", peek=True):\n",
      "        with tokenizer.enclosing_tokens(\n",
      "            \"LEFT_PARENTHESIS\",\n",
      "            \"RIGHT_PARENTHESIS\",\n",
      "            around=\"marker expression\",\n",
      "        ):\n",
      "            tokenizer.consume(\"WS\")\n",
      "            marker: MarkerAtom = _parse_marker(tokenizer)\n",
      "            tokenizer.consume(\"WS\")\n",
      "    else:\n",
      "        marker = _parse_marker_item(tokenizer)\n",
      "    tokenizer.consume(\"WS\")\n",
      "    return marker\n",
      "\n",
      "\n",
      "def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:\n",
      "    \"\"\"\n",
      "    marker_item = WS? marker_var WS? marker_op WS? marker_var WS?\n",
      "    \"\"\"\n",
      "    tokenizer.consume(\"WS\")\n",
      "    marker_var_left = _parse_marker_var(tokenizer)\n",
      "    tokenizer.consume(\"WS\")\n",
      "    marker_op = _parse_marker_op(tokenizer)\n",
      "    tokenizer.consume(\"WS\")\n",
      "    marker_var_right = _parse_marker_var(tokenizer)\n",
      "    tokenizer.consume(\"WS\")\n",
      "    return (marker_var_left, marker_op, marker_var_right)\n",
      "\n",
      "\n",
      "def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:\n",
      "    \"\"\"\n",
      "    marker_var = VARIABLE | QUOTED_STRING\n",
      "    \"\"\"\n",
      "    if tokenizer.check(\"VARIABLE\"):\n",
      "        return process_env_var(tokenizer.read().text.replace(\".\", \"_\"))\n",
      "    elif tokenizer.check(\"QUOTED_STRING\"):\n",
      "        return process_python_str(tokenizer.read().text)\n",
      "    else:\n",
      "        tokenizer.raise_syntax_error(\n",
      "            message=\"Expected a marker variable or quoted string\"\n",
      "        )\n",
      "\n",
      "\n",
      "def process_env_var(env_var: str) -> Variable:\n",
      "    if env_var in (\"platform_python_implementation\", \"python_implementation\"):\n",
      "        return Variable(\"platform_python_implementation\")\n",
      "    else:\n",
      "        return Variable(env_var)\n",
      "\n",
      "\n",
      "def process_python_str(python_str: str) -> Value:\n",
      "    value = ast.literal_eval(python_str)\n",
      "    return Value(str(value))\n",
      "\n",
      "\n",
      "def _parse_marker_op(tokenizer: Tokenizer) -> Op:\n",
      "    \"\"\"\n",
      "    marker_op = IN | NOT IN | OP\n",
      "    \"\"\"\n",
      "    if tokenizer.check(\"IN\"):\n",
      "        tokenizer.read()\n",
      "        return Op(\"in\")\n",
      "    elif tokenizer.check(\"NOT\"):\n",
      "        tokenizer.read()\n",
      "        tokenizer.expect(\"WS\", expected=\"whitespace after 'not'\")\n",
      "        tokenizer.expect(\"IN\", expected=\"'in' after 'not'\")\n",
      "        return Op(\"not in\")\n",
      "    elif tokenizer.check(\"OP\"):\n",
      "        return Op(tokenizer.read().text)\n",
      "    else:\n",
      "        return tokenizer.raise_syntax_error(\n",
      "            \"Expected marker operator, one of \"\n",
      "            \"<=, <, !=, ==, >=, >, ~=, ===, in, not in\"\n",
      "        )\n",
      "\n",
      "[end of src/wheel/vendored/packaging/_parser.py]\n",
      "[start of src/wheel/vendored/packaging/_elffile.py]\n",
      "\"\"\"\n",
      "ELF file parser.\n",
      "\n",
      "This provides a class ``ELFFile`` that parses an ELF executable in a similar\n",
      "interface to ``ZipFile``. Only the read interface is implemented.\n",
      "\n",
      "Based on: https://gist.github.com/lyssdod/f51579ae8d93c8657a5564aefc2ffbca\n",
      "ELF header: https://refspecs.linuxfoundation.org/elf/gabi4+/ch4.eheader.html\n",
      "\"\"\"\n",
      "\n",
      "import enum\n",
      "import os\n",
      "import struct\n",
      "from typing import IO, Optional, Tuple\n",
      "\n",
      "\n",
      "class ELFInvalid(ValueError):\n",
      "    pass\n",
      "\n",
      "\n",
      "class EIClass(enum.IntEnum):\n",
      "    C32 = 1\n",
      "    C64 = 2\n",
      "\n",
      "\n",
      "class EIData(enum.IntEnum):\n",
      "    Lsb = 1\n",
      "    Msb = 2\n",
      "\n",
      "\n",
      "class EMachine(enum.IntEnum):\n",
      "    I386 = 3\n",
      "    S390 = 22\n",
      "    Arm = 40\n",
      "    X8664 = 62\n",
      "    AArc64 = 183\n",
      "\n",
      "\n",
      "class ELFFile:\n",
      "    \"\"\"\n",
      "    Representation of an ELF executable.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, f: IO[bytes]) -> None:\n",
      "        self._f = f\n",
      "\n",
      "        try:\n",
      "            ident = self._read(\"16B\")\n",
      "        except struct.error:\n",
      "            raise ELFInvalid(\"unable to parse identification\")\n",
      "        magic = bytes(ident[:4])\n",
      "        if magic != b\"\\x7fELF\":\n",
      "            raise ELFInvalid(f\"invalid magic: {magic!r}\")\n",
      "\n",
      "        self.capacity = ident[4]  # Format for program header (bitness).\n",
      "        self.encoding = ident[5]  # Data structure encoding (endianness).\n",
      "\n",
      "        try:\n",
      "            # e_fmt: Format for program header.\n",
      "            # p_fmt: Format for section header.\n",
      "            # p_idx: Indexes to find p_type, p_offset, and p_filesz.\n",
      "            e_fmt, self._p_fmt, self._p_idx = {\n",
      "                (1, 1): (\"<HHIIIIIHHH\", \"<IIIIIIII\", (0, 1, 4)),  # 32-bit LSB.\n",
      "                (1, 2): (\">HHIIIIIHHH\", \">IIIIIIII\", (0, 1, 4)),  # 32-bit MSB.\n",
      "                (2, 1): (\"<HHIQQQIHHH\", \"<IIQQQQQQ\", (0, 2, 5)),  # 64-bit LSB.\n",
      "                (2, 2): (\">HHIQQQIHHH\", \">IIQQQQQQ\", (0, 2, 5)),  # 64-bit MSB.\n",
      "            }[(self.capacity, self.encoding)]\n",
      "        except KeyError:\n",
      "            raise ELFInvalid(\n",
      "                f\"unrecognized capacity ({self.capacity}) or \"\n",
      "                f\"encoding ({self.encoding})\"\n",
      "            )\n",
      "\n",
      "        try:\n",
      "            (\n",
      "                _,\n",
      "                self.machine,  # Architecture type.\n",
      "                _,\n",
      "                _,\n",
      "                self._e_phoff,  # Offset of program header.\n",
      "                _,\n",
      "                self.flags,  # Processor-specific flags.\n",
      "                _,\n",
      "                self._e_phentsize,  # Size of section.\n",
      "                self._e_phnum,  # Number of sections.\n",
      "            ) = self._read(e_fmt)\n",
      "        except struct.error as e:\n",
      "            raise ELFInvalid(\"unable to parse machine and section information\") from e\n",
      "\n",
      "    def _read(self, fmt: str) -> Tuple[int, ...]:\n",
      "        return struct.unpack(fmt, self._f.read(struct.calcsize(fmt)))\n",
      "\n",
      "    @property\n",
      "    def interpreter(self) -> Optional[str]:\n",
      "        \"\"\"\n",
      "        The path recorded in the ``PT_INTERP`` section header.\n",
      "        \"\"\"\n",
      "        for index in range(self._e_phnum):\n",
      "            self._f.seek(self._e_phoff + self._e_phentsize * index)\n",
      "            try:\n",
      "                data = self._read(self._p_fmt)\n",
      "            except struct.error:\n",
      "                continue\n",
      "            if data[self._p_idx[0]] != 3:  # Not PT_INTERP.\n",
      "                continue\n",
      "            self._f.seek(data[self._p_idx[1]])\n",
      "            return os.fsdecode(self._f.read(data[self._p_idx[2]])).strip(\"\\0\")\n",
      "        return None\n",
      "\n",
      "[end of src/wheel/vendored/packaging/_elffile.py]\n",
      "[start of src/wheel/vendored/packaging/__init__.py]\n",
      "\n",
      "[end of src/wheel/vendored/packaging/__init__.py]\n",
      "[start of src/wheel/vendored/packaging/tags.py]\n",
      "# This file is dual licensed under the terms of the Apache License, Version\n",
      "# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n",
      "# for complete details.\n",
      "\n",
      "import logging\n",
      "import platform\n",
      "import re\n",
      "import struct\n",
      "import subprocess\n",
      "import sys\n",
      "import sysconfig\n",
      "from importlib.machinery import EXTENSION_SUFFIXES\n",
      "from typing import (\n",
      "    Dict,\n",
      "    FrozenSet,\n",
      "    Iterable,\n",
      "    Iterator,\n",
      "    List,\n",
      "    Optional,\n",
      "    Sequence,\n",
      "    Tuple,\n",
      "    Union,\n",
      "    cast,\n",
      ")\n",
      "\n",
      "from . import _manylinux, _musllinux\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "PythonVersion = Sequence[int]\n",
      "MacVersion = Tuple[int, int]\n",
      "\n",
      "INTERPRETER_SHORT_NAMES: Dict[str, str] = {\n",
      "    \"python\": \"py\",  # Generic.\n",
      "    \"cpython\": \"cp\",\n",
      "    \"pypy\": \"pp\",\n",
      "    \"ironpython\": \"ip\",\n",
      "    \"jython\": \"jy\",\n",
      "}\n",
      "\n",
      "\n",
      "_32_BIT_INTERPRETER = struct.calcsize(\"P\") == 4\n",
      "\n",
      "\n",
      "class Tag:\n",
      "    \"\"\"\n",
      "    A representation of the tag triple for a wheel.\n",
      "\n",
      "    Instances are considered immutable and thus are hashable. Equality checking\n",
      "    is also supported.\n",
      "    \"\"\"\n",
      "\n",
      "    __slots__ = [\"_interpreter\", \"_abi\", \"_platform\", \"_hash\"]\n",
      "\n",
      "    def __init__(self, interpreter: str, abi: str, platform: str) -> None:\n",
      "        self._interpreter = interpreter.lower()\n",
      "        self._abi = abi.lower()\n",
      "        self._platform = platform.lower()\n",
      "        # The __hash__ of every single element in a Set[Tag] will be evaluated each time\n",
      "        # that a set calls its `.disjoint()` method, which may be called hundreds of\n",
      "        # times when scanning a page of links for packages with tags matching that\n",
      "        # Set[Tag]. Pre-computing the value here produces significant speedups for\n",
      "        # downstream consumers.\n",
      "        self._hash = hash((self._interpreter, self._abi, self._platform))\n",
      "\n",
      "    @property\n",
      "    def interpreter(self) -> str:\n",
      "        return self._interpreter\n",
      "\n",
      "    @property\n",
      "    def abi(self) -> str:\n",
      "        return self._abi\n",
      "\n",
      "    @property\n",
      "    def platform(self) -> str:\n",
      "        return self._platform\n",
      "\n",
      "    def __eq__(self, other: object) -> bool:\n",
      "        if not isinstance(other, Tag):\n",
      "            return NotImplemented\n",
      "\n",
      "        return (\n",
      "            (self._hash == other._hash)  # Short-circuit ASAP for perf reasons.\n",
      "            and (self._platform == other._platform)\n",
      "            and (self._abi == other._abi)\n",
      "            and (self._interpreter == other._interpreter)\n",
      "        )\n",
      "\n",
      "    def __hash__(self) -> int:\n",
      "        return self._hash\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        return f\"{self._interpreter}-{self._abi}-{self._platform}\"\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        return f\"<{self} @ {id(self)}>\"\n",
      "\n",
      "\n",
      "def parse_tag(tag: str) -> FrozenSet[Tag]:\n",
      "    \"\"\"\n",
      "    Parses the provided tag (e.g. `py3-none-any`) into a frozenset of Tag instances.\n",
      "\n",
      "    Returning a set is required due to the possibility that the tag is a\n",
      "    compressed tag set.\n",
      "    \"\"\"\n",
      "    tags = set()\n",
      "    interpreters, abis, platforms = tag.split(\"-\")\n",
      "    for interpreter in interpreters.split(\".\"):\n",
      "        for abi in abis.split(\".\"):\n",
      "            for platform_ in platforms.split(\".\"):\n",
      "                tags.add(Tag(interpreter, abi, platform_))\n",
      "    return frozenset(tags)\n",
      "\n",
      "\n",
      "def _get_config_var(name: str, warn: bool = False) -> Union[int, str, None]:\n",
      "    value: Union[int, str, None] = sysconfig.get_config_var(name)\n",
      "    if value is None and warn:\n",
      "        logger.debug(\n",
      "            \"Config variable '%s' is unset, Python ABI tag may be incorrect\", name\n",
      "        )\n",
      "    return value\n",
      "\n",
      "\n",
      "def _normalize_string(string: str) -> str:\n",
      "    return string.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")\n",
      "\n",
      "\n",
      "def _is_threaded_cpython(abis: List[str]) -> bool:\n",
      "    \"\"\"\n",
      "    Determine if the ABI corresponds to a threaded (`--disable-gil`) build.\n",
      "\n",
      "    The threaded builds are indicated by a \"t\" in the abiflags.\n",
      "    \"\"\"\n",
      "    if len(abis) == 0:\n",
      "        return False\n",
      "    # expect e.g., cp313\n",
      "    m = re.match(r\"cp\\d+(.*)\", abis[0])\n",
      "    if not m:\n",
      "        return False\n",
      "    abiflags = m.group(1)\n",
      "    return \"t\" in abiflags\n",
      "\n",
      "\n",
      "def _abi3_applies(python_version: PythonVersion, threading: bool) -> bool:\n",
      "    \"\"\"\n",
      "    Determine if the Python version supports abi3.\n",
      "\n",
      "    PEP 384 was first implemented in Python 3.2. The threaded (`--disable-gil`)\n",
      "    builds do not support abi3.\n",
      "    \"\"\"\n",
      "    return len(python_version) > 1 and tuple(python_version) >= (3, 2) and not threading\n",
      "\n",
      "\n",
      "def _cpython_abis(py_version: PythonVersion, warn: bool = False) -> List[str]:\n",
      "    py_version = tuple(py_version)  # To allow for version comparison.\n",
      "    abis = []\n",
      "    version = _version_nodot(py_version[:2])\n",
      "    threading = debug = pymalloc = ucs4 = \"\"\n",
      "    with_debug = _get_config_var(\"Py_DEBUG\", warn)\n",
      "    has_refcount = hasattr(sys, \"gettotalrefcount\")\n",
      "    # Windows doesn't set Py_DEBUG, so checking for support of debug-compiled\n",
      "    # extension modules is the best option.\n",
      "    # https://github.com/pypa/pip/issues/3383#issuecomment-173267692\n",
      "    has_ext = \"_d.pyd\" in EXTENSION_SUFFIXES\n",
      "    if with_debug or (with_debug is None and (has_refcount or has_ext)):\n",
      "        debug = \"d\"\n",
      "    if py_version >= (3, 13) and _get_config_var(\"Py_GIL_DISABLED\", warn):\n",
      "        threading = \"t\"\n",
      "    if py_version < (3, 8):\n",
      "        with_pymalloc = _get_config_var(\"WITH_PYMALLOC\", warn)\n",
      "        if with_pymalloc or with_pymalloc is None:\n",
      "            pymalloc = \"m\"\n",
      "        if py_version < (3, 3):\n",
      "            unicode_size = _get_config_var(\"Py_UNICODE_SIZE\", warn)\n",
      "            if unicode_size == 4 or (\n",
      "                unicode_size is None and sys.maxunicode == 0x10FFFF\n",
      "            ):\n",
      "                ucs4 = \"u\"\n",
      "    elif debug:\n",
      "        # Debug builds can also load \"normal\" extension modules.\n",
      "        # We can also assume no UCS-4 or pymalloc requirement.\n",
      "        abis.append(f\"cp{version}{threading}\")\n",
      "    abis.insert(0, f\"cp{version}{threading}{debug}{pymalloc}{ucs4}\")\n",
      "    return abis\n",
      "\n",
      "\n",
      "def cpython_tags(\n",
      "    python_version: Optional[PythonVersion] = None,\n",
      "    abis: Optional[Iterable[str]] = None,\n",
      "    platforms: Optional[Iterable[str]] = None,\n",
      "    *,\n",
      "    warn: bool = False,\n",
      ") -> Iterator[Tag]:\n",
      "    \"\"\"\n",
      "    Yields the tags for a CPython interpreter.\n",
      "\n",
      "    The tags consist of:\n",
      "    - cp<python_version>-<abi>-<platform>\n",
      "    - cp<python_version>-abi3-<platform>\n",
      "    - cp<python_version>-none-<platform>\n",
      "    - cp<less than python_version>-abi3-<platform>  # Older Python versions down to 3.2.\n",
      "\n",
      "    If python_version only specifies a major version then user-provided ABIs and\n",
      "    the 'none' ABItag will be used.\n",
      "\n",
      "    If 'abi3' or 'none' are specified in 'abis' then they will be yielded at\n",
      "    their normal position and not at the beginning.\n",
      "    \"\"\"\n",
      "    if not python_version:\n",
      "        python_version = sys.version_info[:2]\n",
      "\n",
      "    interpreter = f\"cp{_version_nodot(python_version[:2])}\"\n",
      "\n",
      "    if abis is None:\n",
      "        if len(python_version) > 1:\n",
      "            abis = _cpython_abis(python_version, warn)\n",
      "        else:\n",
      "            abis = []\n",
      "    abis = list(abis)\n",
      "    # 'abi3' and 'none' are explicitly handled later.\n",
      "    for explicit_abi in (\"abi3\", \"none\"):\n",
      "        try:\n",
      "            abis.remove(explicit_abi)\n",
      "        except ValueError:\n",
      "            pass\n",
      "\n",
      "    platforms = list(platforms or platform_tags())\n",
      "    for abi in abis:\n",
      "        for platform_ in platforms:\n",
      "            yield Tag(interpreter, abi, platform_)\n",
      "\n",
      "    threading = _is_threaded_cpython(abis)\n",
      "    use_abi3 = _abi3_applies(python_version, threading)\n",
      "    if use_abi3:\n",
      "        yield from (Tag(interpreter, \"abi3\", platform_) for platform_ in platforms)\n",
      "    yield from (Tag(interpreter, \"none\", platform_) for platform_ in platforms)\n",
      "\n",
      "    if use_abi3:\n",
      "        for minor_version in range(python_version[1] - 1, 1, -1):\n",
      "            for platform_ in platforms:\n",
      "                interpreter = \"cp{version}\".format(\n",
      "                    version=_version_nodot((python_version[0], minor_version))\n",
      "                )\n",
      "                yield Tag(interpreter, \"abi3\", platform_)\n",
      "\n",
      "\n",
      "def _generic_abi() -> List[str]:\n",
      "    \"\"\"\n",
      "    Return the ABI tag based on EXT_SUFFIX.\n",
      "    \"\"\"\n",
      "    # The following are examples of `EXT_SUFFIX`.\n",
      "    # We want to keep the parts which are related to the ABI and remove the\n",
      "    # parts which are related to the platform:\n",
      "    # - linux:   '.cpython-310-x86_64-linux-gnu.so' => cp310\n",
      "    # - mac:     '.cpython-310-darwin.so'           => cp310\n",
      "    # - win:     '.cp310-win_amd64.pyd'             => cp310\n",
      "    # - win:     '.pyd'                             => cp37 (uses _cpython_abis())\n",
      "    # - pypy:    '.pypy38-pp73-x86_64-linux-gnu.so' => pypy38_pp73\n",
      "    # - graalpy: '.graalpy-38-native-x86_64-darwin.dylib'\n",
      "    #                                               => graalpy_38_native\n",
      "\n",
      "    ext_suffix = _get_config_var(\"EXT_SUFFIX\", warn=True)\n",
      "    if not isinstance(ext_suffix, str) or ext_suffix[0] != \".\":\n",
      "        raise SystemError(\"invalid sysconfig.get_config_var('EXT_SUFFIX')\")\n",
      "    parts = ext_suffix.split(\".\")\n",
      "    if len(parts) < 3:\n",
      "        # CPython3.7 and earlier uses \".pyd\" on Windows.\n",
      "        return _cpython_abis(sys.version_info[:2])\n",
      "    soabi = parts[1]\n",
      "    if soabi.startswith(\"cpython\"):\n",
      "        # non-windows\n",
      "        abi = \"cp\" + soabi.split(\"-\")[1]\n",
      "    elif soabi.startswith(\"cp\"):\n",
      "        # windows\n",
      "        abi = soabi.split(\"-\")[0]\n",
      "    elif soabi.startswith(\"pypy\"):\n",
      "        abi = \"-\".join(soabi.split(\"-\")[:2])\n",
      "    elif soabi.startswith(\"graalpy\"):\n",
      "        abi = \"-\".join(soabi.split(\"-\")[:3])\n",
      "    elif soabi:\n",
      "        # pyston, ironpython, others?\n",
      "        abi = soabi\n",
      "    else:\n",
      "        return []\n",
      "    return [_normalize_string(abi)]\n",
      "\n",
      "\n",
      "def generic_tags(\n",
      "    interpreter: Optional[str] = None,\n",
      "    abis: Optional[Iterable[str]] = None,\n",
      "    platforms: Optional[Iterable[str]] = None,\n",
      "    *,\n",
      "    warn: bool = False,\n",
      ") -> Iterator[Tag]:\n",
      "    \"\"\"\n",
      "    Yields the tags for a generic interpreter.\n",
      "\n",
      "    The tags consist of:\n",
      "    - <interpreter>-<abi>-<platform>\n",
      "\n",
      "    The \"none\" ABI will be added if it was not explicitly provided.\n",
      "    \"\"\"\n",
      "    if not interpreter:\n",
      "        interp_name = interpreter_name()\n",
      "        interp_version = interpreter_version(warn=warn)\n",
      "        interpreter = \"\".join([interp_name, interp_version])\n",
      "    if abis is None:\n",
      "        abis = _generic_abi()\n",
      "    else:\n",
      "        abis = list(abis)\n",
      "    platforms = list(platforms or platform_tags())\n",
      "    if \"none\" not in abis:\n",
      "        abis.append(\"none\")\n",
      "    for abi in abis:\n",
      "        for platform_ in platforms:\n",
      "            yield Tag(interpreter, abi, platform_)\n",
      "\n",
      "\n",
      "def _py_interpreter_range(py_version: PythonVersion) -> Iterator[str]:\n",
      "    \"\"\"\n",
      "    Yields Python versions in descending order.\n",
      "\n",
      "    After the latest version, the major-only version will be yielded, and then\n",
      "    all previous versions of that major version.\n",
      "    \"\"\"\n",
      "    if len(py_version) > 1:\n",
      "        yield f\"py{_version_nodot(py_version[:2])}\"\n",
      "    yield f\"py{py_version[0]}\"\n",
      "    if len(py_version) > 1:\n",
      "        for minor in range(py_version[1] - 1, -1, -1):\n",
      "            yield f\"py{_version_nodot((py_version[0], minor))}\"\n",
      "\n",
      "\n",
      "def compatible_tags(\n",
      "    python_version: Optional[PythonVersion] = None,\n",
      "    interpreter: Optional[str] = None,\n",
      "    platforms: Optional[Iterable[str]] = None,\n",
      ") -> Iterator[Tag]:\n",
      "    \"\"\"\n",
      "    Yields the sequence of tags that are compatible with a specific version of Python.\n",
      "\n",
      "    The tags consist of:\n",
      "    - py*-none-<platform>\n",
      "    - <interpreter>-none-any  # ... if `interpreter` is provided.\n",
      "    - py*-none-any\n",
      "    \"\"\"\n",
      "    if not python_version:\n",
      "        python_version = sys.version_info[:2]\n",
      "    platforms = list(platforms or platform_tags())\n",
      "    for version in _py_interpreter_range(python_version):\n",
      "        for platform_ in platforms:\n",
      "            yield Tag(version, \"none\", platform_)\n",
      "    if interpreter:\n",
      "        yield Tag(interpreter, \"none\", \"any\")\n",
      "    for version in _py_interpreter_range(python_version):\n",
      "        yield Tag(version, \"none\", \"any\")\n",
      "\n",
      "\n",
      "def _mac_arch(arch: str, is_32bit: bool = _32_BIT_INTERPRETER) -> str:\n",
      "    if not is_32bit:\n",
      "        return arch\n",
      "\n",
      "    if arch.startswith(\"ppc\"):\n",
      "        return \"ppc\"\n",
      "\n",
      "    return \"i386\"\n",
      "\n",
      "\n",
      "def _mac_binary_formats(version: MacVersion, cpu_arch: str) -> List[str]:\n",
      "    formats = [cpu_arch]\n",
      "    if cpu_arch == \"x86_64\":\n",
      "        if version < (10, 4):\n",
      "            return []\n",
      "        formats.extend([\"intel\", \"fat64\", \"fat32\"])\n",
      "\n",
      "    elif cpu_arch == \"i386\":\n",
      "        if version < (10, 4):\n",
      "            return []\n",
      "        formats.extend([\"intel\", \"fat32\", \"fat\"])\n",
      "\n",
      "    elif cpu_arch == \"ppc64\":\n",
      "        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?\n",
      "        if version > (10, 5) or version < (10, 4):\n",
      "            return []\n",
      "        formats.append(\"fat64\")\n",
      "\n",
      "    elif cpu_arch == \"ppc\":\n",
      "        if version > (10, 6):\n",
      "            return []\n",
      "        formats.extend([\"fat32\", \"fat\"])\n",
      "\n",
      "    if cpu_arch in {\"arm64\", \"x86_64\"}:\n",
      "        formats.append(\"universal2\")\n",
      "\n",
      "    if cpu_arch in {\"x86_64\", \"i386\", \"ppc64\", \"ppc\", \"intel\"}:\n",
      "        formats.append(\"universal\")\n",
      "\n",
      "    return formats\n",
      "\n",
      "\n",
      "def mac_platforms(\n",
      "    version: Optional[MacVersion] = None, arch: Optional[str] = None\n",
      ") -> Iterator[str]:\n",
      "    \"\"\"\n",
      "    Yields the platform tags for a macOS system.\n",
      "\n",
      "    The `version` parameter is a two-item tuple specifying the macOS version to\n",
      "    generate platform tags for. The `arch` parameter is the CPU architecture to\n",
      "    generate platform tags for. Both parameters default to the appropriate value\n",
      "    for the current system.\n",
      "    \"\"\"\n",
      "    version_str, _, cpu_arch = platform.mac_ver()\n",
      "    if version is None:\n",
      "        version = cast(\"MacVersion\", tuple(map(int, version_str.split(\".\")[:2])))\n",
      "        if version == (10, 16):\n",
      "            # When built against an older macOS SDK, Python will report macOS 10.16\n",
      "            # instead of the real version.\n",
      "            version_str = subprocess.run(\n",
      "                [\n",
      "                    sys.executable,\n",
      "                    \"-sS\",\n",
      "                    \"-c\",\n",
      "                    \"import platform; print(platform.mac_ver()[0])\",\n",
      "                ],\n",
      "                check=True,\n",
      "                env={\"SYSTEM_VERSION_COMPAT\": \"0\"},\n",
      "                stdout=subprocess.PIPE,\n",
      "                text=True,\n",
      "            ).stdout\n",
      "            version = cast(\"MacVersion\", tuple(map(int, version_str.split(\".\")[:2])))\n",
      "    else:\n",
      "        version = version\n",
      "    if arch is None:\n",
      "        arch = _mac_arch(cpu_arch)\n",
      "    else:\n",
      "        arch = arch\n",
      "\n",
      "    if (10, 0) <= version and version < (11, 0):\n",
      "        # Prior to Mac OS 11, each yearly release of Mac OS bumped the\n",
      "        # \"minor\" version number.  The major version was always 10.\n",
      "        for minor_version in range(version[1], -1, -1):\n",
      "            compat_version = 10, minor_version\n",
      "            binary_formats = _mac_binary_formats(compat_version, arch)\n",
      "            for binary_format in binary_formats:\n",
      "                yield \"macosx_{major}_{minor}_{binary_format}\".format(\n",
      "                    major=10, minor=minor_version, binary_format=binary_format\n",
      "                )\n",
      "\n",
      "    if version >= (11, 0):\n",
      "        # Starting with Mac OS 11, each yearly release bumps the major version\n",
      "        # number.   The minor versions are now the midyear updates.\n",
      "        for major_version in range(version[0], 10, -1):\n",
      "            compat_version = major_version, 0\n",
      "            binary_formats = _mac_binary_formats(compat_version, arch)\n",
      "            for binary_format in binary_formats:\n",
      "                yield \"macosx_{major}_{minor}_{binary_format}\".format(\n",
      "                    major=major_version, minor=0, binary_format=binary_format\n",
      "                )\n",
      "\n",
      "    if version >= (11, 0):\n",
      "        # Mac OS 11 on x86_64 is compatible with binaries from previous releases.\n",
      "        # Arm64 support was introduced in 11.0, so no Arm binaries from previous\n",
      "        # releases exist.\n",
      "        #\n",
      "        # However, the \"universal2\" binary format can have a\n",
      "        # macOS version earlier than 11.0 when the x86_64 part of the binary supports\n",
      "        # that version of macOS.\n",
      "        if arch == \"x86_64\":\n",
      "            for minor_version in range(16, 3, -1):\n",
      "                compat_version = 10, minor_version\n",
      "                binary_formats = _mac_binary_formats(compat_version, arch)\n",
      "                for binary_format in binary_formats:\n",
      "                    yield \"macosx_{major}_{minor}_{binary_format}\".format(\n",
      "                        major=compat_version[0],\n",
      "                        minor=compat_version[1],\n",
      "                        binary_format=binary_format,\n",
      "                    )\n",
      "        else:\n",
      "            for minor_version in range(16, 3, -1):\n",
      "                compat_version = 10, minor_version\n",
      "                binary_format = \"universal2\"\n",
      "                yield \"macosx_{major}_{minor}_{binary_format}\".format(\n",
      "                    major=compat_version[0],\n",
      "                    minor=compat_version[1],\n",
      "                    binary_format=binary_format,\n",
      "                )\n",
      "\n",
      "\n",
      "def _linux_platforms(is_32bit: bool = _32_BIT_INTERPRETER) -> Iterator[str]:\n",
      "    linux = _normalize_string(sysconfig.get_platform())\n",
      "    if not linux.startswith(\"linux_\"):\n",
      "        # we should never be here, just yield the sysconfig one and return\n",
      "        yield linux\n",
      "        return\n",
      "    if is_32bit:\n",
      "        if linux == \"linux_x86_64\":\n",
      "            linux = \"linux_i686\"\n",
      "        elif linux == \"linux_aarch64\":\n",
      "            linux = \"linux_armv8l\"\n",
      "    _, arch = linux.split(\"_\", 1)\n",
      "    archs = {\"armv8l\": [\"armv8l\", \"armv7l\"]}.get(arch, [arch])\n",
      "    yield from _manylinux.platform_tags(archs)\n",
      "    yield from _musllinux.platform_tags(archs)\n",
      "    for arch in archs:\n",
      "        yield f\"linux_{arch}\"\n",
      "\n",
      "\n",
      "def _generic_platforms() -> Iterator[str]:\n",
      "    yield _normalize_string(sysconfig.get_platform())\n",
      "\n",
      "\n",
      "def platform_tags() -> Iterator[str]:\n",
      "    \"\"\"\n",
      "    Provides the platform tags for this installation.\n",
      "    \"\"\"\n",
      "    if platform.system() == \"Darwin\":\n",
      "        return mac_platforms()\n",
      "    elif platform.system() == \"Linux\":\n",
      "        return _linux_platforms()\n",
      "    else:\n",
      "        return _generic_platforms()\n",
      "\n",
      "\n",
      "def interpreter_name() -> str:\n",
      "    \"\"\"\n",
      "    Returns the name of the running interpreter.\n",
      "\n",
      "    Some implementations have a reserved, two-letter abbreviation which will\n",
      "    be returned when appropriate.\n",
      "    \"\"\"\n",
      "    name = sys.implementation.name\n",
      "    return INTERPRETER_SHORT_NAMES.get(name) or name\n",
      "\n",
      "\n",
      "def interpreter_version(*, warn: bool = False) -> str:\n",
      "    \"\"\"\n",
      "    Returns the version of the running interpreter.\n",
      "    \"\"\"\n",
      "    version = _get_config_var(\"py_version_nodot\", warn=warn)\n",
      "    if version:\n",
      "        version = str(version)\n",
      "    else:\n",
      "        version = _version_nodot(sys.version_info[:2])\n",
      "    return version\n",
      "\n",
      "\n",
      "def _version_nodot(version: PythonVersion) -> str:\n",
      "    return \"\".join(map(str, version))\n",
      "\n",
      "\n",
      "def sys_tags(*, warn: bool = False) -> Iterator[Tag]:\n",
      "    \"\"\"\n",
      "    Returns the sequence of tag triples for the running interpreter.\n",
      "\n",
      "    The order of the sequence corresponds to priority order for the\n",
      "    interpreter, from most to least important.\n",
      "    \"\"\"\n",
      "\n",
      "    interp_name = interpreter_name()\n",
      "    if interp_name == \"cp\":\n",
      "        yield from cpython_tags(warn=warn)\n",
      "    else:\n",
      "        yield from generic_tags()\n",
      "\n",
      "    if interp_name == \"pp\":\n",
      "        interp = \"pp3\"\n",
      "    elif interp_name == \"cp\":\n",
      "        interp = \"cp\" + interpreter_version(warn=warn)\n",
      "    else:\n",
      "        interp = None\n",
      "    yield from compatible_tags(interpreter=interp)\n",
      "\n",
      "[end of src/wheel/vendored/packaging/tags.py]\n",
      "[start of src/wheel/vendored/packaging/_tokenizer.py]\n",
      "import contextlib\n",
      "import re\n",
      "from dataclasses import dataclass\n",
      "from typing import Dict, Iterator, NoReturn, Optional, Tuple, Union\n",
      "\n",
      "from .specifiers import Specifier\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class Token:\n",
      "    name: str\n",
      "    text: str\n",
      "    position: int\n",
      "\n",
      "\n",
      "class ParserSyntaxError(Exception):\n",
      "    \"\"\"The provided source text could not be parsed correctly.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        message: str,\n",
      "        *,\n",
      "        source: str,\n",
      "        span: Tuple[int, int],\n",
      "    ) -> None:\n",
      "        self.span = span\n",
      "        self.message = message\n",
      "        self.source = source\n",
      "\n",
      "        super().__init__()\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        marker = \" \" * self.span[0] + \"~\" * (self.span[1] - self.span[0]) + \"^\"\n",
      "        return \"\\n    \".join([self.message, self.source, marker])\n",
      "\n",
      "\n",
      "DEFAULT_RULES: \"Dict[str, Union[str, re.Pattern[str]]]\" = {\n",
      "    \"LEFT_PARENTHESIS\": r\"\\(\",\n",
      "    \"RIGHT_PARENTHESIS\": r\"\\)\",\n",
      "    \"LEFT_BRACKET\": r\"\\[\",\n",
      "    \"RIGHT_BRACKET\": r\"\\]\",\n",
      "    \"SEMICOLON\": r\";\",\n",
      "    \"COMMA\": r\",\",\n",
      "    \"QUOTED_STRING\": re.compile(\n",
      "        r\"\"\"\n",
      "            (\n",
      "                ('[^']*')\n",
      "                |\n",
      "                (\"[^\"]*\")\n",
      "            )\n",
      "        \"\"\",\n",
      "        re.VERBOSE,\n",
      "    ),\n",
      "    \"OP\": r\"(===|==|~=|!=|<=|>=|<|>)\",\n",
      "    \"BOOLOP\": r\"\\b(or|and)\\b\",\n",
      "    \"IN\": r\"\\bin\\b\",\n",
      "    \"NOT\": r\"\\bnot\\b\",\n",
      "    \"VARIABLE\": re.compile(\n",
      "        r\"\"\"\n",
      "            \\b(\n",
      "                python_version\n",
      "                |python_full_version\n",
      "                |os[._]name\n",
      "                |sys[._]platform\n",
      "                |platform_(release|system)\n",
      "                |platform[._](version|machine|python_implementation)\n",
      "                |python_implementation\n",
      "                |implementation_(name|version)\n",
      "                |extra\n",
      "            )\\b\n",
      "        \"\"\",\n",
      "        re.VERBOSE,\n",
      "    ),\n",
      "    \"SPECIFIER\": re.compile(\n",
      "        Specifier._operator_regex_str + Specifier._version_regex_str,\n",
      "        re.VERBOSE | re.IGNORECASE,\n",
      "    ),\n",
      "    \"AT\": r\"\\@\",\n",
      "    \"URL\": r\"[^ \\t]+\",\n",
      "    \"IDENTIFIER\": r\"\\b[a-zA-Z0-9][a-zA-Z0-9._-]*\\b\",\n",
      "    \"VERSION_PREFIX_TRAIL\": r\"\\.\\*\",\n",
      "    \"VERSION_LOCAL_LABEL_TRAIL\": r\"\\+[a-z0-9]+(?:[-_\\.][a-z0-9]+)*\",\n",
      "    \"WS\": r\"[ \\t]+\",\n",
      "    \"END\": r\"$\",\n",
      "}\n",
      "\n",
      "\n",
      "class Tokenizer:\n",
      "    \"\"\"Context-sensitive token parsing.\n",
      "\n",
      "    Provides methods to examine the input stream to check whether the next token\n",
      "    matches.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        source: str,\n",
      "        *,\n",
      "        rules: \"Dict[str, Union[str, re.Pattern[str]]]\",\n",
      "    ) -> None:\n",
      "        self.source = source\n",
      "        self.rules: Dict[str, re.Pattern[str]] = {\n",
      "            name: re.compile(pattern) for name, pattern in rules.items()\n",
      "        }\n",
      "        self.next_token: Optional[Token] = None\n",
      "        self.position = 0\n",
      "\n",
      "    def consume(self, name: str) -> None:\n",
      "        \"\"\"Move beyond provided token name, if at current position.\"\"\"\n",
      "        if self.check(name):\n",
      "            self.read()\n",
      "\n",
      "    def check(self, name: str, *, peek: bool = False) -> bool:\n",
      "        \"\"\"Check whether the next token has the provided name.\n",
      "\n",
      "        By default, if the check succeeds, the token *must* be read before\n",
      "        another check. If `peek` is set to `True`, the token is not loaded and\n",
      "        would need to be checked again.\n",
      "        \"\"\"\n",
      "        assert (\n",
      "            self.next_token is None\n",
      "        ), f\"Cannot check for {name!r}, already have {self.next_token!r}\"\n",
      "        assert name in self.rules, f\"Unknown token name: {name!r}\"\n",
      "\n",
      "        expression = self.rules[name]\n",
      "\n",
      "        match = expression.match(self.source, self.position)\n",
      "        if match is None:\n",
      "            return False\n",
      "        if not peek:\n",
      "            self.next_token = Token(name, match[0], self.position)\n",
      "        return True\n",
      "\n",
      "    def expect(self, name: str, *, expected: str) -> Token:\n",
      "        \"\"\"Expect a certain token name next, failing with a syntax error otherwise.\n",
      "\n",
      "        The token is *not* read.\n",
      "        \"\"\"\n",
      "        if not self.check(name):\n",
      "            raise self.raise_syntax_error(f\"Expected {expected}\")\n",
      "        return self.read()\n",
      "\n",
      "    def read(self) -> Token:\n",
      "        \"\"\"Consume the next token and return it.\"\"\"\n",
      "        token = self.next_token\n",
      "        assert token is not None\n",
      "\n",
      "        self.position += len(token.text)\n",
      "        self.next_token = None\n",
      "\n",
      "        return token\n",
      "\n",
      "    def raise_syntax_error(\n",
      "        self,\n",
      "        message: str,\n",
      "        *,\n",
      "        span_start: Optional[int] = None,\n",
      "        span_end: Optional[int] = None,\n",
      "    ) -> NoReturn:\n",
      "        \"\"\"Raise ParserSyntaxError at the given position.\"\"\"\n",
      "        span = (\n",
      "            self.position if span_start is None else span_start,\n",
      "            self.position if span_end is None else span_end,\n",
      "        )\n",
      "        raise ParserSyntaxError(\n",
      "            message,\n",
      "            source=self.source,\n",
      "            span=span,\n",
      "        )\n",
      "\n",
      "    @contextlib.contextmanager\n",
      "    def enclosing_tokens(\n",
      "        self, open_token: str, close_token: str, *, around: str\n",
      "    ) -> Iterator[None]:\n",
      "        if self.check(open_token):\n",
      "            open_position = self.position\n",
      "            self.read()\n",
      "        else:\n",
      "            open_position = None\n",
      "\n",
      "        yield\n",
      "\n",
      "        if open_position is None:\n",
      "            return\n",
      "\n",
      "        if not self.check(close_token):\n",
      "            self.raise_syntax_error(\n",
      "                f\"Expected matching {close_token} for {open_token}, after {around}\",\n",
      "                span_start=open_position,\n",
      "            )\n",
      "\n",
      "        self.read()\n",
      "\n",
      "[end of src/wheel/vendored/packaging/_tokenizer.py]\n",
      "[start of src/wheel/vendored/packaging/specifiers.py]\n",
      "# This file is dual licensed under the terms of the Apache License, Version\n",
      "# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n",
      "# for complete details.\n",
      "\"\"\"\n",
      ".. testsetup::\n",
      "\n",
      "    from packaging.specifiers import Specifier, SpecifierSet, InvalidSpecifier\n",
      "    from packaging.version import Version\n",
      "\"\"\"\n",
      "\n",
      "import abc\n",
      "import itertools\n",
      "import re\n",
      "from typing import Callable, Iterable, Iterator, List, Optional, Tuple, TypeVar, Union\n",
      "\n",
      "from .utils import canonicalize_version\n",
      "from .version import Version\n",
      "\n",
      "UnparsedVersion = Union[Version, str]\n",
      "UnparsedVersionVar = TypeVar(\"UnparsedVersionVar\", bound=UnparsedVersion)\n",
      "CallableOperator = Callable[[Version, str], bool]\n",
      "\n",
      "\n",
      "def _coerce_version(version: UnparsedVersion) -> Version:\n",
      "    if not isinstance(version, Version):\n",
      "        version = Version(version)\n",
      "    return version\n",
      "\n",
      "\n",
      "class InvalidSpecifier(ValueError):\n",
      "    \"\"\"\n",
      "    Raised when attempting to create a :class:`Specifier` with a specifier\n",
      "    string that is invalid.\n",
      "\n",
      "    >>> Specifier(\"lolwat\")\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    packaging.specifiers.InvalidSpecifier: Invalid specifier: 'lolwat'\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "class BaseSpecifier(metaclass=abc.ABCMeta):\n",
      "    @abc.abstractmethod\n",
      "    def __str__(self) -> str:\n",
      "        \"\"\"\n",
      "        Returns the str representation of this Specifier-like object. This\n",
      "        should be representative of the Specifier itself.\n",
      "        \"\"\"\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def __hash__(self) -> int:\n",
      "        \"\"\"\n",
      "        Returns a hash value for this Specifier-like object.\n",
      "        \"\"\"\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def __eq__(self, other: object) -> bool:\n",
      "        \"\"\"\n",
      "        Returns a boolean representing whether or not the two Specifier-like\n",
      "        objects are equal.\n",
      "\n",
      "        :param other: The other object to check against.\n",
      "        \"\"\"\n",
      "\n",
      "    @property\n",
      "    @abc.abstractmethod\n",
      "    def prereleases(self) -> Optional[bool]:\n",
      "        \"\"\"Whether or not pre-releases as a whole are allowed.\n",
      "\n",
      "        This can be set to either ``True`` or ``False`` to explicitly enable or disable\n",
      "        prereleases or it can be set to ``None`` (the default) to use default semantics.\n",
      "        \"\"\"\n",
      "\n",
      "    @prereleases.setter\n",
      "    def prereleases(self, value: bool) -> None:\n",
      "        \"\"\"Setter for :attr:`prereleases`.\n",
      "\n",
      "        :param value: The value to set.\n",
      "        \"\"\"\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def contains(self, item: str, prereleases: Optional[bool] = None) -> bool:\n",
      "        \"\"\"\n",
      "        Determines if the given item is contained within this specifier.\n",
      "        \"\"\"\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def filter(\n",
      "        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None\n",
      "    ) -> Iterator[UnparsedVersionVar]:\n",
      "        \"\"\"\n",
      "        Takes an iterable of items and filters them so that only items which\n",
      "        are contained within this specifier are allowed in it.\n",
      "        \"\"\"\n",
      "\n",
      "\n",
      "class Specifier(BaseSpecifier):\n",
      "    \"\"\"This class abstracts handling of version specifiers.\n",
      "\n",
      "    .. tip::\n",
      "\n",
      "        It is generally not required to instantiate this manually. You should instead\n",
      "        prefer to work with :class:`SpecifierSet` instead, which can parse\n",
      "        comma-separated version specifiers (which is what package metadata contains).\n",
      "    \"\"\"\n",
      "\n",
      "    _operator_regex_str = r\"\"\"\n",
      "        (?P<operator>(~=|==|!=|<=|>=|<|>|===))\n",
      "        \"\"\"\n",
      "    _version_regex_str = r\"\"\"\n",
      "        (?P<version>\n",
      "            (?:\n",
      "                # The identity operators allow for an escape hatch that will\n",
      "                # do an exact string match of the version you wish to install.\n",
      "                # This will not be parsed by PEP 440 and we cannot determine\n",
      "                # any semantic meaning from it. This operator is discouraged\n",
      "                # but included entirely as an escape hatch.\n",
      "                (?<====)  # Only match for the identity operator\n",
      "                \\s*\n",
      "                [^\\s;)]*  # The arbitrary version can be just about anything,\n",
      "                          # we match everything except for whitespace, a\n",
      "                          # semi-colon for marker support, and a closing paren\n",
      "                          # since versions can be enclosed in them.\n",
      "            )\n",
      "            |\n",
      "            (?:\n",
      "                # The (non)equality operators allow for wild card and local\n",
      "                # versions to be specified so we have to define these two\n",
      "                # operators separately to enable that.\n",
      "                (?<===|!=)            # Only match for equals and not equals\n",
      "\n",
      "                \\s*\n",
      "                v?\n",
      "                (?:[0-9]+!)?          # epoch\n",
      "                [0-9]+(?:\\.[0-9]+)*   # release\n",
      "\n",
      "                # You cannot use a wild card and a pre-release, post-release, a dev or\n",
      "                # local version together so group them with a | and make them optional.\n",
      "                (?:\n",
      "                    \\.\\*  # Wild card syntax of .*\n",
      "                    |\n",
      "                    (?:                                  # pre release\n",
      "                        [-_\\.]?\n",
      "                        (alpha|beta|preview|pre|a|b|c|rc)\n",
      "                        [-_\\.]?\n",
      "                        [0-9]*\n",
      "                    )?\n",
      "                    (?:                                  # post release\n",
      "                        (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n",
      "                    )?\n",
      "                    (?:[-_\\.]?dev[-_\\.]?[0-9]*)?         # dev release\n",
      "                    (?:\\+[a-z0-9]+(?:[-_\\.][a-z0-9]+)*)? # local\n",
      "                )?\n",
      "            )\n",
      "            |\n",
      "            (?:\n",
      "                # The compatible operator requires at least two digits in the\n",
      "                # release segment.\n",
      "                (?<=~=)               # Only match for the compatible operator\n",
      "\n",
      "                \\s*\n",
      "                v?\n",
      "                (?:[0-9]+!)?          # epoch\n",
      "                [0-9]+(?:\\.[0-9]+)+   # release  (We have a + instead of a *)\n",
      "                (?:                   # pre release\n",
      "                    [-_\\.]?\n",
      "                    (alpha|beta|preview|pre|a|b|c|rc)\n",
      "                    [-_\\.]?\n",
      "                    [0-9]*\n",
      "                )?\n",
      "                (?:                                   # post release\n",
      "                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n",
      "                )?\n",
      "                (?:[-_\\.]?dev[-_\\.]?[0-9]*)?          # dev release\n",
      "            )\n",
      "            |\n",
      "            (?:\n",
      "                # All other operators only allow a sub set of what the\n",
      "                # (non)equality operators do. Specifically they do not allow\n",
      "                # local versions to be specified nor do they allow the prefix\n",
      "                # matching wild cards.\n",
      "                (?<!==|!=|~=)         # We have special cases for these\n",
      "                                      # operators so we want to make sure they\n",
      "                                      # don't match here.\n",
      "\n",
      "                \\s*\n",
      "                v?\n",
      "                (?:[0-9]+!)?          # epoch\n",
      "                [0-9]+(?:\\.[0-9]+)*   # release\n",
      "                (?:                   # pre release\n",
      "                    [-_\\.]?\n",
      "                    (alpha|beta|preview|pre|a|b|c|rc)\n",
      "                    [-_\\.]?\n",
      "                    [0-9]*\n",
      "                )?\n",
      "                (?:                                   # post release\n",
      "                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n",
      "                )?\n",
      "                (?:[-_\\.]?dev[-_\\.]?[0-9]*)?          # dev release\n",
      "            )\n",
      "        )\n",
      "        \"\"\"\n",
      "\n",
      "    _regex = re.compile(\n",
      "        r\"^\\s*\" + _operator_regex_str + _version_regex_str + r\"\\s*$\",\n",
      "        re.VERBOSE | re.IGNORECASE,\n",
      "    )\n",
      "\n",
      "    _operators = {\n",
      "        \"~=\": \"compatible\",\n",
      "        \"==\": \"equal\",\n",
      "        \"!=\": \"not_equal\",\n",
      "        \"<=\": \"less_than_equal\",\n",
      "        \">=\": \"greater_than_equal\",\n",
      "        \"<\": \"less_than\",\n",
      "        \">\": \"greater_than\",\n",
      "        \"===\": \"arbitrary\",\n",
      "    }\n",
      "\n",
      "    def __init__(self, spec: str = \"\", prereleases: Optional[bool] = None) -> None:\n",
      "        \"\"\"Initialize a Specifier instance.\n",
      "\n",
      "        :param spec:\n",
      "            The string representation of a specifier which will be parsed and\n",
      "            normalized before use.\n",
      "        :param prereleases:\n",
      "            This tells the specifier if it should accept prerelease versions if\n",
      "            applicable or not. The default of ``None`` will autodetect it from the\n",
      "            given specifiers.\n",
      "        :raises InvalidSpecifier:\n",
      "            If the given specifier is invalid (i.e. bad syntax).\n",
      "        \"\"\"\n",
      "        match = self._regex.search(spec)\n",
      "        if not match:\n",
      "            raise InvalidSpecifier(f\"Invalid specifier: '{spec}'\")\n",
      "\n",
      "        self._spec: Tuple[str, str] = (\n",
      "            match.group(\"operator\").strip(),\n",
      "            match.group(\"version\").strip(),\n",
      "        )\n",
      "\n",
      "        # Store whether or not this Specifier should accept prereleases\n",
      "        self._prereleases = prereleases\n",
      "\n",
      "    # https://github.com/python/mypy/pull/13475#pullrequestreview-1079784515\n",
      "    @property  # type: ignore[override]\n",
      "    def prereleases(self) -> bool:\n",
      "        # If there is an explicit prereleases set for this, then we'll just\n",
      "        # blindly use that.\n",
      "        if self._prereleases is not None:\n",
      "            return self._prereleases\n",
      "\n",
      "        # Look at all of our specifiers and determine if they are inclusive\n",
      "        # operators, and if they are if they are including an explicit\n",
      "        # prerelease.\n",
      "        operator, version = self._spec\n",
      "        if operator in [\"==\", \">=\", \"<=\", \"~=\", \"===\"]:\n",
      "            # The == specifier can include a trailing .*, if it does we\n",
      "            # want to remove before parsing.\n",
      "            if operator == \"==\" and version.endswith(\".*\"):\n",
      "                version = version[:-2]\n",
      "\n",
      "            # Parse the version, and if it is a pre-release than this\n",
      "            # specifier allows pre-releases.\n",
      "            if Version(version).is_prerelease:\n",
      "                return True\n",
      "\n",
      "        return False\n",
      "\n",
      "    @prereleases.setter\n",
      "    def prereleases(self, value: bool) -> None:\n",
      "        self._prereleases = value\n",
      "\n",
      "    @property\n",
      "    def operator(self) -> str:\n",
      "        \"\"\"The operator of this specifier.\n",
      "\n",
      "        >>> Specifier(\"==1.2.3\").operator\n",
      "        '=='\n",
      "        \"\"\"\n",
      "        return self._spec[0]\n",
      "\n",
      "    @property\n",
      "    def version(self) -> str:\n",
      "        \"\"\"The version of this specifier.\n",
      "\n",
      "        >>> Specifier(\"==1.2.3\").version\n",
      "        '1.2.3'\n",
      "        \"\"\"\n",
      "        return self._spec[1]\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        \"\"\"A representation of the Specifier that shows all internal state.\n",
      "\n",
      "        >>> Specifier('>=1.0.0')\n",
      "        <Specifier('>=1.0.0')>\n",
      "        >>> Specifier('>=1.0.0', prereleases=False)\n",
      "        <Specifier('>=1.0.0', prereleases=False)>\n",
      "        >>> Specifier('>=1.0.0', prereleases=True)\n",
      "        <Specifier('>=1.0.0', prereleases=True)>\n",
      "        \"\"\"\n",
      "        pre = (\n",
      "            f\", prereleases={self.prereleases!r}\"\n",
      "            if self._prereleases is not None\n",
      "            else \"\"\n",
      "        )\n",
      "\n",
      "        return f\"<{self.__class__.__name__}({str(self)!r}{pre})>\"\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        \"\"\"A string representation of the Specifier that can be round-tripped.\n",
      "\n",
      "        >>> str(Specifier('>=1.0.0'))\n",
      "        '>=1.0.0'\n",
      "        >>> str(Specifier('>=1.0.0', prereleases=False))\n",
      "        '>=1.0.0'\n",
      "        \"\"\"\n",
      "        return \"{}{}\".format(*self._spec)\n",
      "\n",
      "    @property\n",
      "    def _canonical_spec(self) -> Tuple[str, str]:\n",
      "        canonical_version = canonicalize_version(\n",
      "            self._spec[1],\n",
      "            strip_trailing_zero=(self._spec[0] != \"~=\"),\n",
      "        )\n",
      "        return self._spec[0], canonical_version\n",
      "\n",
      "    def __hash__(self) -> int:\n",
      "        return hash(self._canonical_spec)\n",
      "\n",
      "    def __eq__(self, other: object) -> bool:\n",
      "        \"\"\"Whether or not the two Specifier-like objects are equal.\n",
      "\n",
      "        :param other: The other object to check against.\n",
      "\n",
      "        The value of :attr:`prereleases` is ignored.\n",
      "\n",
      "        >>> Specifier(\"==1.2.3\") == Specifier(\"== 1.2.3.0\")\n",
      "        True\n",
      "        >>> (Specifier(\"==1.2.3\", prereleases=False) ==\n",
      "        ...  Specifier(\"==1.2.3\", prereleases=True))\n",
      "        True\n",
      "        >>> Specifier(\"==1.2.3\") == \"==1.2.3\"\n",
      "        True\n",
      "        >>> Specifier(\"==1.2.3\") == Specifier(\"==1.2.4\")\n",
      "        False\n",
      "        >>> Specifier(\"==1.2.3\") == Specifier(\"~=1.2.3\")\n",
      "        False\n",
      "        \"\"\"\n",
      "        if isinstance(other, str):\n",
      "            try:\n",
      "                other = self.__class__(str(other))\n",
      "            except InvalidSpecifier:\n",
      "                return NotImplemented\n",
      "        elif not isinstance(other, self.__class__):\n",
      "            return NotImplemented\n",
      "\n",
      "        return self._canonical_spec == other._canonical_spec\n",
      "\n",
      "    def _get_operator(self, op: str) -> CallableOperator:\n",
      "        operator_callable: CallableOperator = getattr(\n",
      "            self, f\"_compare_{self._operators[op]}\"\n",
      "        )\n",
      "        return operator_callable\n",
      "\n",
      "    def _compare_compatible(self, prospective: Version, spec: str) -> bool:\n",
      "        # Compatible releases have an equivalent combination of >= and ==. That\n",
      "        # is that ~=2.2 is equivalent to >=2.2,==2.*. This allows us to\n",
      "        # implement this in terms of the other specifiers instead of\n",
      "        # implementing it ourselves. The only thing we need to do is construct\n",
      "        # the other specifiers.\n",
      "\n",
      "        # We want everything but the last item in the version, but we want to\n",
      "        # ignore suffix segments.\n",
      "        prefix = _version_join(\n",
      "            list(itertools.takewhile(_is_not_suffix, _version_split(spec)))[:-1]\n",
      "        )\n",
      "\n",
      "        # Add the prefix notation to the end of our string\n",
      "        prefix += \".*\"\n",
      "\n",
      "        return self._get_operator(\">=\")(prospective, spec) and self._get_operator(\"==\")(\n",
      "            prospective, prefix\n",
      "        )\n",
      "\n",
      "    def _compare_equal(self, prospective: Version, spec: str) -> bool:\n",
      "        # We need special logic to handle prefix matching\n",
      "        if spec.endswith(\".*\"):\n",
      "            # In the case of prefix matching we want to ignore local segment.\n",
      "            normalized_prospective = canonicalize_version(\n",
      "                prospective.public, strip_trailing_zero=False\n",
      "            )\n",
      "            # Get the normalized version string ignoring the trailing .*\n",
      "            normalized_spec = canonicalize_version(spec[:-2], strip_trailing_zero=False)\n",
      "            # Split the spec out by bangs and dots, and pretend that there is\n",
      "            # an implicit dot in between a release segment and a pre-release segment.\n",
      "            split_spec = _version_split(normalized_spec)\n",
      "\n",
      "            # Split the prospective version out by bangs and dots, and pretend\n",
      "            # that there is an implicit dot in between a release segment and\n",
      "            # a pre-release segment.\n",
      "            split_prospective = _version_split(normalized_prospective)\n",
      "\n",
      "            # 0-pad the prospective version before shortening it to get the correct\n",
      "            # shortened version.\n",
      "            padded_prospective, _ = _pad_version(split_prospective, split_spec)\n",
      "\n",
      "            # Shorten the prospective version to be the same length as the spec\n",
      "            # so that we can determine if the specifier is a prefix of the\n",
      "            # prospective version or not.\n",
      "            shortened_prospective = padded_prospective[: len(split_spec)]\n",
      "\n",
      "            return shortened_prospective == split_spec\n",
      "        else:\n",
      "            # Convert our spec string into a Version\n",
      "            spec_version = Version(spec)\n",
      "\n",
      "            # If the specifier does not have a local segment, then we want to\n",
      "            # act as if the prospective version also does not have a local\n",
      "            # segment.\n",
      "            if not spec_version.local:\n",
      "                prospective = Version(prospective.public)\n",
      "\n",
      "            return prospective == spec_version\n",
      "\n",
      "    def _compare_not_equal(self, prospective: Version, spec: str) -> bool:\n",
      "        return not self._compare_equal(prospective, spec)\n",
      "\n",
      "    def _compare_less_than_equal(self, prospective: Version, spec: str) -> bool:\n",
      "        # NB: Local version identifiers are NOT permitted in the version\n",
      "        # specifier, so local version labels can be universally removed from\n",
      "        # the prospective version.\n",
      "        return Version(prospective.public) <= Version(spec)\n",
      "\n",
      "    def _compare_greater_than_equal(self, prospective: Version, spec: str) -> bool:\n",
      "        # NB: Local version identifiers are NOT permitted in the version\n",
      "        # specifier, so local version labels can be universally removed from\n",
      "        # the prospective version.\n",
      "        return Version(prospective.public) >= Version(spec)\n",
      "\n",
      "    def _compare_less_than(self, prospective: Version, spec_str: str) -> bool:\n",
      "        # Convert our spec to a Version instance, since we'll want to work with\n",
      "        # it as a version.\n",
      "        spec = Version(spec_str)\n",
      "\n",
      "        # Check to see if the prospective version is less than the spec\n",
      "        # version. If it's not we can short circuit and just return False now\n",
      "        # instead of doing extra unneeded work.\n",
      "        if not prospective < spec:\n",
      "            return False\n",
      "\n",
      "        # This special case is here so that, unless the specifier itself\n",
      "        # includes is a pre-release version, that we do not accept pre-release\n",
      "        # versions for the version mentioned in the specifier (e.g. <3.1 should\n",
      "        # not match 3.1.dev0, but should match 3.0.dev0).\n",
      "        if not spec.is_prerelease and prospective.is_prerelease:\n",
      "            if Version(prospective.base_version) == Version(spec.base_version):\n",
      "                return False\n",
      "\n",
      "        # If we've gotten to here, it means that prospective version is both\n",
      "        # less than the spec version *and* it's not a pre-release of the same\n",
      "        # version in the spec.\n",
      "        return True\n",
      "\n",
      "    def _compare_greater_than(self, prospective: Version, spec_str: str) -> bool:\n",
      "        # Convert our spec to a Version instance, since we'll want to work with\n",
      "        # it as a version.\n",
      "        spec = Version(spec_str)\n",
      "\n",
      "        # Check to see if the prospective version is greater than the spec\n",
      "        # version. If it's not we can short circuit and just return False now\n",
      "        # instead of doing extra unneeded work.\n",
      "        if not prospective > spec:\n",
      "            return False\n",
      "\n",
      "        # This special case is here so that, unless the specifier itself\n",
      "        # includes is a post-release version, that we do not accept\n",
      "        # post-release versions for the version mentioned in the specifier\n",
      "        # (e.g. >3.1 should not match 3.0.post0, but should match 3.2.post0).\n",
      "        if not spec.is_postrelease and prospective.is_postrelease:\n",
      "            if Version(prospective.base_version) == Version(spec.base_version):\n",
      "                return False\n",
      "\n",
      "        # Ensure that we do not allow a local version of the version mentioned\n",
      "        # in the specifier, which is technically greater than, to match.\n",
      "        if prospective.local is not None:\n",
      "            if Version(prospective.base_version) == Version(spec.base_version):\n",
      "                return False\n",
      "\n",
      "        # If we've gotten to here, it means that prospective version is both\n",
      "        # greater than the spec version *and* it's not a pre-release of the\n",
      "        # same version in the spec.\n",
      "        return True\n",
      "\n",
      "    def _compare_arbitrary(self, prospective: Version, spec: str) -> bool:\n",
      "        return str(prospective).lower() == str(spec).lower()\n",
      "\n",
      "    def __contains__(self, item: Union[str, Version]) -> bool:\n",
      "        \"\"\"Return whether or not the item is contained in this specifier.\n",
      "\n",
      "        :param item: The item to check for.\n",
      "\n",
      "        This is used for the ``in`` operator and behaves the same as\n",
      "        :meth:`contains` with no ``prereleases`` argument passed.\n",
      "\n",
      "        >>> \"1.2.3\" in Specifier(\">=1.2.3\")\n",
      "        True\n",
      "        >>> Version(\"1.2.3\") in Specifier(\">=1.2.3\")\n",
      "        True\n",
      "        >>> \"1.0.0\" in Specifier(\">=1.2.3\")\n",
      "        False\n",
      "        >>> \"1.3.0a1\" in Specifier(\">=1.2.3\")\n",
      "        False\n",
      "        >>> \"1.3.0a1\" in Specifier(\">=1.2.3\", prereleases=True)\n",
      "        True\n",
      "        \"\"\"\n",
      "        return self.contains(item)\n",
      "\n",
      "    def contains(\n",
      "        self, item: UnparsedVersion, prereleases: Optional[bool] = None\n",
      "    ) -> bool:\n",
      "        \"\"\"Return whether or not the item is contained in this specifier.\n",
      "\n",
      "        :param item:\n",
      "            The item to check for, which can be a version string or a\n",
      "            :class:`Version` instance.\n",
      "        :param prereleases:\n",
      "            Whether or not to match prereleases with this Specifier. If set to\n",
      "            ``None`` (the default), it uses :attr:`prereleases` to determine\n",
      "            whether or not prereleases are allowed.\n",
      "\n",
      "        >>> Specifier(\">=1.2.3\").contains(\"1.2.3\")\n",
      "        True\n",
      "        >>> Specifier(\">=1.2.3\").contains(Version(\"1.2.3\"))\n",
      "        True\n",
      "        >>> Specifier(\">=1.2.3\").contains(\"1.0.0\")\n",
      "        False\n",
      "        >>> Specifier(\">=1.2.3\").contains(\"1.3.0a1\")\n",
      "        False\n",
      "        >>> Specifier(\">=1.2.3\", prereleases=True).contains(\"1.3.0a1\")\n",
      "        True\n",
      "        >>> Specifier(\">=1.2.3\").contains(\"1.3.0a1\", prereleases=True)\n",
      "        True\n",
      "        \"\"\"\n",
      "\n",
      "        # Determine if prereleases are to be allowed or not.\n",
      "        if prereleases is None:\n",
      "            prereleases = self.prereleases\n",
      "\n",
      "        # Normalize item to a Version, this allows us to have a shortcut for\n",
      "        # \"2.0\" in Specifier(\">=2\")\n",
      "        normalized_item = _coerce_version(item)\n",
      "\n",
      "        # Determine if we should be supporting prereleases in this specifier\n",
      "        # or not, if we do not support prereleases than we can short circuit\n",
      "        # logic if this version is a prereleases.\n",
      "        if normalized_item.is_prerelease and not prereleases:\n",
      "            return False\n",
      "\n",
      "        # Actually do the comparison to determine if this item is contained\n",
      "        # within this Specifier or not.\n",
      "        operator_callable: CallableOperator = self._get_operator(self.operator)\n",
      "        return operator_callable(normalized_item, self.version)\n",
      "\n",
      "    def filter(\n",
      "        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None\n",
      "    ) -> Iterator[UnparsedVersionVar]:\n",
      "        \"\"\"Filter items in the given iterable, that match the specifier.\n",
      "\n",
      "        :param iterable:\n",
      "            An iterable that can contain version strings and :class:`Version` instances.\n",
      "            The items in the iterable will be filtered according to the specifier.\n",
      "        :param prereleases:\n",
      "            Whether or not to allow prereleases in the returned iterator. If set to\n",
      "            ``None`` (the default), it will be intelligently decide whether to allow\n",
      "            prereleases or not (based on the :attr:`prereleases` attribute, and\n",
      "            whether the only versions matching are prereleases).\n",
      "\n",
      "        This method is smarter than just ``filter(Specifier().contains, [...])``\n",
      "        because it implements the rule from :pep:`440` that a prerelease item\n",
      "        SHOULD be accepted if no other versions match the given specifier.\n",
      "\n",
      "        >>> list(Specifier(\">=1.2.3\").filter([\"1.2\", \"1.3\", \"1.5a1\"]))\n",
      "        ['1.3']\n",
      "        >>> list(Specifier(\">=1.2.3\").filter([\"1.2\", \"1.2.3\", \"1.3\", Version(\"1.4\")]))\n",
      "        ['1.2.3', '1.3', <Version('1.4')>]\n",
      "        >>> list(Specifier(\">=1.2.3\").filter([\"1.2\", \"1.5a1\"]))\n",
      "        ['1.5a1']\n",
      "        >>> list(Specifier(\">=1.2.3\").filter([\"1.3\", \"1.5a1\"], prereleases=True))\n",
      "        ['1.3', '1.5a1']\n",
      "        >>> list(Specifier(\">=1.2.3\", prereleases=True).filter([\"1.3\", \"1.5a1\"]))\n",
      "        ['1.3', '1.5a1']\n",
      "        \"\"\"\n",
      "\n",
      "        yielded = False\n",
      "        found_prereleases = []\n",
      "\n",
      "        kw = {\"prereleases\": prereleases if prereleases is not None else True}\n",
      "\n",
      "        # Attempt to iterate over all the values in the iterable and if any of\n",
      "        # them match, yield them.\n",
      "        for version in iterable:\n",
      "            parsed_version = _coerce_version(version)\n",
      "\n",
      "            if self.contains(parsed_version, **kw):\n",
      "                # If our version is a prerelease, and we were not set to allow\n",
      "                # prereleases, then we'll store it for later in case nothing\n",
      "                # else matches this specifier.\n",
      "                if parsed_version.is_prerelease and not (\n",
      "                    prereleases or self.prereleases\n",
      "                ):\n",
      "                    found_prereleases.append(version)\n",
      "                # Either this is not a prerelease, or we should have been\n",
      "                # accepting prereleases from the beginning.\n",
      "                else:\n",
      "                    yielded = True\n",
      "                    yield version\n",
      "\n",
      "        # Now that we've iterated over everything, determine if we've yielded\n",
      "        # any values, and if we have not and we have any prereleases stored up\n",
      "        # then we will go ahead and yield the prereleases.\n",
      "        if not yielded and found_prereleases:\n",
      "            for version in found_prereleases:\n",
      "                yield version\n",
      "\n",
      "\n",
      "_prefix_regex = re.compile(r\"^([0-9]+)((?:a|b|c|rc)[0-9]+)$\")\n",
      "\n",
      "\n",
      "def _version_split(version: str) -> List[str]:\n",
      "    \"\"\"Split version into components.\n",
      "\n",
      "    The split components are intended for version comparison. The logic does\n",
      "    not attempt to retain the original version string, so joining the\n",
      "    components back with :func:`_version_join` may not produce the original\n",
      "    version string.\n",
      "    \"\"\"\n",
      "    result: List[str] = []\n",
      "\n",
      "    epoch, _, rest = version.rpartition(\"!\")\n",
      "    result.append(epoch or \"0\")\n",
      "\n",
      "    for item in rest.split(\".\"):\n",
      "        match = _prefix_regex.search(item)\n",
      "        if match:\n",
      "            result.extend(match.groups())\n",
      "        else:\n",
      "            result.append(item)\n",
      "    return result\n",
      "\n",
      "\n",
      "def _version_join(components: List[str]) -> str:\n",
      "    \"\"\"Join split version components into a version string.\n",
      "\n",
      "    This function assumes the input came from :func:`_version_split`, where the\n",
      "    first component must be the epoch (either empty or numeric), and all other\n",
      "    components numeric.\n",
      "    \"\"\"\n",
      "    epoch, *rest = components\n",
      "    return f\"{epoch}!{'.'.join(rest)}\"\n",
      "\n",
      "\n",
      "def _is_not_suffix(segment: str) -> bool:\n",
      "    return not any(\n",
      "        segment.startswith(prefix) for prefix in (\"dev\", \"a\", \"b\", \"rc\", \"post\")\n",
      "    )\n",
      "\n",
      "\n",
      "def _pad_version(left: List[str], right: List[str]) -> Tuple[List[str], List[str]]:\n",
      "    left_split, right_split = [], []\n",
      "\n",
      "    # Get the release segment of our versions\n",
      "    left_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))\n",
      "    right_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))\n",
      "\n",
      "    # Get the rest of our versions\n",
      "    left_split.append(left[len(left_split[0]) :])\n",
      "    right_split.append(right[len(right_split[0]) :])\n",
      "\n",
      "    # Insert our padding\n",
      "    left_split.insert(1, [\"0\"] * max(0, len(right_split[0]) - len(left_split[0])))\n",
      "    right_split.insert(1, [\"0\"] * max(0, len(left_split[0]) - len(right_split[0])))\n",
      "\n",
      "    return (\n",
      "        list(itertools.chain.from_iterable(left_split)),\n",
      "        list(itertools.chain.from_iterable(right_split)),\n",
      "    )\n",
      "\n",
      "\n",
      "class SpecifierSet(BaseSpecifier):\n",
      "    \"\"\"This class abstracts handling of a set of version specifiers.\n",
      "\n",
      "    It can be passed a single specifier (``>=3.0``), a comma-separated list of\n",
      "    specifiers (``>=3.0,!=3.1``), or no specifier at all.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self, specifiers: str = \"\", prereleases: Optional[bool] = None\n",
      "    ) -> None:\n",
      "        \"\"\"Initialize a SpecifierSet instance.\n",
      "\n",
      "        :param specifiers:\n",
      "            The string representation of a specifier or a comma-separated list of\n",
      "            specifiers which will be parsed and normalized before use.\n",
      "        :param prereleases:\n",
      "            This tells the SpecifierSet if it should accept prerelease versions if\n",
      "            applicable or not. The default of ``None`` will autodetect it from the\n",
      "            given specifiers.\n",
      "\n",
      "        :raises InvalidSpecifier:\n",
      "            If the given ``specifiers`` are not parseable than this exception will be\n",
      "            raised.\n",
      "        \"\"\"\n",
      "\n",
      "        # Split on `,` to break each individual specifier into it's own item, and\n",
      "        # strip each item to remove leading/trailing whitespace.\n",
      "        split_specifiers = [s.strip() for s in specifiers.split(\",\") if s.strip()]\n",
      "\n",
      "        # Make each individual specifier a Specifier and save in a frozen set for later.\n",
      "        self._specs = frozenset(map(Specifier, split_specifiers))\n",
      "\n",
      "        # Store our prereleases value so we can use it later to determine if\n",
      "        # we accept prereleases or not.\n",
      "        self._prereleases = prereleases\n",
      "\n",
      "    @property\n",
      "    def prereleases(self) -> Optional[bool]:\n",
      "        # If we have been given an explicit prerelease modifier, then we'll\n",
      "        # pass that through here.\n",
      "        if self._prereleases is not None:\n",
      "            return self._prereleases\n",
      "\n",
      "        # If we don't have any specifiers, and we don't have a forced value,\n",
      "        # then we'll just return None since we don't know if this should have\n",
      "        # pre-releases or not.\n",
      "        if not self._specs:\n",
      "            return None\n",
      "\n",
      "        # Otherwise we'll see if any of the given specifiers accept\n",
      "        # prereleases, if any of them do we'll return True, otherwise False.\n",
      "        return any(s.prereleases for s in self._specs)\n",
      "\n",
      "    @prereleases.setter\n",
      "    def prereleases(self, value: bool) -> None:\n",
      "        self._prereleases = value\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        \"\"\"A representation of the specifier set that shows all internal state.\n",
      "\n",
      "        Note that the ordering of the individual specifiers within the set may not\n",
      "        match the input string.\n",
      "\n",
      "        >>> SpecifierSet('>=1.0.0,!=2.0.0')\n",
      "        <SpecifierSet('!=2.0.0,>=1.0.0')>\n",
      "        >>> SpecifierSet('>=1.0.0,!=2.0.0', prereleases=False)\n",
      "        <SpecifierSet('!=2.0.0,>=1.0.0', prereleases=False)>\n",
      "        >>> SpecifierSet('>=1.0.0,!=2.0.0', prereleases=True)\n",
      "        <SpecifierSet('!=2.0.0,>=1.0.0', prereleases=True)>\n",
      "        \"\"\"\n",
      "        pre = (\n",
      "            f\", prereleases={self.prereleases!r}\"\n",
      "            if self._prereleases is not None\n",
      "            else \"\"\n",
      "        )\n",
      "\n",
      "        return f\"<SpecifierSet({str(self)!r}{pre})>\"\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        \"\"\"A string representation of the specifier set that can be round-tripped.\n",
      "\n",
      "        Note that the ordering of the individual specifiers within the set may not\n",
      "        match the input string.\n",
      "\n",
      "        >>> str(SpecifierSet(\">=1.0.0,!=1.0.1\"))\n",
      "        '!=1.0.1,>=1.0.0'\n",
      "        >>> str(SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=False))\n",
      "        '!=1.0.1,>=1.0.0'\n",
      "        \"\"\"\n",
      "        return \",\".join(sorted(str(s) for s in self._specs))\n",
      "\n",
      "    def __hash__(self) -> int:\n",
      "        return hash(self._specs)\n",
      "\n",
      "    def __and__(self, other: Union[\"SpecifierSet\", str]) -> \"SpecifierSet\":\n",
      "        \"\"\"Return a SpecifierSet which is a combination of the two sets.\n",
      "\n",
      "        :param other: The other object to combine with.\n",
      "\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") & '<=2.0.0,!=2.0.1'\n",
      "        <SpecifierSet('!=1.0.1,!=2.0.1,<=2.0.0,>=1.0.0')>\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") & SpecifierSet('<=2.0.0,!=2.0.1')\n",
      "        <SpecifierSet('!=1.0.1,!=2.0.1,<=2.0.0,>=1.0.0')>\n",
      "        \"\"\"\n",
      "        if isinstance(other, str):\n",
      "            other = SpecifierSet(other)\n",
      "        elif not isinstance(other, SpecifierSet):\n",
      "            return NotImplemented\n",
      "\n",
      "        specifier = SpecifierSet()\n",
      "        specifier._specs = frozenset(self._specs | other._specs)\n",
      "\n",
      "        if self._prereleases is None and other._prereleases is not None:\n",
      "            specifier._prereleases = other._prereleases\n",
      "        elif self._prereleases is not None and other._prereleases is None:\n",
      "            specifier._prereleases = self._prereleases\n",
      "        elif self._prereleases == other._prereleases:\n",
      "            specifier._prereleases = self._prereleases\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"Cannot combine SpecifierSets with True and False prerelease \"\n",
      "                \"overrides.\"\n",
      "            )\n",
      "\n",
      "        return specifier\n",
      "\n",
      "    def __eq__(self, other: object) -> bool:\n",
      "        \"\"\"Whether or not the two SpecifierSet-like objects are equal.\n",
      "\n",
      "        :param other: The other object to check against.\n",
      "\n",
      "        The value of :attr:`prereleases` is ignored.\n",
      "\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") == SpecifierSet(\">=1.0.0,!=1.0.1\")\n",
      "        True\n",
      "        >>> (SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=False) ==\n",
      "        ...  SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=True))\n",
      "        True\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") == \">=1.0.0,!=1.0.1\"\n",
      "        True\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") == SpecifierSet(\">=1.0.0\")\n",
      "        False\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") == SpecifierSet(\">=1.0.0,!=1.0.2\")\n",
      "        False\n",
      "        \"\"\"\n",
      "        if isinstance(other, (str, Specifier)):\n",
      "            other = SpecifierSet(str(other))\n",
      "        elif not isinstance(other, SpecifierSet):\n",
      "            return NotImplemented\n",
      "\n",
      "        return self._specs == other._specs\n",
      "\n",
      "    def __len__(self) -> int:\n",
      "        \"\"\"Returns the number of specifiers in this specifier set.\"\"\"\n",
      "        return len(self._specs)\n",
      "\n",
      "    def __iter__(self) -> Iterator[Specifier]:\n",
      "        \"\"\"\n",
      "        Returns an iterator over all the underlying :class:`Specifier` instances\n",
      "        in this specifier set.\n",
      "\n",
      "        >>> sorted(SpecifierSet(\">=1.0.0,!=1.0.1\"), key=str)\n",
      "        [<Specifier('!=1.0.1')>, <Specifier('>=1.0.0')>]\n",
      "        \"\"\"\n",
      "        return iter(self._specs)\n",
      "\n",
      "    def __contains__(self, item: UnparsedVersion) -> bool:\n",
      "        \"\"\"Return whether or not the item is contained in this specifier.\n",
      "\n",
      "        :param item: The item to check for.\n",
      "\n",
      "        This is used for the ``in`` operator and behaves the same as\n",
      "        :meth:`contains` with no ``prereleases`` argument passed.\n",
      "\n",
      "        >>> \"1.2.3\" in SpecifierSet(\">=1.0.0,!=1.0.1\")\n",
      "        True\n",
      "        >>> Version(\"1.2.3\") in SpecifierSet(\">=1.0.0,!=1.0.1\")\n",
      "        True\n",
      "        >>> \"1.0.1\" in SpecifierSet(\">=1.0.0,!=1.0.1\")\n",
      "        False\n",
      "        >>> \"1.3.0a1\" in SpecifierSet(\">=1.0.0,!=1.0.1\")\n",
      "        False\n",
      "        >>> \"1.3.0a1\" in SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=True)\n",
      "        True\n",
      "        \"\"\"\n",
      "        return self.contains(item)\n",
      "\n",
      "    def contains(\n",
      "        self,\n",
      "        item: UnparsedVersion,\n",
      "        prereleases: Optional[bool] = None,\n",
      "        installed: Optional[bool] = None,\n",
      "    ) -> bool:\n",
      "        \"\"\"Return whether or not the item is contained in this SpecifierSet.\n",
      "\n",
      "        :param item:\n",
      "            The item to check for, which can be a version string or a\n",
      "            :class:`Version` instance.\n",
      "        :param prereleases:\n",
      "            Whether or not to match prereleases with this SpecifierSet. If set to\n",
      "            ``None`` (the default), it uses :attr:`prereleases` to determine\n",
      "            whether or not prereleases are allowed.\n",
      "\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(\"1.2.3\")\n",
      "        True\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(Version(\"1.2.3\"))\n",
      "        True\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(\"1.0.1\")\n",
      "        False\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(\"1.3.0a1\")\n",
      "        False\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=True).contains(\"1.3.0a1\")\n",
      "        True\n",
      "        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(\"1.3.0a1\", prereleases=True)\n",
      "        True\n",
      "        \"\"\"\n",
      "        # Ensure that our item is a Version instance.\n",
      "        if not isinstance(item, Version):\n",
      "            item = Version(item)\n",
      "\n",
      "        # Determine if we're forcing a prerelease or not, if we're not forcing\n",
      "        # one for this particular filter call, then we'll use whatever the\n",
      "        # SpecifierSet thinks for whether or not we should support prereleases.\n",
      "        if prereleases is None:\n",
      "            prereleases = self.prereleases\n",
      "\n",
      "        # We can determine if we're going to allow pre-releases by looking to\n",
      "        # see if any of the underlying items supports them. If none of them do\n",
      "        # and this item is a pre-release then we do not allow it and we can\n",
      "        # short circuit that here.\n",
      "        # Note: This means that 1.0.dev1 would not be contained in something\n",
      "        #       like >=1.0.devabc however it would be in >=1.0.debabc,>0.0.dev0\n",
      "        if not prereleases and item.is_prerelease:\n",
      "            return False\n",
      "\n",
      "        if installed and item.is_prerelease:\n",
      "            item = Version(item.base_version)\n",
      "\n",
      "        # We simply dispatch to the underlying specs here to make sure that the\n",
      "        # given version is contained within all of them.\n",
      "        # Note: This use of all() here means that an empty set of specifiers\n",
      "        #       will always return True, this is an explicit design decision.\n",
      "        return all(s.contains(item, prereleases=prereleases) for s in self._specs)\n",
      "\n",
      "    def filter(\n",
      "        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None\n",
      "    ) -> Iterator[UnparsedVersionVar]:\n",
      "        \"\"\"Filter items in the given iterable, that match the specifiers in this set.\n",
      "\n",
      "        :param iterable:\n",
      "            An iterable that can contain version strings and :class:`Version` instances.\n",
      "            The items in the iterable will be filtered according to the specifier.\n",
      "        :param prereleases:\n",
      "            Whether or not to allow prereleases in the returned iterator. If set to\n",
      "            ``None`` (the default), it will be intelligently decide whether to allow\n",
      "            prereleases or not (based on the :attr:`prereleases` attribute, and\n",
      "            whether the only versions matching are prereleases).\n",
      "\n",
      "        This method is smarter than just ``filter(SpecifierSet(...).contains, [...])``\n",
      "        because it implements the rule from :pep:`440` that a prerelease item\n",
      "        SHOULD be accepted if no other versions match the given specifier.\n",
      "\n",
      "        >>> list(SpecifierSet(\">=1.2.3\").filter([\"1.2\", \"1.3\", \"1.5a1\"]))\n",
      "        ['1.3']\n",
      "        >>> list(SpecifierSet(\">=1.2.3\").filter([\"1.2\", \"1.3\", Version(\"1.4\")]))\n",
      "        ['1.3', <Version('1.4')>]\n",
      "        >>> list(SpecifierSet(\">=1.2.3\").filter([\"1.2\", \"1.5a1\"]))\n",
      "        []\n",
      "        >>> list(SpecifierSet(\">=1.2.3\").filter([\"1.3\", \"1.5a1\"], prereleases=True))\n",
      "        ['1.3', '1.5a1']\n",
      "        >>> list(SpecifierSet(\">=1.2.3\", prereleases=True).filter([\"1.3\", \"1.5a1\"]))\n",
      "        ['1.3', '1.5a1']\n",
      "\n",
      "        An \"empty\" SpecifierSet will filter items based on the presence of prerelease\n",
      "        versions in the set.\n",
      "\n",
      "        >>> list(SpecifierSet(\"\").filter([\"1.3\", \"1.5a1\"]))\n",
      "        ['1.3']\n",
      "        >>> list(SpecifierSet(\"\").filter([\"1.5a1\"]))\n",
      "        ['1.5a1']\n",
      "        >>> list(SpecifierSet(\"\", prereleases=True).filter([\"1.3\", \"1.5a1\"]))\n",
      "        ['1.3', '1.5a1']\n",
      "        >>> list(SpecifierSet(\"\").filter([\"1.3\", \"1.5a1\"], prereleases=True))\n",
      "        ['1.3', '1.5a1']\n",
      "        \"\"\"\n",
      "        # Determine if we're forcing a prerelease or not, if we're not forcing\n",
      "        # one for this particular filter call, then we'll use whatever the\n",
      "        # SpecifierSet thinks for whether or not we should support prereleases.\n",
      "        if prereleases is None:\n",
      "            prereleases = self.prereleases\n",
      "\n",
      "        # If we have any specifiers, then we want to wrap our iterable in the\n",
      "        # filter method for each one, this will act as a logical AND amongst\n",
      "        # each specifier.\n",
      "        if self._specs:\n",
      "            for spec in self._specs:\n",
      "                iterable = spec.filter(iterable, prereleases=bool(prereleases))\n",
      "            return iter(iterable)\n",
      "        # If we do not have any specifiers, then we need to have a rough filter\n",
      "        # which will filter out any pre-releases, unless there are no final\n",
      "        # releases.\n",
      "        else:\n",
      "            filtered: List[UnparsedVersionVar] = []\n",
      "            found_prereleases: List[UnparsedVersionVar] = []\n",
      "\n",
      "            for item in iterable:\n",
      "                parsed_version = _coerce_version(item)\n",
      "\n",
      "                # Store any item which is a pre-release for later unless we've\n",
      "                # already found a final version or we are accepting prereleases\n",
      "                if parsed_version.is_prerelease and not prereleases:\n",
      "                    if not filtered:\n",
      "                        found_prereleases.append(item)\n",
      "                else:\n",
      "                    filtered.append(item)\n",
      "\n",
      "            # If we've found no items except for pre-releases, then we'll go\n",
      "            # ahead and use the pre-releases\n",
      "            if not filtered and found_prereleases and prereleases is None:\n",
      "                return iter(found_prereleases)\n",
      "\n",
      "            return iter(filtered)\n",
      "\n",
      "[end of src/wheel/vendored/packaging/specifiers.py]\n",
      "[start of src/wheel/vendored/packaging/markers.py]\n",
      "# This file is dual licensed under the terms of the Apache License, Version\n",
      "# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n",
      "# for complete details.\n",
      "\n",
      "import operator\n",
      "import os\n",
      "import platform\n",
      "import sys\n",
      "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
      "\n",
      "from ._parser import (\n",
      "    MarkerAtom,\n",
      "    MarkerList,\n",
      "    Op,\n",
      "    Value,\n",
      "    Variable,\n",
      ")\n",
      "from ._parser import (\n",
      "    parse_marker as _parse_marker,\n",
      ")\n",
      "from ._tokenizer import ParserSyntaxError\n",
      "from .specifiers import InvalidSpecifier, Specifier\n",
      "from .utils import canonicalize_name\n",
      "\n",
      "__all__ = [\n",
      "    \"InvalidMarker\",\n",
      "    \"UndefinedComparison\",\n",
      "    \"UndefinedEnvironmentName\",\n",
      "    \"Marker\",\n",
      "    \"default_environment\",\n",
      "]\n",
      "\n",
      "Operator = Callable[[str, str], bool]\n",
      "\n",
      "\n",
      "class InvalidMarker(ValueError):\n",
      "    \"\"\"\n",
      "    An invalid marker was found, users should refer to PEP 508.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "class UndefinedComparison(ValueError):\n",
      "    \"\"\"\n",
      "    An invalid operation was attempted on a value that doesn't support it.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "class UndefinedEnvironmentName(ValueError):\n",
      "    \"\"\"\n",
      "    A name was attempted to be used that does not exist inside of the\n",
      "    environment.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "def _normalize_extra_values(results: Any) -> Any:\n",
      "    \"\"\"\n",
      "    Normalize extra values.\n",
      "    \"\"\"\n",
      "    if isinstance(results[0], tuple):\n",
      "        lhs, op, rhs = results[0]\n",
      "        if isinstance(lhs, Variable) and lhs.value == \"extra\":\n",
      "            normalized_extra = canonicalize_name(rhs.value)\n",
      "            rhs = Value(normalized_extra)\n",
      "        elif isinstance(rhs, Variable) and rhs.value == \"extra\":\n",
      "            normalized_extra = canonicalize_name(lhs.value)\n",
      "            lhs = Value(normalized_extra)\n",
      "        results[0] = lhs, op, rhs\n",
      "    return results\n",
      "\n",
      "\n",
      "def _format_marker(\n",
      "    marker: Union[List[str], MarkerAtom, str], first: Optional[bool] = True\n",
      ") -> str:\n",
      "    assert isinstance(marker, (list, tuple, str))\n",
      "\n",
      "    # Sometimes we have a structure like [[...]] which is a single item list\n",
      "    # where the single item is itself it's own list. In that case we want skip\n",
      "    # the rest of this function so that we don't get extraneous () on the\n",
      "    # outside.\n",
      "    if (\n",
      "        isinstance(marker, list)\n",
      "        and len(marker) == 1\n",
      "        and isinstance(marker[0], (list, tuple))\n",
      "    ):\n",
      "        return _format_marker(marker[0])\n",
      "\n",
      "    if isinstance(marker, list):\n",
      "        inner = (_format_marker(m, first=False) for m in marker)\n",
      "        if first:\n",
      "            return \" \".join(inner)\n",
      "        else:\n",
      "            return \"(\" + \" \".join(inner) + \")\"\n",
      "    elif isinstance(marker, tuple):\n",
      "        return \" \".join([m.serialize() for m in marker])\n",
      "    else:\n",
      "        return marker\n",
      "\n",
      "\n",
      "_operators: Dict[str, Operator] = {\n",
      "    \"in\": lambda lhs, rhs: lhs in rhs,\n",
      "    \"not in\": lambda lhs, rhs: lhs not in rhs,\n",
      "    \"<\": operator.lt,\n",
      "    \"<=\": operator.le,\n",
      "    \"==\": operator.eq,\n",
      "    \"!=\": operator.ne,\n",
      "    \">=\": operator.ge,\n",
      "    \">\": operator.gt,\n",
      "}\n",
      "\n",
      "\n",
      "def _eval_op(lhs: str, op: Op, rhs: str) -> bool:\n",
      "    try:\n",
      "        spec = Specifier(\"\".join([op.serialize(), rhs]))\n",
      "    except InvalidSpecifier:\n",
      "        pass\n",
      "    else:\n",
      "        return spec.contains(lhs, prereleases=True)\n",
      "\n",
      "    oper: Optional[Operator] = _operators.get(op.serialize())\n",
      "    if oper is None:\n",
      "        raise UndefinedComparison(f\"Undefined {op!r} on {lhs!r} and {rhs!r}.\")\n",
      "\n",
      "    return oper(lhs, rhs)\n",
      "\n",
      "\n",
      "def _normalize(*values: str, key: str) -> Tuple[str, ...]:\n",
      "    # PEP 685 – Comparison of extra names for optional distribution dependencies\n",
      "    # https://peps.python.org/pep-0685/\n",
      "    # > When comparing extra names, tools MUST normalize the names being\n",
      "    # > compared using the semantics outlined in PEP 503 for names\n",
      "    if key == \"extra\":\n",
      "        return tuple(canonicalize_name(v) for v in values)\n",
      "\n",
      "    # other environment markers don't have such standards\n",
      "    return values\n",
      "\n",
      "\n",
      "def _evaluate_markers(markers: MarkerList, environment: Dict[str, str]) -> bool:\n",
      "    groups: List[List[bool]] = [[]]\n",
      "\n",
      "    for marker in markers:\n",
      "        assert isinstance(marker, (list, tuple, str))\n",
      "\n",
      "        if isinstance(marker, list):\n",
      "            groups[-1].append(_evaluate_markers(marker, environment))\n",
      "        elif isinstance(marker, tuple):\n",
      "            lhs, op, rhs = marker\n",
      "\n",
      "            if isinstance(lhs, Variable):\n",
      "                environment_key = lhs.value\n",
      "                lhs_value = environment[environment_key]\n",
      "                rhs_value = rhs.value\n",
      "            else:\n",
      "                lhs_value = lhs.value\n",
      "                environment_key = rhs.value\n",
      "                rhs_value = environment[environment_key]\n",
      "\n",
      "            lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n",
      "            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n",
      "        else:\n",
      "            assert marker in [\"and\", \"or\"]\n",
      "            if marker == \"or\":\n",
      "                groups.append([])\n",
      "\n",
      "    return any(all(item) for item in groups)\n",
      "\n",
      "\n",
      "def format_full_version(info: \"sys._version_info\") -> str:\n",
      "    version = \"{0.major}.{0.minor}.{0.micro}\".format(info)\n",
      "    kind = info.releaselevel\n",
      "    if kind != \"final\":\n",
      "        version += kind[0] + str(info.serial)\n",
      "    return version\n",
      "\n",
      "\n",
      "def default_environment() -> Dict[str, str]:\n",
      "    iver = format_full_version(sys.implementation.version)\n",
      "    implementation_name = sys.implementation.name\n",
      "    return {\n",
      "        \"implementation_name\": implementation_name,\n",
      "        \"implementation_version\": iver,\n",
      "        \"os_name\": os.name,\n",
      "        \"platform_machine\": platform.machine(),\n",
      "        \"platform_release\": platform.release(),\n",
      "        \"platform_system\": platform.system(),\n",
      "        \"platform_version\": platform.version(),\n",
      "        \"python_full_version\": platform.python_version(),\n",
      "        \"platform_python_implementation\": platform.python_implementation(),\n",
      "        \"python_version\": \".\".join(platform.python_version_tuple()[:2]),\n",
      "        \"sys_platform\": sys.platform,\n",
      "    }\n",
      "\n",
      "\n",
      "class Marker:\n",
      "    def __init__(self, marker: str) -> None:\n",
      "        # Note: We create a Marker object without calling this constructor in\n",
      "        #       packaging.requirements.Requirement. If any additional logic is\n",
      "        #       added here, make sure to mirror/adapt Requirement.\n",
      "        try:\n",
      "            self._markers = _normalize_extra_values(_parse_marker(marker))\n",
      "            # The attribute `_markers` can be described in terms of a recursive type:\n",
      "            # MarkerList = List[Union[Tuple[Node, ...], str, MarkerList]]\n",
      "            #\n",
      "            # For example, the following expression:\n",
      "            # python_version > \"3.6\" or (python_version == \"3.6\" and os_name == \"unix\")\n",
      "            #\n",
      "            # is parsed into:\n",
      "            # [\n",
      "            #     (<Variable('python_version')>, <Op('>')>, <Value('3.6')>),\n",
      "            #     'and',\n",
      "            #     [\n",
      "            #         (<Variable('python_version')>, <Op('==')>, <Value('3.6')>),\n",
      "            #         'or',\n",
      "            #         (<Variable('os_name')>, <Op('==')>, <Value('unix')>)\n",
      "            #     ]\n",
      "            # ]\n",
      "        except ParserSyntaxError as e:\n",
      "            raise InvalidMarker(str(e)) from e\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        return _format_marker(self._markers)\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        return f\"<Marker('{self}')>\"\n",
      "\n",
      "    def __hash__(self) -> int:\n",
      "        return hash((self.__class__.__name__, str(self)))\n",
      "\n",
      "    def __eq__(self, other: Any) -> bool:\n",
      "        if not isinstance(other, Marker):\n",
      "            return NotImplemented\n",
      "\n",
      "        return str(self) == str(other)\n",
      "\n",
      "    def evaluate(self, environment: Optional[Dict[str, str]] = None) -> bool:\n",
      "        \"\"\"Evaluate a marker.\n",
      "\n",
      "        Return the boolean from evaluating the given marker against the\n",
      "        environment. environment is an optional argument to override all or\n",
      "        part of the determined environment.\n",
      "\n",
      "        The environment is determined from the current Python process.\n",
      "        \"\"\"\n",
      "        current_environment = default_environment()\n",
      "        current_environment[\"extra\"] = \"\"\n",
      "        if environment is not None:\n",
      "            current_environment.update(environment)\n",
      "            # The API used to allow setting extra to None. We need to handle this\n",
      "            # case for backwards compatibility.\n",
      "            if current_environment[\"extra\"] is None:\n",
      "                current_environment[\"extra\"] = \"\"\n",
      "\n",
      "        return _evaluate_markers(self._markers, current_environment)\n",
      "\n",
      "[end of src/wheel/vendored/packaging/markers.py]\n",
      "[start of src/wheel/vendored/packaging/_manylinux.py]\n",
      "import collections\n",
      "import contextlib\n",
      "import functools\n",
      "import os\n",
      "import re\n",
      "import sys\n",
      "import warnings\n",
      "from typing import Dict, Generator, Iterator, NamedTuple, Optional, Sequence, Tuple\n",
      "\n",
      "from ._elffile import EIClass, EIData, ELFFile, EMachine\n",
      "\n",
      "EF_ARM_ABIMASK = 0xFF000000\n",
      "EF_ARM_ABI_VER5 = 0x05000000\n",
      "EF_ARM_ABI_FLOAT_HARD = 0x00000400\n",
      "\n",
      "\n",
      "# `os.PathLike` not a generic type until Python 3.9, so sticking with `str`\n",
      "# as the type for `path` until then.\n",
      "@contextlib.contextmanager\n",
      "def _parse_elf(path: str) -> Generator[Optional[ELFFile], None, None]:\n",
      "    try:\n",
      "        with open(path, \"rb\") as f:\n",
      "            yield ELFFile(f)\n",
      "    except (OSError, TypeError, ValueError):\n",
      "        yield None\n",
      "\n",
      "\n",
      "def _is_linux_armhf(executable: str) -> bool:\n",
      "    # hard-float ABI can be detected from the ELF header of the running\n",
      "    # process\n",
      "    # https://static.docs.arm.com/ihi0044/g/aaelf32.pdf\n",
      "    with _parse_elf(executable) as f:\n",
      "        return (\n",
      "            f is not None\n",
      "            and f.capacity == EIClass.C32\n",
      "            and f.encoding == EIData.Lsb\n",
      "            and f.machine == EMachine.Arm\n",
      "            and f.flags & EF_ARM_ABIMASK == EF_ARM_ABI_VER5\n",
      "            and f.flags & EF_ARM_ABI_FLOAT_HARD == EF_ARM_ABI_FLOAT_HARD\n",
      "        )\n",
      "\n",
      "\n",
      "def _is_linux_i686(executable: str) -> bool:\n",
      "    with _parse_elf(executable) as f:\n",
      "        return (\n",
      "            f is not None\n",
      "            and f.capacity == EIClass.C32\n",
      "            and f.encoding == EIData.Lsb\n",
      "            and f.machine == EMachine.I386\n",
      "        )\n",
      "\n",
      "\n",
      "def _have_compatible_abi(executable: str, archs: Sequence[str]) -> bool:\n",
      "    if \"armv7l\" in archs:\n",
      "        return _is_linux_armhf(executable)\n",
      "    if \"i686\" in archs:\n",
      "        return _is_linux_i686(executable)\n",
      "    allowed_archs = {\n",
      "        \"x86_64\",\n",
      "        \"aarch64\",\n",
      "        \"ppc64\",\n",
      "        \"ppc64le\",\n",
      "        \"s390x\",\n",
      "        \"loongarch64\",\n",
      "        \"riscv64\",\n",
      "    }\n",
      "    return any(arch in allowed_archs for arch in archs)\n",
      "\n",
      "\n",
      "# If glibc ever changes its major version, we need to know what the last\n",
      "# minor version was, so we can build the complete list of all versions.\n",
      "# For now, guess what the highest minor version might be, assume it will\n",
      "# be 50 for testing. Once this actually happens, update the dictionary\n",
      "# with the actual value.\n",
      "_LAST_GLIBC_MINOR: Dict[int, int] = collections.defaultdict(lambda: 50)\n",
      "\n",
      "\n",
      "class _GLibCVersion(NamedTuple):\n",
      "    major: int\n",
      "    minor: int\n",
      "\n",
      "\n",
      "def _glibc_version_string_confstr() -> Optional[str]:\n",
      "    \"\"\"\n",
      "    Primary implementation of glibc_version_string using os.confstr.\n",
      "    \"\"\"\n",
      "    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely\n",
      "    # to be broken or missing. This strategy is used in the standard library\n",
      "    # platform module.\n",
      "    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c/Lib/platform.py#L175-L183\n",
      "    try:\n",
      "        # Should be a string like \"glibc 2.17\".\n",
      "        version_string: Optional[str] = os.confstr(\"CS_GNU_LIBC_VERSION\")\n",
      "        assert version_string is not None\n",
      "        _, version = version_string.rsplit()\n",
      "    except (AssertionError, AttributeError, OSError, ValueError):\n",
      "        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...\n",
      "        return None\n",
      "    return version\n",
      "\n",
      "\n",
      "def _glibc_version_string_ctypes() -> Optional[str]:\n",
      "    \"\"\"\n",
      "    Fallback implementation of glibc_version_string using ctypes.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        import ctypes\n",
      "    except ImportError:\n",
      "        return None\n",
      "\n",
      "    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen\n",
      "    # manpage says, \"If filename is NULL, then the returned handle is for the\n",
      "    # main program\". This way we can let the linker do the work to figure out\n",
      "    # which libc our process is actually using.\n",
      "    #\n",
      "    # We must also handle the special case where the executable is not a\n",
      "    # dynamically linked executable. This can occur when using musl libc,\n",
      "    # for example. In this situation, dlopen() will error, leading to an\n",
      "    # OSError. Interestingly, at least in the case of musl, there is no\n",
      "    # errno set on the OSError. The single string argument used to construct\n",
      "    # OSError comes from libc itself and is therefore not portable to\n",
      "    # hard code here. In any case, failure to call dlopen() means we\n",
      "    # can proceed, so we bail on our attempt.\n",
      "    try:\n",
      "        process_namespace = ctypes.CDLL(None)\n",
      "    except OSError:\n",
      "        return None\n",
      "\n",
      "    try:\n",
      "        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n",
      "    except AttributeError:\n",
      "        # Symbol doesn't exist -> therefore, we are not linked to\n",
      "        # glibc.\n",
      "        return None\n",
      "\n",
      "    # Call gnu_get_libc_version, which returns a string like \"2.5\"\n",
      "    gnu_get_libc_version.restype = ctypes.c_char_p\n",
      "    version_str: str = gnu_get_libc_version()\n",
      "    # py2 / py3 compatibility:\n",
      "    if not isinstance(version_str, str):\n",
      "        version_str = version_str.decode(\"ascii\")\n",
      "\n",
      "    return version_str\n",
      "\n",
      "\n",
      "def _glibc_version_string() -> Optional[str]:\n",
      "    \"\"\"Returns glibc version string, or None if not using glibc.\"\"\"\n",
      "    return _glibc_version_string_confstr() or _glibc_version_string_ctypes()\n",
      "\n",
      "\n",
      "def _parse_glibc_version(version_str: str) -> Tuple[int, int]:\n",
      "    \"\"\"Parse glibc version.\n",
      "\n",
      "    We use a regexp instead of str.split because we want to discard any\n",
      "    random junk that might come after the minor version -- this might happen\n",
      "    in patched/forked versions of glibc (e.g. Linaro's version of glibc\n",
      "    uses version strings like \"2.20-2014.11\"). See gh-3588.\n",
      "    \"\"\"\n",
      "    m = re.match(r\"(?P<major>[0-9]+)\\.(?P<minor>[0-9]+)\", version_str)\n",
      "    if not m:\n",
      "        warnings.warn(\n",
      "            f\"Expected glibc version with 2 components major.minor,\"\n",
      "            f\" got: {version_str}\",\n",
      "            RuntimeWarning,\n",
      "        )\n",
      "        return -1, -1\n",
      "    return int(m.group(\"major\")), int(m.group(\"minor\"))\n",
      "\n",
      "\n",
      "@functools.lru_cache\n",
      "def _get_glibc_version() -> Tuple[int, int]:\n",
      "    version_str = _glibc_version_string()\n",
      "    if version_str is None:\n",
      "        return (-1, -1)\n",
      "    return _parse_glibc_version(version_str)\n",
      "\n",
      "\n",
      "# From PEP 513, PEP 600\n",
      "def _is_compatible(arch: str, version: _GLibCVersion) -> bool:\n",
      "    sys_glibc = _get_glibc_version()\n",
      "    if sys_glibc < version:\n",
      "        return False\n",
      "    # Check for presence of _manylinux module.\n",
      "    try:\n",
      "        import _manylinux\n",
      "    except ImportError:\n",
      "        return True\n",
      "    if hasattr(_manylinux, \"manylinux_compatible\"):\n",
      "        result = _manylinux.manylinux_compatible(version[0], version[1], arch)\n",
      "        if result is not None:\n",
      "            return bool(result)\n",
      "        return True\n",
      "    if version == _GLibCVersion(2, 5):\n",
      "        if hasattr(_manylinux, \"manylinux1_compatible\"):\n",
      "            return bool(_manylinux.manylinux1_compatible)\n",
      "    if version == _GLibCVersion(2, 12):\n",
      "        if hasattr(_manylinux, \"manylinux2010_compatible\"):\n",
      "            return bool(_manylinux.manylinux2010_compatible)\n",
      "    if version == _GLibCVersion(2, 17):\n",
      "        if hasattr(_manylinux, \"manylinux2014_compatible\"):\n",
      "            return bool(_manylinux.manylinux2014_compatible)\n",
      "    return True\n",
      "\n",
      "\n",
      "_LEGACY_MANYLINUX_MAP = {\n",
      "    # CentOS 7 w/ glibc 2.17 (PEP 599)\n",
      "    (2, 17): \"manylinux2014\",\n",
      "    # CentOS 6 w/ glibc 2.12 (PEP 571)\n",
      "    (2, 12): \"manylinux2010\",\n",
      "    # CentOS 5 w/ glibc 2.5 (PEP 513)\n",
      "    (2, 5): \"manylinux1\",\n",
      "}\n",
      "\n",
      "\n",
      "def platform_tags(archs: Sequence[str]) -> Iterator[str]:\n",
      "    \"\"\"Generate manylinux tags compatible to the current platform.\n",
      "\n",
      "    :param archs: Sequence of compatible architectures.\n",
      "        The first one shall be the closest to the actual architecture and be the part of\n",
      "        platform tag after the ``linux_`` prefix, e.g. ``x86_64``.\n",
      "        The ``linux_`` prefix is assumed as a prerequisite for the current platform to\n",
      "        be manylinux-compatible.\n",
      "\n",
      "    :returns: An iterator of compatible manylinux tags.\n",
      "    \"\"\"\n",
      "    if not _have_compatible_abi(sys.executable, archs):\n",
      "        return\n",
      "    # Oldest glibc to be supported regardless of architecture is (2, 17).\n",
      "    too_old_glibc2 = _GLibCVersion(2, 16)\n",
      "    if set(archs) & {\"x86_64\", \"i686\"}:\n",
      "        # On x86/i686 also oldest glibc to be supported is (2, 5).\n",
      "        too_old_glibc2 = _GLibCVersion(2, 4)\n",
      "    current_glibc = _GLibCVersion(*_get_glibc_version())\n",
      "    glibc_max_list = [current_glibc]\n",
      "    # We can assume compatibility across glibc major versions.\n",
      "    # https://sourceware.org/bugzilla/show_bug.cgi?id=24636\n",
      "    #\n",
      "    # Build a list of maximum glibc versions so that we can\n",
      "    # output the canonical list of all glibc from current_glibc\n",
      "    # down to too_old_glibc2, including all intermediary versions.\n",
      "    for glibc_major in range(current_glibc.major - 1, 1, -1):\n",
      "        glibc_minor = _LAST_GLIBC_MINOR[glibc_major]\n",
      "        glibc_max_list.append(_GLibCVersion(glibc_major, glibc_minor))\n",
      "    for arch in archs:\n",
      "        for glibc_max in glibc_max_list:\n",
      "            if glibc_max.major == too_old_glibc2.major:\n",
      "                min_minor = too_old_glibc2.minor\n",
      "            else:\n",
      "                # For other glibc major versions oldest supported is (x, 0).\n",
      "                min_minor = -1\n",
      "            for glibc_minor in range(glibc_max.minor, min_minor, -1):\n",
      "                glibc_version = _GLibCVersion(glibc_max.major, glibc_minor)\n",
      "                tag = \"manylinux_{}_{}\".format(*glibc_version)\n",
      "                if _is_compatible(arch, glibc_version):\n",
      "                    yield f\"{tag}_{arch}\"\n",
      "                # Handle the legacy manylinux1, manylinux2010, manylinux2014 tags.\n",
      "                if glibc_version in _LEGACY_MANYLINUX_MAP:\n",
      "                    legacy_tag = _LEGACY_MANYLINUX_MAP[glibc_version]\n",
      "                    if _is_compatible(arch, glibc_version):\n",
      "                        yield f\"{legacy_tag}_{arch}\"\n",
      "\n",
      "[end of src/wheel/vendored/packaging/_manylinux.py]\n",
      "[start of src/wheel/vendored/packaging/LICENSE.BSD]\n",
      "Copyright (c) Donald Stufft and individual contributors.\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "    1. Redistributions of source code must retain the above copyright notice,\n",
      "       this list of conditions and the following disclaimer.\n",
      "\n",
      "    2. Redistributions in binary form must reproduce the above copyright\n",
      "       notice, this list of conditions and the following disclaimer in the\n",
      "       documentation and/or other materials provided with the distribution.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
      "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
      "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "[end of src/wheel/vendored/packaging/LICENSE.BSD]\n",
      "[start of src/wheel/vendored/packaging/requirements.py]\n",
      "# This file is dual licensed under the terms of the Apache License, Version\n",
      "# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n",
      "# for complete details.\n",
      "\n",
      "from typing import Any, Iterator, Optional, Set\n",
      "\n",
      "from ._parser import parse_requirement as _parse_requirement\n",
      "from ._tokenizer import ParserSyntaxError\n",
      "from .markers import Marker, _normalize_extra_values\n",
      "from .specifiers import SpecifierSet\n",
      "from .utils import canonicalize_name\n",
      "\n",
      "\n",
      "class InvalidRequirement(ValueError):\n",
      "    \"\"\"\n",
      "    An invalid requirement was found, users should refer to PEP 508.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "class Requirement:\n",
      "    \"\"\"Parse a requirement.\n",
      "\n",
      "    Parse a given requirement string into its parts, such as name, specifier,\n",
      "    URL, and extras. Raises InvalidRequirement on a badly-formed requirement\n",
      "    string.\n",
      "    \"\"\"\n",
      "\n",
      "    # TODO: Can we test whether something is contained within a requirement?\n",
      "    #       If so how do we do that? Do we need to test against the _name_ of\n",
      "    #       the thing as well as the version? What about the markers?\n",
      "    # TODO: Can we normalize the name and extra name?\n",
      "\n",
      "    def __init__(self, requirement_string: str) -> None:\n",
      "        try:\n",
      "            parsed = _parse_requirement(requirement_string)\n",
      "        except ParserSyntaxError as e:\n",
      "            raise InvalidRequirement(str(e)) from e\n",
      "\n",
      "        self.name: str = parsed.name\n",
      "        self.url: Optional[str] = parsed.url or None\n",
      "        self.extras: Set[str] = set(parsed.extras or [])\n",
      "        self.specifier: SpecifierSet = SpecifierSet(parsed.specifier)\n",
      "        self.marker: Optional[Marker] = None\n",
      "        if parsed.marker is not None:\n",
      "            self.marker = Marker.__new__(Marker)\n",
      "            self.marker._markers = _normalize_extra_values(parsed.marker)\n",
      "\n",
      "    def _iter_parts(self, name: str) -> Iterator[str]:\n",
      "        yield name\n",
      "\n",
      "        if self.extras:\n",
      "            formatted_extras = \",\".join(sorted(self.extras))\n",
      "            yield f\"[{formatted_extras}]\"\n",
      "\n",
      "        if self.specifier:\n",
      "            yield str(self.specifier)\n",
      "\n",
      "        if self.url:\n",
      "            yield f\"@ {self.url}\"\n",
      "            if self.marker:\n",
      "                yield \" \"\n",
      "\n",
      "        if self.marker:\n",
      "            yield f\"; {self.marker}\"\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        return \"\".join(self._iter_parts(self.name))\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        return f\"<Requirement('{self}')>\"\n",
      "\n",
      "    def __hash__(self) -> int:\n",
      "        return hash(\n",
      "            (\n",
      "                self.__class__.__name__,\n",
      "                *self._iter_parts(canonicalize_name(self.name)),\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def __eq__(self, other: Any) -> bool:\n",
      "        if not isinstance(other, Requirement):\n",
      "            return NotImplemented\n",
      "\n",
      "        return (\n",
      "            canonicalize_name(self.name) == canonicalize_name(other.name)\n",
      "            and self.extras == other.extras\n",
      "            and self.specifier == other.specifier\n",
      "            and self.url == other.url\n",
      "            and self.marker == other.marker\n",
      "        )\n",
      "\n",
      "[end of src/wheel/vendored/packaging/requirements.py]\n",
      "[start of src/wheel/vendored/packaging/LICENSE]\n",
      "This software is made available under the terms of *either* of the licenses\n",
      "found in LICENSE.APACHE or LICENSE.BSD. Contributions to this software is made\n",
      "under the terms of *both* these licenses.\n",
      "\n",
      "[end of src/wheel/vendored/packaging/LICENSE]\n",
      "[start of src/wheel/vendored/packaging/utils.py]\n",
      "# This file is dual licensed under the terms of the Apache License, Version\n",
      "# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n",
      "# for complete details.\n",
      "\n",
      "import re\n",
      "from typing import FrozenSet, NewType, Tuple, Union, cast\n",
      "\n",
      "from .tags import Tag, parse_tag\n",
      "from .version import InvalidVersion, Version\n",
      "\n",
      "BuildTag = Union[Tuple[()], Tuple[int, str]]\n",
      "NormalizedName = NewType(\"NormalizedName\", str)\n",
      "\n",
      "\n",
      "class InvalidName(ValueError):\n",
      "    \"\"\"\n",
      "    An invalid distribution name; users should refer to the packaging user guide.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "class InvalidWheelFilename(ValueError):\n",
      "    \"\"\"\n",
      "    An invalid wheel filename was found, users should refer to PEP 427.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "class InvalidSdistFilename(ValueError):\n",
      "    \"\"\"\n",
      "    An invalid sdist filename was found, users should refer to the packaging user guide.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "# Core metadata spec for `Name`\n",
      "_validate_regex = re.compile(\n",
      "    r\"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$\", re.IGNORECASE\n",
      ")\n",
      "_canonicalize_regex = re.compile(r\"[-_.]+\")\n",
      "_normalized_regex = re.compile(r\"^([a-z0-9]|[a-z0-9]([a-z0-9-](?!--))*[a-z0-9])$\")\n",
      "# PEP 427: The build number must start with a digit.\n",
      "_build_tag_regex = re.compile(r\"(\\d+)(.*)\")\n",
      "\n",
      "\n",
      "def canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:\n",
      "    if validate and not _validate_regex.match(name):\n",
      "        raise InvalidName(f\"name is invalid: {name!r}\")\n",
      "    # This is taken from PEP 503.\n",
      "    value = _canonicalize_regex.sub(\"-\", name).lower()\n",
      "    return cast(NormalizedName, value)\n",
      "\n",
      "\n",
      "def is_normalized_name(name: str) -> bool:\n",
      "    return _normalized_regex.match(name) is not None\n",
      "\n",
      "\n",
      "def canonicalize_version(\n",
      "    version: Union[Version, str], *, strip_trailing_zero: bool = True\n",
      ") -> str:\n",
      "    \"\"\"\n",
      "    This is very similar to Version.__str__, but has one subtle difference\n",
      "    with the way it handles the release segment.\n",
      "    \"\"\"\n",
      "    if isinstance(version, str):\n",
      "        try:\n",
      "            parsed = Version(version)\n",
      "        except InvalidVersion:\n",
      "            # Legacy versions cannot be normalized\n",
      "            return version\n",
      "    else:\n",
      "        parsed = version\n",
      "\n",
      "    parts = []\n",
      "\n",
      "    # Epoch\n",
      "    if parsed.epoch != 0:\n",
      "        parts.append(f\"{parsed.epoch}!\")\n",
      "\n",
      "    # Release segment\n",
      "    release_segment = \".\".join(str(x) for x in parsed.release)\n",
      "    if strip_trailing_zero:\n",
      "        # NB: This strips trailing '.0's to normalize\n",
      "        release_segment = re.sub(r\"(\\.0)+$\", \"\", release_segment)\n",
      "    parts.append(release_segment)\n",
      "\n",
      "    # Pre-release\n",
      "    if parsed.pre is not None:\n",
      "        parts.append(\"\".join(str(x) for x in parsed.pre))\n",
      "\n",
      "    # Post-release\n",
      "    if parsed.post is not None:\n",
      "        parts.append(f\".post{parsed.post}\")\n",
      "\n",
      "    # Development release\n",
      "    if parsed.dev is not None:\n",
      "        parts.append(f\".dev{parsed.dev}\")\n",
      "\n",
      "    # Local version segment\n",
      "    if parsed.local is not None:\n",
      "        parts.append(f\"+{parsed.local}\")\n",
      "\n",
      "    return \"\".join(parts)\n",
      "\n",
      "\n",
      "def parse_wheel_filename(\n",
      "    filename: str,\n",
      ") -> Tuple[NormalizedName, Version, BuildTag, FrozenSet[Tag]]:\n",
      "    if not filename.endswith(\".whl\"):\n",
      "        raise InvalidWheelFilename(\n",
      "            f\"Invalid wheel filename (extension must be '.whl'): {filename}\"\n",
      "        )\n",
      "\n",
      "    filename = filename[:-4]\n",
      "    dashes = filename.count(\"-\")\n",
      "    if dashes not in (4, 5):\n",
      "        raise InvalidWheelFilename(\n",
      "            f\"Invalid wheel filename (wrong number of parts): {filename}\"\n",
      "        )\n",
      "\n",
      "    parts = filename.split(\"-\", dashes - 2)\n",
      "    name_part = parts[0]\n",
      "    # See PEP 427 for the rules on escaping the project name.\n",
      "    if \"__\" in name_part or re.match(r\"^[\\w\\d._]*$\", name_part, re.UNICODE) is None:\n",
      "        raise InvalidWheelFilename(f\"Invalid project name: {filename}\")\n",
      "    name = canonicalize_name(name_part)\n",
      "\n",
      "    try:\n",
      "        version = Version(parts[1])\n",
      "    except InvalidVersion as e:\n",
      "        raise InvalidWheelFilename(\n",
      "            f\"Invalid wheel filename (invalid version): {filename}\"\n",
      "        ) from e\n",
      "\n",
      "    if dashes == 5:\n",
      "        build_part = parts[2]\n",
      "        build_match = _build_tag_regex.match(build_part)\n",
      "        if build_match is None:\n",
      "            raise InvalidWheelFilename(\n",
      "                f\"Invalid build number: {build_part} in '{filename}'\"\n",
      "            )\n",
      "        build = cast(BuildTag, (int(build_match.group(1)), build_match.group(2)))\n",
      "    else:\n",
      "        build = ()\n",
      "    tags = parse_tag(parts[-1])\n",
      "    return (name, version, build, tags)\n",
      "\n",
      "\n",
      "def parse_sdist_filename(filename: str) -> Tuple[NormalizedName, Version]:\n",
      "    if filename.endswith(\".tar.gz\"):\n",
      "        file_stem = filename[: -len(\".tar.gz\")]\n",
      "    elif filename.endswith(\".zip\"):\n",
      "        file_stem = filename[: -len(\".zip\")]\n",
      "    else:\n",
      "        raise InvalidSdistFilename(\n",
      "            f\"Invalid sdist filename (extension must be '.tar.gz' or '.zip'):\"\n",
      "            f\" {filename}\"\n",
      "        )\n",
      "\n",
      "    # We are requiring a PEP 440 version, which cannot contain dashes,\n",
      "    # so we split on the last dash.\n",
      "    name_part, sep, version_part = file_stem.rpartition(\"-\")\n",
      "    if not sep:\n",
      "        raise InvalidSdistFilename(f\"Invalid sdist filename: {filename}\")\n",
      "\n",
      "    name = canonicalize_name(name_part)\n",
      "\n",
      "    try:\n",
      "        version = Version(version_part)\n",
      "    except InvalidVersion as e:\n",
      "        raise InvalidSdistFilename(\n",
      "            f\"Invalid sdist filename (invalid version): {filename}\"\n",
      "        ) from e\n",
      "\n",
      "    return (name, version)\n",
      "\n",
      "[end of src/wheel/vendored/packaging/utils.py]\n",
      "[start of src/wheel/vendored/packaging/_musllinux.py]\n",
      "\"\"\"PEP 656 support.\n",
      "\n",
      "This module implements logic to detect if the currently running Python is\n",
      "linked against musl, and what musl version is used.\n",
      "\"\"\"\n",
      "\n",
      "import functools\n",
      "import re\n",
      "import subprocess\n",
      "import sys\n",
      "from typing import Iterator, NamedTuple, Optional, Sequence\n",
      "\n",
      "from ._elffile import ELFFile\n",
      "\n",
      "\n",
      "class _MuslVersion(NamedTuple):\n",
      "    major: int\n",
      "    minor: int\n",
      "\n",
      "\n",
      "def _parse_musl_version(output: str) -> Optional[_MuslVersion]:\n",
      "    lines = [n for n in (n.strip() for n in output.splitlines()) if n]\n",
      "    if len(lines) < 2 or lines[0][:4] != \"musl\":\n",
      "        return None\n",
      "    m = re.match(r\"Version (\\d+)\\.(\\d+)\", lines[1])\n",
      "    if not m:\n",
      "        return None\n",
      "    return _MuslVersion(major=int(m.group(1)), minor=int(m.group(2)))\n",
      "\n",
      "\n",
      "@functools.lru_cache\n",
      "def _get_musl_version(executable: str) -> Optional[_MuslVersion]:\n",
      "    \"\"\"Detect currently-running musl runtime version.\n",
      "\n",
      "    This is done by checking the specified executable's dynamic linking\n",
      "    information, and invoking the loader to parse its output for a version\n",
      "    string. If the loader is musl, the output would be something like::\n",
      "\n",
      "        musl libc (x86_64)\n",
      "        Version 1.2.2\n",
      "        Dynamic Program Loader\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(executable, \"rb\") as f:\n",
      "            ld = ELFFile(f).interpreter\n",
      "    except (OSError, TypeError, ValueError):\n",
      "        return None\n",
      "    if ld is None or \"musl\" not in ld:\n",
      "        return None\n",
      "    proc = subprocess.run([ld], stderr=subprocess.PIPE, text=True)\n",
      "    return _parse_musl_version(proc.stderr)\n",
      "\n",
      "\n",
      "def platform_tags(archs: Sequence[str]) -> Iterator[str]:\n",
      "    \"\"\"Generate musllinux tags compatible to the current platform.\n",
      "\n",
      "    :param archs: Sequence of compatible architectures.\n",
      "        The first one shall be the closest to the actual architecture and be the part of\n",
      "        platform tag after the ``linux_`` prefix, e.g. ``x86_64``.\n",
      "        The ``linux_`` prefix is assumed as a prerequisite for the current platform to\n",
      "        be musllinux-compatible.\n",
      "\n",
      "    :returns: An iterator of compatible musllinux tags.\n",
      "    \"\"\"\n",
      "    sys_musl = _get_musl_version(sys.executable)\n",
      "    if sys_musl is None:  # Python not dynamically linked against musl.\n",
      "        return\n",
      "    for arch in archs:\n",
      "        for minor in range(sys_musl.minor, -1, -1):\n",
      "            yield f\"musllinux_{sys_musl.major}_{minor}_{arch}\"\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":  # pragma: no cover\n",
      "    import sysconfig\n",
      "\n",
      "    plat = sysconfig.get_platform()\n",
      "    assert plat.startswith(\"linux-\"), \"not linux\"\n",
      "\n",
      "    print(\"plat:\", plat)\n",
      "    print(\"musl:\", _get_musl_version(sys.executable))\n",
      "    print(\"tags:\", end=\" \")\n",
      "    for t in platform_tags(re.sub(r\"[.-]\", \"_\", plat.split(\"-\", 1)[-1])):\n",
      "        print(t, end=\"\\n      \")\n",
      "\n",
      "[end of src/wheel/vendored/packaging/_musllinux.py]\n",
      "\n",
      "\n",
      "\n",
      "Question:\n",
      "When building manylinux wheels for the 'dm_reverb' project, what is the correct process to ensure they are properly created and packaged?\n",
      "\n",
      "A) Use the cibuildwheel tool with the --only <identifier> option, which uses the manylinux docker image and auditwheel.\n",
      "B) Build them directly without using any specific tools or processes.\n",
      "C) Use the auditwheel tool directly on the wheels without cibuildwheel.\n",
      "D) Manually compile the wheels on a local machine and hope for compatibility.\n"
     ]
    }
   ],
   "source": [
    "print(data[1]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' D'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
