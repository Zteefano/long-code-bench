[INFO]: Starting head node lrdn1725 at 10.5.1.29
[INFO]: Removing old headnode overlay and creating a new writable layer
[INFO]: srun --nodes=1 --ntasks=1 -w lrdn1725 singularity exec --nv --bind /leonardo_scratch:/leonardo_scratch,/leonardo_work:/leonardo_work --overlay /leonardo_scratch/large/userexternal/lromani0/worker_overlay_0.img /leonardo/home/userexternal/lromani0/IscrC_TfG/docker_images/vllm_singularity-2025-01-14-2e6300811da9.sif bash -c "ray start --head --include-dashboard True --dashboard-port 8265 --dashboard-host=0.0.0.0 --node-ip-address=10.5.1.29 --port=6379 --num-cpus 32 --disable-usage-stats --block & sleep 60 && vllm serve /leonardo/home/userexternal/lromani0/IscrC_TfG/cache/cache/hub/models--ai21labs--AI21-Jamba-1.5-Mini/snapshots/1840d3373c51e4937f4dbaaaaf8cac1427b46858 --host 0.0.0.0 --port 8000 --tensor-parallel-size 4 --pipeline-parallel-size 2 --api-key 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"
===================================================
[INFO]: To access the RAY web ui, remember to open a ssh tunnel with:
ssh -L 26802:10.5.1.29:8265 lromani0@login02-ext.leonardo.cineca.it -N
then you can connect to the dashboard at http://127.0.0.1:26802
===================================================
[INFO]: Removing old workernode 1 overlay and creating a new writable layer
2025-02-05 10:57:19,742	INFO usage_lib.py:441 -- Usage stats collection is disabled.
2025-02-05 10:57:19,742	INFO scripts.py:823 -- [37mLocal node IP[39m: [1m10.5.1.29[22m
[INFO]: Starting worker 1 lrdn1734 at 10.5.1.38
[INFO]: srun --nodes=1 --ntasks=1 -w lrdn1734 singularity exec --nv --bind /leonardo_scratch:/leonardo_scratch,/leonardo_work:/leonardo_work --overlay /leonardo_scratch/large/userexternal/lromani0/worker_overlay_1.img /leonardo/home/userexternal/lromani0/IscrC_TfG/docker_images/vllm_singularity-2025-01-14-2e6300811da9.sif ray start --address 10.5.1.29:6379 --num-cpus 32 --disable-usage-stats --block
2025-02-05 10:57:20,915	SUCC scripts.py:860 -- [32m--------------------[39m
2025-02-05 10:57:20,915	SUCC scripts.py:861 -- [32mRay runtime started.[39m
2025-02-05 10:57:20,915	SUCC scripts.py:862 -- [32m--------------------[39m
2025-02-05 10:57:20,915	INFO scripts.py:864 -- [36mNext steps[39m
2025-02-05 10:57:20,915	INFO scripts.py:867 -- To add another node to this Ray cluster, run
2025-02-05 10:57:20,915	INFO scripts.py:870 -- [1m  ray start --address='10.5.1.29:6379'[22m
2025-02-05 10:57:20,915	INFO scripts.py:879 -- To connect to this Ray cluster:
2025-02-05 10:57:20,915	INFO scripts.py:881 -- [35mimport[39m[26m ray
2025-02-05 10:57:20,916	INFO scripts.py:882 -- ray[35m.[39m[26minit(_node_ip_address[35m=[39m[26m[33m'10.5.1.29'[39m[26m)
2025-02-05 10:57:20,916	INFO scripts.py:894 -- To submit a Ray job using the Ray Jobs CLI:
2025-02-05 10:57:20,916	INFO scripts.py:895 -- [1m  RAY_ADDRESS='http://10.5.1.29:8265' ray job submit --working-dir . -- python my_script.py[22m
2025-02-05 10:57:20,916	INFO scripts.py:904 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html 
2025-02-05 10:57:20,916	INFO scripts.py:908 -- for more information on submitting Ray jobs to the Ray cluster.
2025-02-05 10:57:20,916	INFO scripts.py:913 -- To terminate the Ray runtime, run
2025-02-05 10:57:20,916	INFO scripts.py:914 -- [1m  ray stop[22m
2025-02-05 10:57:20,916	INFO scripts.py:917 -- To view the status of the cluster, use
2025-02-05 10:57:20,916	INFO scripts.py:918 --   [1mray status[22m[26m
2025-02-05 10:57:20,916	INFO scripts.py:922 -- To monitor and debug Ray, view the dashboard at 
2025-02-05 10:57:20,916	INFO scripts.py:923 --   [1m10.5.1.29:8265[22m[26m
2025-02-05 10:57:20,916	INFO scripts.py:930 -- [4mIf connection to the dashboard fails, check your firewall settings and network configuration.[24m
2025-02-05 10:57:20,916	INFO scripts.py:1031 -- [36m[1m--block[22m[39m
2025-02-05 10:57:20,916	INFO scripts.py:1032 -- This command will now block forever until terminated by a signal.
2025-02-05 10:57:20,916	INFO scripts.py:1035 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-02-05 10:57:29,818	INFO scripts.py:1005 -- [37mLocal node IP[39m: [1m10.5.1.38[22m
[INFO]: everything is running.
2025-02-05 10:57:30,944	SUCC scripts.py:1018 -- [32m--------------------[39m
2025-02-05 10:57:30,944	SUCC scripts.py:1019 -- [32mRay runtime started.[39m
2025-02-05 10:57:30,944	SUCC scripts.py:1020 -- [32m--------------------[39m
2025-02-05 10:57:30,944	INFO scripts.py:1022 -- To terminate the Ray runtime, run
2025-02-05 10:57:30,944	INFO scripts.py:1023 -- [1m  ray stop[22m
2025-02-05 10:57:30,944	INFO scripts.py:1031 -- [36m[1m--block[22m[39m
2025-02-05 10:57:30,944	INFO scripts.py:1032 -- This command will now block forever until terminated by a signal.
2025-02-05 10:57:30,944	INFO scripts.py:1035 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
INFO 02-05 10:58:24 api_server.py:585] vLLM API server version 0.6.4.post1
INFO 02-05 10:58:24 api_server.py:586] args: Namespace(subparser='serve', model_tag='/leonardo/home/userexternal/lromani0/IscrC_TfG/cache/cache/hub/models--ai21labs--AI21-Jamba-1.5-Mini/snapshots/1840d3373c51e4937f4dbaaaaf8cac1427b46858', config='', host='0.0.0.0', port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='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', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/leonardo/home/userexternal/lromani0/IscrC_TfG/cache/cache/hub/models--ai21labs--AI21-Jamba-1.5-Mini/snapshots/1840d3373c51e4937f4dbaaaaf8cac1427b46858', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x14fcbe1116c0>)
INFO 02-05 10:58:28 config.py:1020] Defaulting to use ray for distributed inference
WARNING 02-05 10:58:28 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
WARNING 02-05 10:58:28 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 02-05 10:58:28 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.
WARNING 02-05 10:58:28 config.py:479] Async output processing can not be enabled with pipeline parallel
2025-02-05 10:59:30,955	ERR scripts.py:1065 -- [31mSome Ray subprocesses exited unexpectedly:[39m
2025-02-05 10:59:30,956	ERR scripts.py:1069 -- [31m[1mraylet[22m[26m[39m[0m[2m [exit code=1][22m[0m
2025-02-05 10:59:30,956	ERR scripts.py:1076 -- [31mRemaining processes will be killed.[39m
