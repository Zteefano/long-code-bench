{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"dataset/gpt-results/gpt4o_128K.json\"\n",
    "\n",
    "with open(data_dir, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts: 170it [01:47,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"dataset/output_64K/final_dataset.json\"\n",
    "\n",
    "result_dir = \"dataset/output_64K/results\"\n",
    "\n",
    "with open(base_dir, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "input_file = \"dataset/gemini-results/gemini_64K.json\"\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    gpt_answer = json.load(f)[\"results\"]\n",
    "\n",
    "for i, entry in tqdm(enumerate(dataset), desc=\"Evaluating prompts\"):\n",
    "\n",
    "    # entry.keys() = dict_keys(['prompt', 'generation', 'correct_letter', 'question', 'repo_text', 'prompt_goal', 'repo'])\n",
    "    question = entry[\"question\"]\n",
    "    correct_answer = entry[\"correct_letter\"]\n",
    "\n",
    "    model_answer = gpt_answer[i][\"text\"]\n",
    "\n",
    "    # If empty string, go to the next entry\n",
    "    if model_answer == \"\":\n",
    "        continue\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "I will give you a question, that an AI model answered, and the correct answer. \n",
    "The question is: {question}\n",
    "The AI model answered: {model_answer}\n",
    "The correct answer is: {correct_answer}\n",
    "Please tell me if the AI model answered correctly or not.\n",
    "If the AI model answered correctly, please answer with \"yes\".\n",
    "If the AI model answered incorrectly, please answer with \"no\".\n",
    "\"\"\"\n",
    "\n",
    "    client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = client.chat.completions.create(model = \"gpt-4o\",\n",
    "                                                messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "                                                temperature = 0.0)\n",
    "    generation = response.choices[0].message.content\n",
    "    entry[\"is_correct\"] = generation\n",
    "\n",
    "# Save the modified dataset back to a JSONL file\n",
    "# Output file name = model_name + \"-eval.jsonl\"\n",
    "# Expect - in the model name\n",
    "# Example: AI21-Jamba-1.5-Large-eval.jsonl\n",
    "model_name = \"gemini\"  # Remove the last 6 characters (\".jsonl\")\n",
    "output_file_name = f\"{model_name}-eval.jsonl\"\n",
    "output_file = os.path.join(result_dir, output_file_name)\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    for entry in dataset:\n",
    "        file.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty generations: 0\n"
     ]
    }
   ],
   "source": [
    "empty_gen = 0\n",
    "\n",
    "for entry in gpt_answer:\n",
    "    if entry[\"text\"] == \"\":\n",
    "        empty_gen += 1\n",
    "\n",
    "print(f\"Number of empty generations: {empty_gen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"dataset/output_32K/final_dataset.json\"\n",
    "\n",
    "with open(base_dir, \"r\") as f:\n",
    "    dataset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Why does `importlib.metadata` in Python 3.12 use the `METADATA` file inside the `dist-info` directory for package names and versions instead of the directory name itself?\n",
      "A) To ensure compatibility with `pkg_resources` module.\n",
      "B) To rely on a canonical source for all metadata, allowing package metadata to exist independently of filesystem structure.\n",
      "C) To allow dynamic generation of package names and versions.\n",
      "D) To increase efficiency by reducing the number of read operations.\n",
      " B\n"
     ]
    }
   ],
   "source": [
    "idx = 6\n",
    "\n",
    "print(dataset[idx][\"question\"], data[idx][\"text\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"dataset/output_128K/final_dataset.json\"\n",
    "\n",
    "result_dir = \"dataset/output_32K/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"dataset/gemini-results/gemini_1M.json\"\n",
    "\n",
    "with open(data_dir, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty text found\n",
      "{'text': '', 'id': '18'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '19'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '20'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '21'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '22'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '23'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '24'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '25'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '26'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '27'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '28'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '29'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '30'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '31'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '32'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '33'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '34'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '35'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '36'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '37'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '38'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '39'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '40'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '41'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '42'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '43'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '44'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '45'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '46'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '131'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '132'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '133'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '134'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '135'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '136'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '137'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '138'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '139'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '140'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '141'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '142'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '143'}\n",
      "Empty text found\n",
      "{'text': '', 'id': '144'}\n",
      "Number of empty generations: 43\n"
     ]
    }
   ],
   "source": [
    "empty_gen = 0\n",
    "\n",
    "for point in data:\n",
    "    # Check if point[\"text\"] is an empty string\n",
    "    if point[\"text\"] == \"\":\n",
    "        print(\"Empty text found\")\n",
    "        print(point)\n",
    "        empty_gen += 1\n",
    "\n",
    "print(f\"Number of empty generations: {empty_gen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The provided code snippets don't explicitly mention any specific potential reasons for test failures when regenerating certificates with OpenSSL version greater than 3.2.0. However, the file `HISTORY.md` and `setup.py` give some clues. \\n\\nThe provided text focuses primarily on:\\n\\n1.  Supported Python versions and dependency management using `setup.py`, `.pre-commit-config.yaml`, and `tox.ini`.\\n2.  General API usage of the Requests library.\\n3.  Release process and guidelines for contributors.\\n4.  Internal architecture of Requests, such as session management and transport adapters.\\n5.  Configuration of SSL certificate verification.\\n\\nHowever, `HISTORY.md` mentions a specific potential bug in the packaging due to the presence of test certificates. It also shows that there is a renewed focus on compatibility with OpenSSL and related cryptography libraries. The security extra is deprecated, which indirectly suggests that default security settings should work without special configurations.\\n\\nGiven this information, the most likely issue would be that certificates generated are not considered valid due to changed security requirements in newer OpenSSL versions. The most likely reasons would be that the generated certificate does not conform to new standards of OpenSSL, relating to its key size or signature algorithms, not necessarily the ordering of certificates. The provided context does not give information on the necessity of the 'CA' constraint.\\n\\nTherefore, the most relevant and plausible answer is:\\n\\nFinal Answer: A\",\n",
       " 'id': '0'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
